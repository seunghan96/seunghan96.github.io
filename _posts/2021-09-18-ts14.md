---
title: (paper) Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
categories: [TS]
tags: [TS]
excerpt: Time Series Forecasting (2019, 193)
---

# Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting (2019, 193)

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

### Contents

0. Abstract
1. Introduction
2. Background
   1. Problem Definition
   2. Transformer
3. Methodology
   1. Enhancing the locality of Transformer
   2. Breaking the memory bottleneck of Transformer

<br>

# 0. Abstract

propose to tackle Transformer

2 major weakness

- 1) locality-agnostic
  - point-wise dot-production self attention is "insensitive" to local context
- 2) memory bottleneck
  - space complexity of Transformer "grows quadratically" with sequence length $L$

<br>

Proposal

1. propose "CONVOLUTIONAL self-attention"
   - by producing Q & K with "causal convolution"
2. propose "LogSparse Transformer"
   - $O(L(logL)^2)$ memory cost

<br>

# 1. Introduction

Traditional TSF models

- ex) SSMs, AR models $\rightarrow$ fit EACH TS independently & manually select trend/seasonality

<br>

### Transformer

- leverage attention mechansim
- BUT, ***canonical dot-product self-attention : insensitive to LOCAL context***
- also, ***BAD space complexity***

<br>

# 2. Background

## (1) Problem Definition

Univariate Time Series ( \# of data : $N$ ) :

-  $\left\{\mathbf{z}_{i, 1: t_{0}}\right\}_{i=1}^{N}$ 
- $\mathbf{z}_{i, 1: t_{0}} \triangleq\left[\mathbf{z}_{i, 1}, \mathbf{z}_{i, 2}, \cdots, \mathbf{z}_{i, t_{0}}\right]$ , where $\mathbf{z}_{i, t} \in \mathbb{R}$

<br>

Covariate : $\left\{\mathrm{x}_{i, 1: t_{0}+\tau}\right\}_{i=1}^{N}$

- known entire period

<br>

Goal : predict the next $\tau$ time steps ( $\left\{\mathbf{z}_{i, t_{0}+1: t_{0}+\tau}\right\}_{i=1}^{N} $ )

<br>

Model : conditional distribution...

$p\left(\mathbf{z}_{i, t_{0}+1: t_{0}+\tau} \mid \mathbf{z}_{i, 1: t_{0}}, \mathbf{x}_{i, 1: t_{0}+\tau} ; \boldsymbol{\Phi}\right)=\prod_{t=t_{0}+1}^{t_{0}+\tau} p\left(\mathbf{z}_{i, t} \mid \mathbf{z}_{i, 1: t-1}, \mathbf{x}_{i, 1: t} ; \boldsymbol{\Phi}\right)$.

- reduce to "one-step-ahead prediction model"
- use both "observation & covariates"
  - $\mathbf{y}_{t} \triangleq\left[\mathbf{z}_{t-1} \circ \mathbf{x}_{t}\right] \in \mathbb{R}^{d+1}, \quad \mathbf{Y}_{t}=\left[\mathbf{y}_{1}, \cdots, \mathbf{y}_{t}\right]^{T} \in \mathbb{R}^{t \times(d+1)}$.

<br>

## (2) Transformer

capture both LONG & SHORT term dependencies ( with different attention heads )

$\mathbf{O}_{h}=\operatorname{Attention}\left(\mathbf{Q}_{h}, \mathbf{K}_{h}, \mathbf{V}_{h}\right)=\operatorname{softmax}\left(\frac{\mathbf{Q}_{h} \mathbf{K}_{h}^{T}}{\sqrt{d_{k}}} \cdot \mathbf{M}\right) \mathbf{V}_{h}$.

- $\mathbf{Q}_{h}=\mathbf{Y} \mathbf{W}_{h}^{Q}$.

- $\mathbf{K}_{h}=\mathbf{Y} \mathbf{W}_{h}^{K}$.

- $\mathbf{V}_{h}=\mathbf{Y} \mathbf{W}_{h}^{V}$.

  with $h=1, \cdots, H .$ 

- $\mathbf{M}$ : mask matrix ( setting all upper triangular elements to $-\infty$ )

<br>

Parameters

- $\mathbf{W}_{h}^{Q}, \mathbf{W}_{h}^{K} \in \mathbb{R}^{(d+1) \times d_{k}}$.
- $\mathbf{W}_{h}^{V} \in \mathbb{R}^{(d+1) \times d_{v}}$.

<br>

Afterwards, $\mathbf{O}_{1}, \mathbf{O}_{2}, \cdots, \mathbf{O}_{H}$ are concatenated & FC layer

<br>

# 3. Methodology

## (1) Enhancing the locality of Transformer

*Patterns in time series may evolve with time!*

Self-attention

- original ) point-wise
- proposed ) convolution

<br>

![figure2](/assets/img/ts/img186.png)

<br>

Causal Convolution 

- filter of kernel size $k$ with stride $1$

- to get Q & K

  ( V is still produced by point-wise attention )

- **more aware of local context**

<br>

## (2) Breaking the memory bottleneck of Transformer

![figure2](/assets/img/ts/img187.png)

<br>

# 4. Experiment

## (1) Synthetic datasets

$f(x)= \begin{cases}A_{1} \sin (\pi x / 6)+72+N_{x} & x \in[0,12) \\ A_{2} \sin (\pi x / 6)+72+N_{x} & x \in[12,24) \\ A_{3} \sin (\pi x / 6)+72+N_{x} & x \in\left[24, t_{0}\right) \\ A_{4} \sin (\pi x / 12)+72+N_{x} & x \in\left[t_{0}, t_{0}+24\right)\end{cases}$.

- where $x$ is an integer,
- $A_{1}, A_{2}, A_{3}$ are randomly generated by Unif $[0,60]$
- $A_{4}=$ $\max \left(A_{1}, A_{2}\right)$ and $N_{x} \sim \mathcal{N}(0,1)$. 

<br>

## (2) Real-world Dataset

- `electricity-f` ( fine )
- `electricity-c` ( coarse)
- `traffic-f`
- `traffic-c`



