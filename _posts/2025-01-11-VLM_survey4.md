---
title: (VLM survey) (Part 4; VLM Transfer Learning)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey

https://arxiv.org/pdf/2304.00685

<br>

# Contents

- (6) VLM Transfer Learning
  - Motivation of Transfer Learning
  - Common Setup of Transfer Learning
  - Common Transfer Learning Methods
    - Text PT
    - Visual PT
    - Text-Visual PT

  - Summary & Discussion


<br>

# 6. VLM Transfer Learning

(Beyond zero-shot prediction) Transfer learning has been studied!

$\rightarrow$ Adapts VLMs to fit downstream tasks via...

- (1) Prompt tuning [31], [132]
- (2) Feature adapter [33], [34]

<br>

Sections

- (1) Motivation of TL for pre-trained VLMs
- (2) Common TL setup
- (3) Three TL approaches
  - 3-1) Prompt tuning methods
  - 3-2) Feature adapter methods
  - 3-3) Etc.

<br>

## (1) Motivation of Transfer Learning

Two types of gaps while applied to various downstream tasks:

- (1) Gaps in **image and text distributions**
  - e.g., Downstream dataset may have **"task-specific image styles and text formats"** 
- (2) Gaps in **training objectives**
  - e.g., VLMs are generally ..
    - pretrained with **"task-agnostic objectives"** 
      - e.g., learn general concepts 
    - while downstream tasks often involve **"task-specific objectives"** 
      - e.g., coarse or fine-grained classification, region or pixel-level recognition, etc...

<br>

## (2) Common Setup of Transfer Learning

Three TL setups (to mitigate the domain gaps!)

- (1) Supervised transfer
- (2) Few-shot supervised transfer
- (3) Unsupervised transfer

<br>

## (3) Common Transfer Learning Methods

- (1) Prompt tuning approaches
- (2) Feature adapter approaches
- (3) Etc..

<br>

![figure2](/assets/img/llm/img407.png)

<br>

### P1) via Prompt Tuning (PT)

(Previous) Prompt engineering: **Manually** designs text prompts for each task

By finding **optimal prompts**, **without fine-tuning the entire VLM**

- a) **Text** PT
- b) **Visual** PT
- c) **Text-visual** PT

![figure2](/assets/img/llm/img408.png)

<br>

### P1-1) Text PT

Explores more effective ***learnable*** text prompts

With several ***labelled*** downstream samples for each class

Example) 

- **CoOp [31]** 

  - Expands a category word [label] into a sentence ‘[V]1, [V]2, ..., [V]m [label]’
  - [V] = **Learnable word vectors**

- **CoCoOp [32]**

  - To mitigate the overfitting of CoOP 

  - **Conditional** context optimization

    $\rightarrow$ Generates a specific prompt **for each image**

- **SubPT [132]** 

  - Designs **subspace** prompt tuning

- **LASP [133]** 

  - Regularizes learnable prompts with **hand-engineered prompts**

- **VPT [135]** 

  - Models text prompts with **instance-specific distribution**

- **KgCoOp [145]** 

  - (= Knowledge-guided Context Optimization for prompt tuning)
  - Enhances the **generalization of unseen class** by mitigating the forgetting of textual knowledge

- **SoftCPT [141]** 
  - (= PT with Soft Context Sharing)
  - Fine-tunes VLMs on multiple few-shot tasks (via multitask learning)

- **PLOT [138]** 
  - Employs optimal transport to learn multiple prompts to describe the diverse characteristics of a category. 

- **DualCoOp [139]**
  - Transfer VLMs to multi-label classification tasks
  - Adopts both positive and negative prompts for multilabel classification
- **TaI-DP [140]**
  - Transfer VLMs to multi-label classification tasks
  - Introduces double-grained prompt tuning for capturing both coarse-grained and fine-grained embeddings. 
- **DenseCLIP [142]** 
  - Explores language guided fine-tuning that employs visual features to tune text prompts for dense prediction
- **ProTeCt [146]** 
  - Improves the consistency of model predictions for hierarchical classification task.
- **UPL [143]**
  - (Unsupervised PT) Learnable prompts with **self-training** on selected **pseudo-labeled** samples. 
- **TPT [144]** 
  - (Unsupervised PT) Explores **test-time PT** to learn **adaptive prompts** from a single downstream sample

<br>

CoCoOp

![figure2](/assets/img/llm/img409.png)

<br>

LASP

![figure2](/assets/img/llm/img410.png)

![figure2](/assets/img/llm/img411.png)

<br>

VPT

![figure2](/assets/img/llm/img412.png)

<br>

KgCoOp

![figure2](/assets/img/llm/img413.png)

<br>

SoftCPT

![figure2](/assets/img/llm/img414.png)

![figure2](/assets/img/llm/img415.png)

<br>

PLOT

![figure2](/assets/img/llm/img416.png)

![figure2](/assets/img/llm/img417.png)

<br>

DualCoOp 

![figure2](/assets/img/llm/img418.png)

![figure2](/assets/img/llm/img419.png)

<br>

DenseCLIP

![figure2](/assets/img/llm/img420.png)

<br>

UPL 

![figure2](/assets/img/llm/img421.png)

![figure2](/assets/img/llm/img422.png)

<br>

TPT 

![figure2](/assets/img/llm/img423.png)

<br>

### P1-2) Visual PT

Transfers VLMs by modulating the input of image encoder

Example)

- **VP [147]** 

  - Learnable image perturbations $v$ 

  - How? Modify the input image $x^I$ by $x^I+v$

    $\rightarrow$ Aim to adjust $v$ to minimize a recognition loss

- **RePrompt [148]** 

  - Integrates retrieval mechanisms into visual prompt tuning

$\rightarrow$ Enables pixel-level adaptation to downstream tasks ( Benefit dense prediction tasks )

<br>

VP

![figure2](/assets/img/llm/img424.png)

<br>

### P1-3) Text-Visual PT

Benefiting from **joint** prompt optimization on **multiple modalities**

Example)

- **UPT [149]** 

  - **Unified** vision and language PT

- **MVLPT [150]** 

  - **Multitask** Vision-Language Prompt Tuning
  - Incorporate cross-task knowledge into text and image PT

- **MaPLe [151]** 

  - **Multi-modal** prompt learning

  - By **aligning** visual prompts with their corresponding language prompts 

    $\rightarrow$ Enabling a **mutual promotion** between text prompts & image prompts. 

- **CAVPT [152]** 

  - **Cross attention** between class-aware visual prompts & text prompts, 

<br>

UPT

![figure2](/assets/img/llm/img425.png)

![figure2](/assets/img/llm/img426.png)

<br>

MVLPT

![figure2](/assets/img/llm/img427.png)

<br>

MaPLe

![figure2](/assets/img/llm/img428.png)

![figure2](/assets/img/llm/img429.png)

<br>

### P1-4) Discussion

PT = Parameter-efficient VLM transfer

- with a few learnable text/image prompts

- requires little extra network layers or complex network modifications. 

Nonetheless, low flexibility!

<br>

### P2) via Feature Adaptation

Fine-tunes VLMs with an additional light-weight feature adapter

Example)

- Clip-Adapter [33] 

  - Several trainable linear layers after CLIP’s language and image encoders
  - Keep others frozen

- TipAdapter [34] 

  - Training-free adapter

    $\rightarrow$ Directly employs the embeddings of few-shot labelled images as the adapter weights! 

- SVL-Adapter [153] 

  - designs a selfsupervised adapter which employs an additional encoder for self-supervised learning on input images. In summary, feature adapter adapts image and text features to fit VLMs to downstream data, which provides a promising alternative to prompt tuning for VLMs transfer

<br>

Clip-Adapter

![figure2](/assets/img/llm/img430.png)

<br>

TipAdapter

![figure2](/assets/img/llm/img431.png)

<br>

SVL-Adapter

![figure2](/assets/img/llm/img432.png)

<br>

**Discussion** 

Feature adaptation adapts VLMs by modifying image and text features with an additional light-weight feature adapter. It is flexible and effective as its architecture and  the insertion manner allow tailoring flexibly for different downstream tasks. Therefore, feature adaptation has clear advantages in adapting VLMs to work on very different and complex downstream tasks [168], [169], [170], [171]. On the other hand, it requires modifying network architecture and thus can not handle VLMs that have concerns in intellectual property.

<br>

### P3) Other Methods

Several studies transfer VLMs by direct fine-tuning [162], architecture modification [163], and cross attention [157], [158]. Specifically, Wise-FT [162] combines the weights of a fine-tuned VLM and the original VLM for learning new information from downstream tasks. MaskCLIP [163] extracts dense image features by modifying the architecture of the CLIP image encoder. VT-CLIP [157] introduces visualguided attention to semantically correlate text features with downstream images, leading to a better transfer performance. CALIP [158] introduces parameter-free attention for effective interaction and communication between visual and text features, leading to text-aware image features and visual-guided text features. TaskRes [159] directly tunes text-based classifier to exploit the old knowledge in the pre-trained VLM. CuPL [160] and VCD [161] employ large language models, e.g., GPT3 [172], to augment text prompts for learning rich discriminative text information.

<br>

## (4) Summary & Discussion

In summary, prompt tuning and feature adapter are two major approaches for VLM transfer which work by modifying the input text/image and adapting image/text features, respectively. In addition, both approaches introduce very limited parameters while freezing the original VLMs, leading to efficient transfer. Further, while most studies follow few-shot supervised transfer [31], [32], [132], [134], recent studies show that unsupervised VLM transfer can achieve competitive performance on various tasks [143], [144], [160], inspiring more research on unsupervised VLM transfer.

<br>
