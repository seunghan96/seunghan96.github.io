---
title: 22.Q-Learning
categories: [RL]
tags: [Reinforcement Learning, Q-Learning]
excerpt: Q-Learning, On & Off Policy
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

( 참고 : Fastcampus 강의 )

# [ 22. Q-Learning ]

### Contents

1. On & Off Policy
   1. On Policy
   2. Off Policy
2. Q-Learning
   1. 학습하는 Policy (target policy , $$\pi$$ ) 
   2. 행동하는 Policy (behavior policy , $$\mu$$ ) 
3. Summary

<br>

# 1. On & Off Policy
## (1) On Policy
- **학습하는 policy = 행동하는 policy**

- 한 번의 policy improvement를 한 이후에는, 그 policy가 했던 **과거의 경험들은 모두 이용 불가!** 
  
  ( **현재 policy 하에서 최적**이라고 생각하는 것을 선택한 이후, 그 **이전의 경험들은 모두 잊어버림** )
  
- 단점 ) 데이터의 효율성이 떨어짐

- ex) **SARSA**
  

<br>

## (2) Off Policy

- **학습하는 policy $$\neq$$ 행동하는 policy** ( 반드시 같을 필요는 없다 )
- 현재 학습하는 policy가 과거에 했던 경험들도 **모두 기억**!
- RE-USE previous policies
- 장점 ) **여러 개의 agent, 심지어 사람**으로부터 학습 가능!
- ex) **Q-Learning**

<br>

( 앞으로 '행동하는 policy(**behavior policy**)'를 $$\mu$$, 학습하는 policy(**target policy**)'를 <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a>라고 하겠다 ) 

# 2. Q-Learning
## (1) 학습하는 Policy (target policy , $$\pi$$ ) 

[ Key : Q-function을 update ]

- **'학습'만 할 뿐**, 실제로 이를 **행동하는 것은 아님**!

- 다음 state의 maximum Q-function를, 현재 state의 Q-function으로 update

<br>

**Q-function updating equation**

- SARSA : $$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t}))$$
  - 필요한 sample : $$[S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}]$$
- Q-Learning : $$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(R_{t+1} + \gamma \underset{\alpha^{'}}{\cdot \text{max}} Q(S_{t+1},\alpha^{'}) - Q(S_{t},A_{t}))$$
  - 필요한 sample : $$[S_t, A_t, R_{t+1}, S_{t+1}]$$

<br>

다음 상태에서 다음 행동을 하는 것이 아니라,

다음 상태에서 maximum Q-function을 가지고 update

<br>

## (2) 행동하는 Policy (behavior policy , $$\mu$$ ) 

[ Key: $$\epsilon$$-greedy method ]

### Algorithm

- step 1) 현재 $$S$$(state)에서 $$\mu$$ 에 따라 $$A$$(action) 선택
- step 2) 다음 $$S'$$(state)에서 할 행동($$A'$$)은 $$\pi$$ 에 따라 선택 
  - $$\pi(S_{t+1}) = \underset{a'}{argmax}\;Q(S_{t+1},a')$$.
- step 3) Q-learning의 target
  - $$\begin{align*}
    R_{t+1} + \gamma\;Q(S_{t+1},A') &= R_{t+1} + \gamma Q(S_{t+1},\underset{a'}{argmax}\;Q(S_{t+1,a'}))\\
    &= R_{t+1} + \underset{a'}{max}\;\gamma Q(S_{t+1},a')
    \end{align*}$$.
- step 4) Q-function update
  - $$Q(S,A) \leftarrow Q(S,A) + \alpha(R+\gamma\;\underset{a'}{max}Q(S',\alpha')-Q(S,A))$$.

<br>

# 3. Pseudo-code
SARSA는 "on-policy"방법의 TD이고, Q-learning은 "off-policy"방법의 TD이다. 

이 둘의 차이점은, **Q-learning의 경우에는 behavior policy(행동하는 policy)와 target policy(학습하는 policy)가 따로 존재** 한다는 점이다. 

<br>

아래의 두 사진 중, 밑에 사진인 Q-Learning을 보면, $$\epsilon$$-greedy method로부터 얻어진 behavior policy에서 Action을 선택하여 행동한다. 이렇게 해서 Action을 하고 Reward와 다음 State를 받은 뒤, Q-function을 max로하는 action으로 Q-function을 update한다(여기서는 target policy로 동작)

<img src="https://i.stack.imgur.com/wmFny.png" width="800" /> <br>
(  https://i.stack.imgur.com/wmFny.png )

<br>

# 4. Summary

1. On-policy & Off-policy

   - 1) On-policy : 학습 $$=$$ 행동 

     - 현재 최고면 바로 선택! ( 과거의 경험 기억 X )

     - **ex) SARSA**

       $$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t}))$$.

   - 2) Off-policy : 학습 $$\neq$$ 행동

     - 현재 최고를 바로 선택하는 것이 아님 (과거의 경험 활용)

     - **ex) Q-learning**

       $$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(R_{t+1} + \gamma \underset{\alpha^{'}}{\cdot \text{max}} Q(S_{t+1},\alpha^{'}) - Q(S_{t},A_{t}))$$.

2. Off-policy

   - 1) Behavior policy : $$\mu$$
   - 2) Target policy : $$\pi$$

   