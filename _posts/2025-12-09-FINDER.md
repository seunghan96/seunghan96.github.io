---
title: FinDER; Financial Dataset for Question Answering and Evaluating Retrieval-Augmented Generation
categories: [TS]
tags: []
excerpt: ICLRW 2025

---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# FinDER: Financial Dataset for Question Answering and Evaluating Retrieval-Augmented Generation

<br>

# 1. Introduction

## (1) Motivaton

[Financial 도메인] 

- **정확한 Information Retrieval(IR)**이 매우 중요!


[Financial 데이터] 

- **복잡&동적**이며, 문서(10-K), 테이블, narrative 등 다양한 형태가 혼합

  $$\rightarrow$$ IR 난이도가 높음!

[Financial 질의] 

- 보통 **짧고, 모호하고, 약어·jargon·acronym**이 많이 포함
- e.g., *“Recent CAGR in MS trading revenue”*처럼 회사명 (*MS*)조차 축약된 경우가 많음!

<br>

## (2) 기존 dataset & 방법론

**[기존 QA datasets]** 

미리 제공된 고정 context를 기반

$$\rightarrow$$ 실제 Financial QA의 retrieval 난이도를 반영하지 못함!

<br>

**[LLM (w/o RAG)]** 

Financial QA를 **closed-book**으로 처리할 경우 정확도가 매우 낮음

- 단순히 **context window를 늘리는 방식**은 비효율적이고 비용이 큼!

<br>

## (3) FinDER

Solution: **"RAG"**이 Financial QA가 필수적!!

$$\rightarrow$$ **FinDER**는 이러한 문제를 해결하기 위해 설계된 dataset

<br>

현실적인 세팅

- (1) Ambiguous query
- (2) Realistic financial search behavior
- (3) Expert-grounded evidence & answers

<br>

## (4) Main Contributions

- 전문가가 만든 **5,703개의 Q-E-A triplets** 제공
  - (Q)uery
  - (E)vidence
  - (A)nswer

- 금융 QA benchmark 중 **가장 높은 수준의 query complexity** 포함
- SoTA retrievers/LLMs 성능 평가

<br>

# 2. Related Works

## (1) Financial QA Datasets

기존 Financial QA dataset

- 특정 reasoning task에는 강하지만, **retrieval 자체를 핵심 문제로 다루지 않음!**

- **핵심 문제**: 대부분의 datasets는

  1. **잘 정제된 질문**

  2. **명확한 context**,

  3. **모호성 없는 쿼리**

     를 기반으로 만들어져 **real-world 금융 IR 난이도**를 반영하지 못함!

<br>

Proposal: FinDER

- **Ambiguous, brief, acronym-heavy** real search queries 사용
  - e.g., Ambiguous: *“AAPL segment margin YoY?”*
  - e.g., Brief: *“TSLA delivery numbers”*
  - e.g., Acronym-heavy (약어·도메인 jargon 엄청 많음): MS=Morgan Stanley, EPS=Earnings Per Share
- Annotated **ground-truth evidence**를 제공
- Retrieval 난이도를 dataset 설계의 중심으로 둠.

<br>

## (2) RAG in Finance

- **[RAG]** LLM에 **external documents**를 retrieval하여 context로 제공

  $$\rightarrow$$ Hallucination, outdated knowledge 문제를 완화하는 핵심 기술.

- **[Financial 도메인]** **정보 업데이트 속도가 빠르고**, 전문 용어가 많아 LLM 단독으로는 신뢰성 부족

  → Retrieval 단계 품질이 금융 QA 성능을 크게 좌우함

- 최근 연구들의 주요 주제?

  - **Document indexing**: 검색 효율을 높이기 위해 문서를 구조화해 빠르게 조회할 수 있는 형태로 저장하는 과정
  - **Chunking**: 긴 문서를 검색 가능한 작은 단위(문단·섹션 등)로 분할해 retrieval 성능을 높이는 기법
  - **Reranking**: 1차 retrieval 결과를 더 정교한 모델(LLM 등)로 재정렬해 가장 관련성 높은 문서를 상위에 올리는 단계
  - **Query expansion**: 원래 질의를 synonym·관련 용어·도메인 knowledge로 확장해 retrieval 정확도를 높이는 기법.
  - **Embedding-based retrieval**: 문서와 질의를 벡터 공간에 임베딩해 유사도 기반으로 관련 문서를 검색하는 방식.

  $$\rightarrow$$ 모두 RAG의 성능을 결정하는 critical pipeline.

- Retrieval 성능이 낮으면 Generation 모델도 실패!!

  → 금융 QA에서는 **retrieval이 곧 성능의 upper bound**.

<br>

FinDER

- RAG 평가를 위해 만들어진 최초의 규모 있는 **domain-specific benchmark** 
- Retrieval 모델이 ambiguous query를 어떻게 파싱·해석하는지 평가할 수 있도록 설계

- e.g., Figure 1)

  ![figure2](/assets/img/ts/img830.png)

  - *“MS trading revenue”*처럼 **ambiguous query**를 요구하는 경우 종종 O

  - System이 먼저 **MS → Morgan Stanley**를 해석해야 올바른 paragraph를 찾을 수 있음!

    → 기존 datasets에서 등장하지 않는 고난도 retrieval reasoning


<br>



# 3. FinDER Dataset

![figure2](/assets/img/ts/img888.png)

<br>

## (1) Overview

- **5,703개의 query–evidence–answer** triplets로 구성된 금융 QA용 RAG benchmark
- 기존 QA dataset과의 차이점?
  - **predefined context 없음** → 모델이 **retrieval 자체를 수행해야 함**.
- Query는 실제 금융 전문가의 검색 행태를 반영해 **짧고, ambiguous하고, acronym-heavy**함.
- Evidence는 10-K annual report에서 **전문가가 수동으로 선택한 문단**.
- FinDER의 목적: **retrieval 난이도 + generation 정확도**를 동시에 평가하는 현실적 benchmark.

<br>

## (2) Components

FinDER는 아래 4개의 구성요소:

1. **Documents**: S&P 500 기업의 최신 10-K annual report (총 490개).
2. **Questions**: Hedge fund·IB·PM 등 금융 전문가가 실제 사용한 search query.
3. **Ground-truth Evidence**: 문서에서 해당 질문을 해결하는 데 필요한 문단/테이블.
4. **Answers**: 전문가가 evidence 기반으로 작성·검증한 정확한 답변.

<br>

## (3) Collection

- Query 수집: 금융 Q&A 플랫폼에서 전문가가 남긴 실제 질문 추출.
- Company filtering: S&P 500 기준으로 매칭, 중복 제거, 질문 없는 기업 제외.
- Evidence filtering: 10-K에서 **관련 evidence가 없는 질문은 제거**하여 품질 확보.
- 최종적으로 **7,000 → 5,703개의 정제된 QA pair** 도출.

<br>

## (4) Annotation Process

- Annotation은 **투자은행 애널리스트 + CPA** 두 명의 전문가가 수행.
- 단계별 절차:
  1. 두 명이 **독립적으로 evidence 후보**를 수집
  2. evidence 기반으로 각각 answer 초안 생성
  3. GPT-o1으로 **format standardization** (내용은 그대로 유지)
  4. 두 annotator가 cross-review하여 불일치 해결
