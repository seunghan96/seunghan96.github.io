---
title: \[Explicit DGM\] 02. Auto Encoders & Latent Variable 
categories: [GAN]
tags: [GAN]
excerpt: KAIST 문일철 교수님 강의 참고
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [Explicit DGM] 02. Auto Encoders & Latent Variable 

( 참고 : KAIST 문일철 교수님 : 한국어 기계학습 강좌 심화 2)

<br>

## Contents

1. Inference with Latent Variables
2. Lower bound & KL Divergence
3. Optimizing the Lower Bound

<br>

# 1. Inference with Latent Variables

Notation

- $$\{X, Z\}$$ : set of variables
  - $$X$$ : observed
  - $$Z$$ : hidden ( = latent )

- $$\theta:$$ parameters ( of distributions )

<br>

Likelihood ( $$P(X \mid \theta)$$ )

- marginalize w.r.t $$Z$$ ( latent variable ) :
  - ( original ) $$P(X \mid \theta)=\sum_{Z} P(X, Z \mid \theta)$$ 
  - ( log ) $$\ln P(X \mid \theta)=\ln \left\{\sum_{Z} P(X, Z \mid \theta)\right\}$$

<br>

알고자 하는 것 : $$Z$$ & $$\theta$$

- $$P(X \mid \theta)=\Sigma_{Z} P(X, Z \mid \theta)$$을 maximize 하는 방향으로! 

<br>

# 2. Lower bound & KL Divergence

( 선수 지식 : Concave/Convex function, Jensen's Inequality )

<br>

**Evidence**, 즉 주어진 data에 대한 **log probability**는 다음과 같다.

- $$\ln P(E)=\ln \sum_{H} P(H, E)=\ln \sum_{H} Q(H \mid E) \frac{P(H, E)}{Q(H \mid E)}$$.
  - 임의로 새로운 distribution $$Q(H \mid E) $$ 를 도입한다

<br>

위 식은, Jensen's Inequality에 의해, 아래와 같은 lower bound를 가지게 된다.

$$\begin{aligned}
\ln \sum_{H} Q(H \mid E) \frac{P(H, E)}{Q(H \mid E)} &\geq \sum_{H} Q(H \mid E) \ln \left[\frac{P(H, E)}{Q(H \mid E)}\right] \\
&=\sum_{H} Q(H \mid E) \ln P(H, E)-Q(H \mid E) \ln Q(H \mid E) \\
&=\sum_{H} Q(H \mid E)\{\ln P(E \mid H)+\ln P(H)\}-Q(H \mid E) \ln Q(H \mid E) \\
&=\sum_{H} Q(H \mid E) \ln P(E \mid H)-Q(H \mid E) \ln  \frac{Q(H \mid E)}{ P(H)} \\
&=E_{Q(H \mid E)} \ln P(E \mid H)-K L(Q(H \mid E) \| P(H))
\end{aligned}$$.

<br>

Lower Bound : $$E_{Q(H \mid E)} \ln P(E \mid H)-K L(Q(H \mid E)  \mid \mid  P(H))$$

- 위 식은, (1) expectation term 과 (2) KL divergence term으로 분해 된다.

- 우리는 위 lower bound를 maximize 함으로써 최적화를 진행한다.

- 하지만, (1)은 데이터에 따라 의존하는 부분이므로,

  곧 (2) KL divergence ( 특징 : non-negative )을 **minimize** 하는 방향으로 최적화를 하면 된다.

<br>

# 3. Optimizing the Lower Bound

위 2에서 우리는 아래와 같은 lower bound를 구했었다.

- ( 표현 1 ) $$\sum_{H} Q(H \mid E) \ln P(H, E)-Q(H \mid E) \ln Q(H \mid E)$$.
- ( 표현 2 ) $$E_{Q(H \mid E)} \ln P(E \mid H)-K L(Q(H \mid E)  \mid \mid  P(H))$$

<br>

우리가 새롭게 도입한 variational distribution $$Q$$의 파라미터를 $$\lambda$$라고 하고, $$P$$의 파라미터를 $$\theta$$라 하고  식을 재정리하면 아래와 같다.

- ( 표현 1 ) $$L(\lambda, \theta)=\sum_{H} Q(H \mid E, \lambda) \ln P(H, E \mid \theta)-Q(H \mid E, \lambda) \ln Q(H \mid E, \lambda)$$.
- ( 표현 2 ) $$L(\lambda, \theta)=E_{Q(H \mid E, \lambda)} \ln P(E \mid H, \theta)-K L(Q(H \mid E, \lambda)  \mid \mid  P(H \mid \theta))$$

<br>

그러면, 어떠한 $$\lambda$$ 가 좋은 값일까?

- goal : (1) = (2) = (3)
  - (1) maximizie evidence
  - (2) maximize lower bound ( = ELBO )
  - (3) minimize KL-divergence
- KL-divergence를 minimize하기 위해서는, $$Q(H \mid E, \lambda)=P(H \mid E, \theta)$$여야 한다. 그럴 경우, lower bound에 도달한다.

<br>

EM-algorithm

- 위를 활용한 방법이 바로 EM ( Expectation - Maximization ) 알고리즘이다.
- EM 알고리즘은 아래의 2 step으로 이루어진다.
  - ( E-step ) $$\lambda^{t+1}=\operatorname{argmax}_{\lambda} L\left(\lambda^{t}, \theta^{t}\right)$$
  - ( M-step ) $$\theta^{t+1}=\operatorname{argmax}_{\theta} L\left(\lambda^{t+1}, \theta^{t}\right)$$

<br>