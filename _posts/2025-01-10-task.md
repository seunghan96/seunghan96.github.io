---
title: MLLM Benchmarks
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: MME, MMMU, GQA, ChartQA, POPE, NoCaps, TextVQA
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# MLLM Benchmarks

1. **MME**
   - **Multi-modal Model Evaluation (MME)**
   - 대형 멀티모달 모델의 성능을 평가하기 위한 벤치마크
   - 다양한 멀티모달 태스크(예: 이미지-텍스트 이해, 논리적 추론)에 대해 모델의 한계를 분석
2. **MMMU**
   - **MultiModal Math Understanding (MMMU)**
   - 멀티모달 모델의 수학적 이해 능력을 평가하는 데이터셋
   - 이미지(수식, 그래프)와 텍스트가 결합된 문제를 해결하는 능력을 측정
3. **GQA**
   - **Graph Question Answering (GQA)**
   - 구조화된 장면 그래프(Scene Graph)를 활용한 비주얼 QA 데이터셋
   - 객체 간의 관계를 이해하고 논리적으로 답변하는 능력을 테스트
4. **ChartQA**
   - 차트(Chart)와 표(Table)를 포함하는 QA 데이터셋
   - 시각적 데이터를 해석하고 질문에 답변하는 능력을 평가
5. **POPE**
   - **Position-aware Pretraining (POPE)**
   - OCR 기반의 비주얼 문서 이해를 위한 사전 학습 데이터셋
   - 문서 내 텍스트의 위치 정보(position)를 활용하는 모델 성능 평가
6. **NoCaps**
   - **Novel Object Captioning (NoCaps)**
   - 기존 캡션 데이터셋에 없는 새로운 객체(novel objects)를 캡션에 포함하는 데이터셋
   - Open Vocabulary Image Captioning 성능을 평가
7. **TextVQA**
   - **Text-based Visual Question Answering (TextVQA)**
   - 이미지 내 OCR 텍스트를 활용하여 질문에 답하는 비주얼 QA 데이터셋
   - 일반적인 VQA 모델이 아닌, OCR과 자연어 이해를 결합한 모델 성능을 측정
