---
title: VLMo; Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: NeurIPS 2022
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

https://arxiv.org/pdf/2111.02358

<br>

# VLMo

Unified **Vision-Language pretrained Model (VLMo)**

- **Dual** encoder & **Fusion** encoder
- **Mixture-of-Modality-Experts (MOME)** Transformer
  - Each block contains a pool of **"modality-specific"** experts & a shared selfattention layer. 
- Can be fine-tuned as ...
  - **Fusion encoder**:  For vision-language classification tasks
  - **Dual encoder**: For efficient image-text retrieval
- Stagewise pre-training strategy
  - Not only **image-text pairs**
  - But also **image-only** & **text-only data**

<br>

![figure2](/assets/img/llm/img646.png)
