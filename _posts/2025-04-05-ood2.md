---
title: OOD detectionForecasting
categories: [TS, CV, TAB]
tags: []
excerpt: 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# OOD detection

1. 신뢰도 기반 방법 (Confidence-based Methods)
2. ODIN (Liang et al., ICLR 2018)
3. Energy-based OOD detection (Liu et al., NeurIPS 2020)
4. Mahalanobis distance
5. Generative 모델 기반의 OOD detection
6. SSL 기반 OOD detection

<br>

# 1. 신뢰도 기반 방법 (Confidence-based Methods)

**딥러닝 분류 모델의 출력값 (softmax 또는 logit)** 기반

Key idea: *OOD 입력은 모델이 낮은 확신(confidence)을 가지므로, 이걸로 구분하자!*

<br>

### 주요 방법 3가지

1. **Maximum Softmax Probability (MSP)**
2. **Maximum Logit (Max Logit Score)**

<br>

## (1) **Maximum Softmax Probability (MSP)**

https://arxiv.org/pdf/1610.02136 (*Hendrycks & Gimpel, 2017*)

- Softmax 출력에서 **가장 높은 확률값**을 사용
  - Ex)  `[0.9, 0.05, 0.05]`을 출력하면 0.9가 MSP
- Intuition
  - ID 데이터: **MSP가 큼**
  - OOD 데이터: **MSP가 작음**
- How?  `max softmax < τ → OOD`

- 장 & 단점

  - 장점: 추가 학습 없이 바로 사용 가능

  - 단점: 일반적으로 softmax가 과하게 confident하게 출력

<br>

## (2) **Maximum Logit (Max Logit Score)**

- (softmax 이전 단계인) **logit 값 중 가장 큰 값**을 사용
  - Ex)  모델의 logit이 `[3.2, -0.7, 1.1]`이라면 score는 `3.2`
- Motivation: Softmax는 비선형 .. 정보가 왜곡될 수 있음!

<br>

## (3) **Negative Entropy (−H(p))**

- Softmax 출력을 확률 분포 `p`로 보고, 그 엔트로피를 사용
  - $$H(p)=-\sum_i p_i \log p_i$$
- Entropy 높다 $$\rightarrow$$ 불확실한 예측
  - ID는 low entropy
  - OOD는 high entropy
- OOD score: `−H(p)`
  - 높은 OOD score = 낮은 entropy  $$\rightarrow$$ ID
  - 낮은 OOD score = 높은 entropy  $$\rightarrow$$ OOD

<br>

### Code

```python
import torch
import torch.nn.functional as F

def get_ood_score(logits, method='msp'):
    if method == 'msp':
        probs = F.softmax(logits, dim=1)
        return probs.max(dim=1).values
    elif method == 'maxlogit':
        return logits.max(dim=1).values
    elif method == 'entropy':
        probs = F.softmax(logits, dim=1)
        entropy = -(probs * probs.log()).sum(dim=1)
        return -entropy
```

<br>

### 한계점

1. **Softmax overconfidence 문제**
   - 신경망은 OOD 입력에도 **높은 확률로 잘못된 클래스**를 선택할 수 있음

2. **분포 간 간극이 작은 경우** 성능 저하
   - OOD가 ID와 시각적으로 유사한 경우 구분 어려움

3. **Temperature scaling**이나 다른 보정 없이 단순 threshold는 신뢰도 낮음

<br>

### 💡 정리

| 방법      | 계산 방식          | 특징                      |
| --------- | ------------------ | ------------------------- |
| MSP       | max softmax        | 간단함, baseline          |
| Max Logit | max(logits)        | softmax보다 더 raw한 정보 |
| −Entropy  | 예측 불확실성 활용 | 확률 분포의 분산 활용     |

<br>

# 2. ODIN (Liang et al., ICLR 2018)

- https://arxiv.org/pdf/1706.02690

<br>

DL 기반 OOD detection에서 매우 대표적인 초기 기법

$$\rightarrow$$ 기존 MSP보다 **훨씬 높은 성능**!

<br>

## (1) Key Idea

기존의 softmax 기반 방법 (예: MSP)의 한계점

1. **Softmax는 과하게 confident한 값을 낼 수 있음**
2. **ID/OOD 사이의 차이가 미묘할 때, 단순히 softmax 값만으로 구분하기 어려움**

<br>

Solution:

1. Temperature scaling
2. Input perturbation

<br>

## (2) 두 가지 구성 요소

### a) Temperature Scaling

$$\operatorname{Softmax}_T\left(z_i\right)=\frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$$.

- $$T\rightarrow \infty$$ :  Uniform distn.
- $$T\rightarrow 0$$ :  Delta func.

<br>

### b) Small Input Perturbation

입력 이미지에 **작은 방향성 변화 (gradient sign 기반)를 더함**

$$\tilde{x}=x-\epsilon \cdot \operatorname{sign}\left(-\nabla_x \log p(y \mid x ; T)\right)$$.

- OOD: perturbation에 민감
- ID: perturbation에 robust

→ Perturbation 이후 softmax max 값을 비교!

<br>

## (3) OOD score 계산 과정

1. 입력 $$x$$ 에 perturbation $$\epsilon$$ 추가 $$\rightarrow \bar{x}$$
2. 모델에 $$\tilde{x}$$를 넣고 logit 출력
3. softmax를 temperature $$T$$ 적융
4. max softmax score를 OOD score로 사용

<br>

## (4) Hyperparameters

- $$\epsilon$$ : Perturbation의 크기 (보통 0.001 ~ 0.004 사이)
- $$T$$ : Temperature (보통 1000까지도 사용)

```
We use a separate OOD validation dataset for hyperparameter selection, which is independent from the OOD test datasets. ~
```

<br>

### Code

```python
def odin_score(model, x, T=1000, eps=0.001):
    x.requires_grad = True
    logits = model(x)
    logits = logits / T
    probs = F.softmax(logits, dim=1)
    max_prob, pred = probs.max(dim=1)

    # Gradient of the negative log-prob of the predicted class
    loss = -F.log_softmax(logits, dim=1)[range(len(pred)), pred]
    loss = loss.sum()
    loss.backward()

    # Small perturbation in the direction that increases confidence
    x_perturbed = x - eps * x.grad.data.sign()
    with torch.no_grad():
        logits_perturbed = model(x_perturbed) / T
        score = F.softmax(logits_perturbed, dim=1).max(dim=1).values
    return score
```

<br>

### 장점 & 단점

- 장점: 기존 MSP보다 훨씬 나은 성능. 간단한 아이디어로 효과적
- 단점: 입력 gradient 계산 필요 → **추론 시간 증가**

<br>

### Summary

- **(1) Temperature Scaling**:  softmax 분포를 flatten → OOD confidence 낮추기
- **(2) Input Perturbation**: OOD 입력이 perturbation에 더 민감함을 이용

<br>

### Future works

- ODIN은 이후 나온 많은 방법들의 기반이 되었음!
- 특히 energy-based 방법이나 Mahalanobis distance 기반 방법들도, ODIN의 두 구성요소 중 일부를 응용

<br>

# 3. Energy-based OOD detection (Liu et al., NeurIPS 2020)

## (1) Key Idea

- Softmax 기반 방법들보다 더 정교하게, ***logit 정보***를 활용
- Logit 전체 정보를 반영한 **에너지 함수(Energy Function)**를 기반으로 OOD score를 계산!

<br>

Intuition: ***왜 더 좋은가?***

- softmax는 항상 0~1로 정규화되기 때문에, 때로는 OOD 입력도 **높은 max probability**를 갖는 경우가 있음 
  - (즉, overconfident).
- 이에반해, energy는 softmax 분포의 shape까지 고려하므로, 더 **정밀한 confidence 표현**이 가능!

<br>

How?  `E(x) > τ → OOD`

<br>

## (2) Energy function

Softmax의 numerator만 사용한 형태

$$E(x)=-T \cdot \log \sum_{i=1}^C \exp \left(z_i / T\right)$$

- $$z_i$$: Logit 값
- $$C$$: 클래스의 개수

<br>

에너지 ($$E(x)$$)의 특징

- **logit 전체를 반영**함 (max만 보는 MSP보다 정보 풍부)
- 값이 **낮을수록 ID일 가능성 높음**, **높을수록 OOD일 가능성 높음**
- (참고: 에너지가 높다는 건 softmax 분포가 flat하다는 의미)

<br>

## (3) MSP vs. Energy

| 항목       | MSP                     | Energy                      |
| ---------- | ----------------------- | --------------------------- |
| 사용 정보  | max logit만 사용        | 모든 logit 사용             |
| 추론 방식  | softmax 후 max          | softmax 전 log-sum-exp      |
| score 의미 | 확신 정도 (높을수록 ID) | 불확실성 정도 (낮을수록 ID) |
| 장점       | 간단함                  | 정보량 많고 robust함        |
| 단점       | 정보 손실 있음          | 계산 조금 더 복잡           |

<br>

### Code

```python
def energy_score(logits, T=1.0):
    # logits: [B, C]
    energy = -T * torch.logsumexp(logits / T, dim=1)
    return energy  # 낮을수록 ID, 높을수록 OOD
```

<br>

## (4) 추가 활용: Energy + ODIN-style Perturbation

Liu et al. 논문: ODIN에서처럼 **입력 perturbation**도 함께 쓰면 더 좋다고 제안!

$$\rightarrow$$ 즉, ***energy 함수도 gradient에 민감***하므로 ODIN처럼 다음을 적용할 수 있음

```python
# E(x)를 기준으로 입력에 perturbation
grad = torch.autograd.grad(energy.sum(), x)[0]
x_perturbed = x + eps * grad.sign()
```

<br>

## (5) Entropy vs. Energy

- 공통점: 둘 다 **softmax 분포의 불확실성**을 활용
  - MSP처럼 max 값 하나만 보는 것보다 훨씬 더 **분포 전체를 반영**
  - **확신(confidence)이 낮을수록** OOD로 판단하는 구조
- 차이점: **OOD에 얼마나 민감하게 반응하는지**

<br>

| 항목     | Entropy                      | Energy                                          |
| -------- | ---------------------------- | ----------------------------------------------- |
| 입력     | softmax 확률                 | logit 원값                                      |
| scale    | 항상 0~log(C) 범위 (bounded) | unbounded (logit 크기에 따라 다양)              |
| gradient | softmax의 gradient           | logit의 gradient (softmax보다 안정적)           |
| 표현력   | 확률적 퍼짐만 반영           | logit 값 크기 + 퍼짐 모두 반영                  |
| 장점     | 직관적, 간단                 | 더 정교하고 표현력 높음                         |
| 단점     | bounded → sensitivity 낮음   | logit scale 민감 → normalization 필요할 수 있음 |

<br>

# 4. Mahalanobis distance

(Softmax나 energy처럼) **logit 기반의 confidence**가 아니라...

$$\rightarrow$$ **"Feature space에서의 통계적 거리"**를 이용해서 OOD 여부를 판단!

https://papers.nips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf (NeurIPS 2018)

<br>

## (1) Key Idea

- feature embedding은 클래스마다 다른 양상을 보임!
- 따라서,
  - (1) embedding의 분포를 **클래스별 Gaussian 분포**로 근사
  - (2) 새로운 입력의 feature가 **어느 클래스 중심에서 얼마나 먼가?**를 측정해서 OOD 여부를 판단함
- 한 줄 요약: **Mahalanobis distance**를 활용해 *“어느 분포에도 잘 속하지 않는다”*는 것을 수치화

<br>

## (2) Mahalanobis Distance

$$D_M(x)=\sqrt{(f(x)-\mu)^T \Sigma^{-1}(f(x)-\mu)}$$.

- $$f(x)$$ : 입력 x 의 feature (에: penultimate layer 출력)
- $$\mu$$ : 득정 클래스의 평균 feature
- $$\Sigma$$ : 클래스 공댱의 공분산 헝렬

$$\rightarrow$$ 일반적인 Euclidean distance보다 ***feature correlation까지 고려***

<br>

How? 

- 작을수록 ID
- 클수록 OOD

<br>

## (3) Procedure

1. **사전 학습된 분류 모델** 준비 (예: ResNet-50)

2. Training set의 feature를 뽑아 클래스별 평균 & 공분산 계산

3. Test set의 $$x$$ 에 대해  $$f(x)$$ 추출

4. Mahalanobis distance를 모든 클래스 $$c$$에 대해 계산한 뒤 **최솟값** 사용

   - $$\operatorname{score}(x)=-\min _c D_M\left(f(x), \mu_c\right)$$.

   → 음수를 붙여서 **높을수록 ID**, 낮을수록 OOD

<br>

## (4) Mahalanobis vs Softmax-based

| 항목           | Mahalanobis             | MSP / Energy                |
| -------------- | ----------------------- | --------------------------- |
| 기반           | Feature space (은닉층)  | Logit space                 |
| 특징           | 통계적 거리             | 확률/에너지 기반 confidence |
| 설명력         | 통계적 해석 명확        | 모델 confidence 해석        |
| 추가 학습 필요 | ❌ 없음                  | ❌ 없음                      |
| 계산비용       | 공분산 역행렬 계산 필요 | 비교적 간단                 |

<br>

### Code

```python
# assume: feature_extractor, class_means, inv_cov (Σ⁻¹)

def mahalanobis_score(x):
    f = feature_extractor(x)  # feature: [B, D]
    distances = []
    for mu in class_means:  # list of [D]
        delta = f - mu
        d = torch.einsum('bi,ij,bj->b', delta, inv_cov, delta)
        distances.append(d)
    return -torch.stack(distances, dim=1).min(dim=1).values  # high = ID-like
```

<br>

## (5) 추가 요소

아래의 요소들을 추가할 수 있음!

- **입력 perturbation** (ODIN처럼 gradient로 input 조금 변경)
- **multi-layer fusion**: 하나의 layer feature만 쓰지 않고 여러 layer의 Mahalanobis distance를 ensemble

<br>

## (6) 장점 & 단점

장점

- **confidence 기반 logit의 한계**를 벗어남
- Adversarial attack까지 robust하게 방어 가능
- **해석력**이 높고, Gaussian class-conditional 가정이 직관적

<br>

단점

- **공분산 계산이 필요** $$\rightarrow$$ 차원이 높으면 느릴 수 있음
- 클래스 수 많거나 모델이 복잡할수록 계산 비용 증가
- 학습이 아니라 post-hoc 방식이긴 하지만 **feature extractor**가 중요

<br>

## (7) Summary

| 항목      | 설명                                           |
| --------- | ---------------------------------------------- |
| 접근 방식 | 클래스별 feature 분포(Gaussian) 기반 거리 측정 |
| 핵심 함수 | Mahalanobis distance                           |
| 입력      | 은닉층 feature                                 |
| 결과      | 거리 작으면 ID, 크면 OOD                       |
| 확장      | multi-layer, perturbation                      |

<br>

# 5. Generative 모델 기반의 OOD detection

지금까지 봤던 **discriminative (분류 기반)** 방법과는 달리...

$$\rightarrow$$ 입력 데이터를 **직접 모델링**하는 접근!

<br>

## (1) Key Idea

- **ID 데이터**를 잘 설명하는 **생성 모델**을 학습해놓으면...
- **OOD 데이터**는 **"low likelihood를 가질 것"**이라고 기대!

→ 학습된 생성 모델이 입력 $$x$$에 대해 계산한 log-likelihood $$\log p(x)$$ 를 이용해서 ..

- **확률이 낮은 샘플 = OOD**
- **확률이 높은 샘플 = ID**

라고 판단함

<br>

## (2) 대표 모델 종류

### a) **Normalizing Flow**

- $$p(x)=p(z) \mid \operatorname{det} \frac{\partial z}{\partial x} \mid ^{-1}$$.
- Exact likelihood 계산 가능
- Ex) Glow, RealNVP, Flow++, etc.

<br>

### b) **Autoregressive Model**

- $$p(x)=\prod_i p\left(x_i \mid x_{<i}\right)$$.
- 각 픽셀(또는 차원)을 순차적으로 예측해 전체 분포 모델링
- Ex) PixelCNN, WaveNet

<br>

### c) **Variational Autoencoder (VAE)**

- $$x \sim p(x \mid z)$$.
- ELBO (Evidence Lower Bound)를 log-likelihood 근사로 사용

<br>

### Code (NF)

```python
# x: input image or data
log_likelihood = flow_model.log_prob(x)
score = -log_likelihood  # 낮을수록 OOD 가능성 높음
```

→ 이 값을 thresholding 해서 OOD 여부 판단

<br>

## (3) 한계점: "Likelihood ≠ Semantic ID-ness"

- 예: CIFAR-10로 학습한 Flow 모델이 SVHN보다 CIFAR-10에 **낮은 likelihood**를 주는 현상 (Nalisnick et al., 2019)
- Why? Likelihood는 **저차원 통계적 구조**(텍스처, 해상도 등)에 민감
  - 즉, 우리가 원하는 "이게 이 분포에서 의미 있는 샘플인가?"는 반영하지 않기 때문

<br>

## (4) Solution

### a) **Likelihood Ratio**

- Flow + classifier를 같이 학습
  - class-conditional likelihood p(x∣y)와 class-marginal p(x) 비교한다!
- $$\text{score}(x) = \frac{p(x|y)}{p(x)}$$.

<br>

### b) **Input complexity regularization**

- likelihood가 높은 **"쉬운"** 입력들을 penalize
- 예: ID보다 픽셀값이 평탄한 SVHN이 더 높은 likelihood 받는 문제를 보완

<br>

### c) **Latent space 활용 (e.g., VAE)**

- encoder의 latent space에서의 distance, reconstruction error 등을 OOD score로 사용

<br>

## (5) 장점 & 단점 

장점

| 항목        | 설명                                              |
| ----------- | ------------------------------------------------- |
| 학습        | label 없이 가능 (unsupervised)                    |
| 적용 범위   | 이미지, 오디오, 텍스트 등 다양한 입력             |
| 해석 가능성 | 확률 모델이므로 log-likelihood의 직관적 의미 있음 |

<br>

단점

| 항목                     | 설명                                           |
| ------------------------ | ---------------------------------------------- |
| semantic OOD에 취약      | 진짜 의미의 "분포 바깥"을 감지 못할 수 있음    |
| 이상하게 높은 likelihood | SVHN처럼 ID보다 더 높은 likelihood 받는 경우   |
| 계산 비용                | 특히 high-dim 이미지에서는 느림 (Flow, VAE 등) |

<br>

# 6. SSL 기반 OOD detection

Supervised signal 없이도 **OOD에 강인한 feature**를 뽑을 수 있다!

<br>

## (1) Key Idea

- SSL을 통해 I(D 데이터에 특화된) feature space를 학습

- Test input이 그 공간에서 **일반적인 ID feature와 유사한가?**를 판단해서 OOD 여부 판단

  ( 학습된 feature extractor를 고정 )

<br>

Summary

- Training: label (X)
- Detection: feature similarity / classifier confidence 등

<br>

## (2) Examples

- Rotation prediction, Jigsaw 등 Pretext Task 기반
- Contrastive Learning 기반 (예: SimCLR, MoCo 등)

<br>

## (3) OOD Score 예시:

- Nearest neighbor distance (k-NN)
- Feature 중심과의 cosine similarity
- Linear classifier의 softmax confidence

<br>

## (4) Examples

| 논문           | 방식                                   | 설명                                                     |
| -------------- | -------------------------------------- | -------------------------------------------------------- |
| CSI (2020)     | Contrastive + augmentation + one-class | Self-supervised contrastive learning + novel OOD scoring |
| SSD (2021)     | Self-supervised detection              | Rotation, jigsaw 등 pretext loss로 OOD 감지              |
| KNN-OOD (2020) | k-NN in contrastive space              | Nearest neighbor distance로 score 계산                   |

<br>

## (5) Procedure

1. ID 데이터만 사용해서 SSL로 encoder $$f(x)$$ 학습
2. 학습된 encoder를 고정한 채, **ID feature 저장**
3. Test input $$x$$ 에 대해 feature $$f(x)$$ 계산
4. OOD score 계산:
   - 가장 가까운 ID feature와의 거리 (k-NN)
   - cosine similarity
   - classifier를 붙였다면 softmax-based score

<br>

## (6) 장점 & 단점

장점

| 항목                    | 설명                                   |
| ----------------------- | -------------------------------------- |
| label 불필요            | ID 데이터만으로 학습 가능              |
| generalization          | 다양한 downstream task에서도 활용 가능 |
| 다양한 입력에 적용      | 이미지, 텍스트, 시계열 등 범용성 있음  |
| 다른 방식들과 결합 가능 | MSP, ODIN 등과 hybrid 가능             |

<br>

단점

| 항목                     | 설명                                             |
| ------------------------ | ------------------------------------------------ |
| representation 품질 의존 | contrastive 학습이 잘 안 되면 OOD 성능도 저하    |
| score 기준 설계 필요     | distance, classifier 등 추가 설계 요소 필요      |
| feature drift 위험       | training/test domain 차이 발생 시 성능 저하 가능 |

