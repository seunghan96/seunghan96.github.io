---
title: OOD detectionForecasting
categories: [TS, CV, TAB]
tags: []
excerpt: 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# OOD detection

1. ì‹ ë¢°ë„ ê¸°ë°˜ ë°©ë²• (Confidence-based Methods)
2. ODIN (Liang et al., ICLR 2018)
3. Energy-based OOD detection (Liu et al., NeurIPS 2020)
4. Mahalanobis distance
5. Generative ëª¨ë¸ ê¸°ë°˜ì˜ OOD detection
6. SSL ê¸°ë°˜ OOD detection

<br>

# 1. ì‹ ë¢°ë„ ê¸°ë°˜ ë°©ë²• (Confidence-based Methods)

**ë”¥ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ì˜ ì¶œë ¥ê°’ (softmax ë˜ëŠ” logit)** ê¸°ë°˜

Key idea: *OOD ì…ë ¥ì€ ëª¨ë¸ì´ ë‚®ì€ í™•ì‹ (confidence)ì„ ê°€ì§€ë¯€ë¡œ, ì´ê±¸ë¡œ êµ¬ë¶„í•˜ì!*

<br>

### ì£¼ìš” ë°©ë²• 3ê°€ì§€

1. **Maximum Softmax Probability (MSP)**
2. **Maximum Logit (Max Logit Score)**

<br>

## (1) **Maximum Softmax Probability (MSP)**

https://arxiv.org/pdf/1610.02136 (*Hendrycks & Gimpel, 2017*)

- Softmax ì¶œë ¥ì—ì„œ **ê°€ì¥ ë†’ì€ í™•ë¥ ê°’**ì„ ì‚¬ìš©
  - Ex)  `[0.9, 0.05, 0.05]`ì„ ì¶œë ¥í•˜ë©´ 0.9ê°€ MSP
- Intuition
  - ID ë°ì´í„°: **MSPê°€ í¼**
  - OOD ë°ì´í„°: **MSPê°€ ì‘ìŒ**
- How?  `max softmax < Ï„ â†’ OOD`

- ì¥ & ë‹¨ì 

  - ì¥ì : ì¶”ê°€ í•™ìŠµ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

  - ë‹¨ì : ì¼ë°˜ì ìœ¼ë¡œ softmaxê°€ ê³¼í•˜ê²Œ confidentí•˜ê²Œ ì¶œë ¥

<br>

## (2) **Maximum Logit (Max Logit Score)**

- (softmax ì´ì „ ë‹¨ê³„ì¸) **logit ê°’ ì¤‘ ê°€ì¥ í° ê°’**ì„ ì‚¬ìš©
  - Ex)  ëª¨ë¸ì˜ logitì´ `[3.2, -0.7, 1.1]`ì´ë¼ë©´ scoreëŠ” `3.2`
- Motivation: SoftmaxëŠ” ë¹„ì„ í˜• .. ì •ë³´ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŒ!

<br>

## (3) **Negative Entropy (âˆ’H(p))**

- Softmax ì¶œë ¥ì„ í™•ë¥  ë¶„í¬ `p`ë¡œ ë³´ê³ , ê·¸ ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©
  - $$H(p)=-\sum_i p_i \log p_i$$
- Entropy ë†’ë‹¤ $$\rightarrow$$ ë¶ˆí™•ì‹¤í•œ ì˜ˆì¸¡
  - IDëŠ” low entropy
  - OODëŠ” high entropy
- OOD score: `âˆ’H(p)`
  - ë†’ì€ OOD score = ë‚®ì€ entropy  $$\rightarrow$$ ID
  - ë‚®ì€ OOD score = ë†’ì€ entropy  $$\rightarrow$$ OOD

<br>

### Code

```python
import torch
import torch.nn.functional as F

def get_ood_score(logits, method='msp'):
    if method == 'msp':
        probs = F.softmax(logits, dim=1)
        return probs.max(dim=1).values
    elif method == 'maxlogit':
        return logits.max(dim=1).values
    elif method == 'entropy':
        probs = F.softmax(logits, dim=1)
        entropy = -(probs * probs.log()).sum(dim=1)
        return -entropy
```

<br>

### í•œê³„ì 

1. **Softmax overconfidence ë¬¸ì œ**
   - ì‹ ê²½ë§ì€ OOD ì…ë ¥ì—ë„ **ë†’ì€ í™•ë¥ ë¡œ ì˜ëª»ëœ í´ë˜ìŠ¤**ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŒ

2. **ë¶„í¬ ê°„ ê°„ê·¹ì´ ì‘ì€ ê²½ìš°** ì„±ëŠ¥ ì €í•˜
   - OODê°€ IDì™€ ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ê²½ìš° êµ¬ë¶„ ì–´ë ¤ì›€

3. **Temperature scaling**ì´ë‚˜ ë‹¤ë¥¸ ë³´ì • ì—†ì´ ë‹¨ìˆœ thresholdëŠ” ì‹ ë¢°ë„ ë‚®ìŒ

<br>

### ğŸ’¡ ì •ë¦¬

| ë°©ë²•      | ê³„ì‚° ë°©ì‹          | íŠ¹ì§•                      |
| --------- | ------------------ | ------------------------- |
| MSP       | max softmax        | ê°„ë‹¨í•¨, baseline          |
| Max Logit | max(logits)        | softmaxë³´ë‹¤ ë” rawí•œ ì •ë³´ |
| âˆ’Entropy  | ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± í™œìš© | í™•ë¥  ë¶„í¬ì˜ ë¶„ì‚° í™œìš©     |

<br>

# 2. ODIN (Liang et al., ICLR 2018)

- https://arxiv.org/pdf/1706.02690

<br>

DL ê¸°ë°˜ OOD detectionì—ì„œ ë§¤ìš° ëŒ€í‘œì ì¸ ì´ˆê¸° ê¸°ë²•

$$\rightarrow$$ ê¸°ì¡´ MSPë³´ë‹¤ **í›¨ì”¬ ë†’ì€ ì„±ëŠ¥**!

<br>

## (1) Key Idea

ê¸°ì¡´ì˜ softmax ê¸°ë°˜ ë°©ë²• (ì˜ˆ: MSP)ì˜ í•œê³„ì 

1. **SoftmaxëŠ” ê³¼í•˜ê²Œ confidentí•œ ê°’ì„ ë‚¼ ìˆ˜ ìˆìŒ**
2. **ID/OOD ì‚¬ì´ì˜ ì°¨ì´ê°€ ë¯¸ë¬˜í•  ë•Œ, ë‹¨ìˆœíˆ softmax ê°’ë§Œìœ¼ë¡œ êµ¬ë¶„í•˜ê¸° ì–´ë ¤ì›€**

<br>

Solution:

1. Temperature scaling
2. Input perturbation

<br>

## (2) ë‘ ê°€ì§€ êµ¬ì„± ìš”ì†Œ

### a) Temperature Scaling

$$\operatorname{Softmax}_T\left(z_i\right)=\frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$$.

- $$T\rightarrow \infty$$ :  Uniform distn.
- $$T\rightarrow 0$$ :  Delta func.

<br>

### b) Small Input Perturbation

ì…ë ¥ ì´ë¯¸ì§€ì— **ì‘ì€ ë°©í–¥ì„± ë³€í™” (gradient sign ê¸°ë°˜)ë¥¼ ë”í•¨**

$$\tilde{x}=x-\epsilon \cdot \operatorname{sign}\left(-\nabla_x \log p(y \mid x ; T)\right)$$.

- OOD: perturbationì— ë¯¼ê°
- ID: perturbationì— robust

â†’ Perturbation ì´í›„ softmax max ê°’ì„ ë¹„êµ!

<br>

## (3) OOD score ê³„ì‚° ê³¼ì •

1. ì…ë ¥ $$x$$ ì— perturbation $$\epsilon$$ ì¶”ê°€ $$\rightarrow \bar{x}$$
2. ëª¨ë¸ì— $$\tilde{x}$$ë¥¼ ë„£ê³  logit ì¶œë ¥
3. softmaxë¥¼ temperature $$T$$ ì ìœµ
4. max softmax scoreë¥¼ OOD scoreë¡œ ì‚¬ìš©

<br>

## (4) Hyperparameters

- $$\epsilon$$ : Perturbationì˜ í¬ê¸° (ë³´í†µ 0.001 ~ 0.004 ì‚¬ì´)
- $$T$$ : Temperature (ë³´í†µ 1000ê¹Œì§€ë„ ì‚¬ìš©)

```
We use a separate OOD validation dataset for hyperparameter selection, which is independent from the OOD test datasets. ~
```

<br>

### Code

```python
def odin_score(model, x, T=1000, eps=0.001):
    x.requires_grad = True
    logits = model(x)
    logits = logits / T
    probs = F.softmax(logits, dim=1)
    max_prob, pred = probs.max(dim=1)

    # Gradient of the negative log-prob of the predicted class
    loss = -F.log_softmax(logits, dim=1)[range(len(pred)), pred]
    loss = loss.sum()
    loss.backward()

    # Small perturbation in the direction that increases confidence
    x_perturbed = x - eps * x.grad.data.sign()
    with torch.no_grad():
        logits_perturbed = model(x_perturbed) / T
        score = F.softmax(logits_perturbed, dim=1).max(dim=1).values
    return score
```

<br>

### ì¥ì  & ë‹¨ì 

- ì¥ì : ê¸°ì¡´ MSPë³´ë‹¤ í›¨ì”¬ ë‚˜ì€ ì„±ëŠ¥. ê°„ë‹¨í•œ ì•„ì´ë””ì–´ë¡œ íš¨ê³¼ì 
- ë‹¨ì : ì…ë ¥ gradient ê³„ì‚° í•„ìš” â†’ **ì¶”ë¡  ì‹œê°„ ì¦ê°€**

<br>

### Summary

- **(1) Temperature Scaling**:  softmax ë¶„í¬ë¥¼ flatten â†’ OOD confidence ë‚®ì¶”ê¸°
- **(2) Input Perturbation**: OOD ì…ë ¥ì´ perturbationì— ë” ë¯¼ê°í•¨ì„ ì´ìš©

<br>

### Future works

- ODINì€ ì´í›„ ë‚˜ì˜¨ ë§ì€ ë°©ë²•ë“¤ì˜ ê¸°ë°˜ì´ ë˜ì—ˆìŒ!
- íŠ¹íˆ energy-based ë°©ë²•ì´ë‚˜ Mahalanobis distance ê¸°ë°˜ ë°©ë²•ë“¤ë„, ODINì˜ ë‘ êµ¬ì„±ìš”ì†Œ ì¤‘ ì¼ë¶€ë¥¼ ì‘ìš©

<br>

# 3. Energy-based OOD detection (Liu et al., NeurIPS 2020)

## (1) Key Idea

- Softmax ê¸°ë°˜ ë°©ë²•ë“¤ë³´ë‹¤ ë” ì •êµí•˜ê²Œ, ***logit ì •ë³´***ë¥¼ í™œìš©
- Logit ì „ì²´ ì •ë³´ë¥¼ ë°˜ì˜í•œ **ì—ë„ˆì§€ í•¨ìˆ˜(Energy Function)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ OOD scoreë¥¼ ê³„ì‚°!

<br>

Intuition: ***ì™œ ë” ì¢‹ì€ê°€?***

- softmaxëŠ” í•­ìƒ 0~1ë¡œ ì •ê·œí™”ë˜ê¸° ë•Œë¬¸ì—, ë•Œë¡œëŠ” OOD ì…ë ¥ë„ **ë†’ì€ max probability**ë¥¼ ê°–ëŠ” ê²½ìš°ê°€ ìˆìŒ 
  - (ì¦‰, overconfident).
- ì´ì—ë°˜í•´, energyëŠ” softmax ë¶„í¬ì˜ shapeê¹Œì§€ ê³ ë ¤í•˜ë¯€ë¡œ, ë” **ì •ë°€í•œ confidence í‘œí˜„**ì´ ê°€ëŠ¥!

<br>

How?  `E(x) > Ï„ â†’ OOD`

<br>

## (2) Energy function

Softmaxì˜ numeratorë§Œ ì‚¬ìš©í•œ í˜•íƒœ

$$E(x)=-T \cdot \log \sum_{i=1}^C \exp \left(z_i / T\right)$$

- $$z_i$$: Logit ê°’
- $$C$$: í´ë˜ìŠ¤ì˜ ê°œìˆ˜

<br>

ì—ë„ˆì§€ ($$E(x)$$)ì˜ íŠ¹ì§•

- **logit ì „ì²´ë¥¼ ë°˜ì˜**í•¨ (maxë§Œ ë³´ëŠ” MSPë³´ë‹¤ ì •ë³´ í’ë¶€)
- ê°’ì´ **ë‚®ì„ìˆ˜ë¡ IDì¼ ê°€ëŠ¥ì„± ë†’ìŒ**, **ë†’ì„ìˆ˜ë¡ OODì¼ ê°€ëŠ¥ì„± ë†’ìŒ**
- (ì°¸ê³ : ì—ë„ˆì§€ê°€ ë†’ë‹¤ëŠ” ê±´ softmax ë¶„í¬ê°€ flatí•˜ë‹¤ëŠ” ì˜ë¯¸)

<br>

## (3) MSP vs. Energy

| í•­ëª©       | MSP                     | Energy                      |
| ---------- | ----------------------- | --------------------------- |
| ì‚¬ìš© ì •ë³´  | max logitë§Œ ì‚¬ìš©        | ëª¨ë“  logit ì‚¬ìš©             |
| ì¶”ë¡  ë°©ì‹  | softmax í›„ max          | softmax ì „ log-sum-exp      |
| score ì˜ë¯¸ | í™•ì‹  ì •ë„ (ë†’ì„ìˆ˜ë¡ ID) | ë¶ˆí™•ì‹¤ì„± ì •ë„ (ë‚®ì„ìˆ˜ë¡ ID) |
| ì¥ì        | ê°„ë‹¨í•¨                  | ì •ë³´ëŸ‰ ë§ê³  robustí•¨        |
| ë‹¨ì        | ì •ë³´ ì†ì‹¤ ìˆìŒ          | ê³„ì‚° ì¡°ê¸ˆ ë” ë³µì¡           |

<br>

### Code

```python
def energy_score(logits, T=1.0):
    # logits: [B, C]
    energy = -T * torch.logsumexp(logits / T, dim=1)
    return energy  # ë‚®ì„ìˆ˜ë¡ ID, ë†’ì„ìˆ˜ë¡ OOD
```

<br>

## (4) ì¶”ê°€ í™œìš©: Energy + ODIN-style Perturbation

Liu et al. ë…¼ë¬¸: ODINì—ì„œì²˜ëŸ¼ **ì…ë ¥ perturbation**ë„ í•¨ê»˜ ì“°ë©´ ë” ì¢‹ë‹¤ê³  ì œì•ˆ!

$$\rightarrow$$ ì¦‰, ***energy í•¨ìˆ˜ë„ gradientì— ë¯¼ê°***í•˜ë¯€ë¡œ ODINì²˜ëŸ¼ ë‹¤ìŒì„ ì ìš©í•  ìˆ˜ ìˆìŒ

```python
# E(x)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ì— perturbation
grad = torch.autograd.grad(energy.sum(), x)[0]
x_perturbed = x + eps * grad.sign()
```

<br>

## (5) Entropy vs. Energy

- ê³µí†µì : ë‘˜ ë‹¤ **softmax ë¶„í¬ì˜ ë¶ˆí™•ì‹¤ì„±**ì„ í™œìš©
  - MSPì²˜ëŸ¼ max ê°’ í•˜ë‚˜ë§Œ ë³´ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” **ë¶„í¬ ì „ì²´ë¥¼ ë°˜ì˜**
  - **í™•ì‹ (confidence)ì´ ë‚®ì„ìˆ˜ë¡** OODë¡œ íŒë‹¨í•˜ëŠ” êµ¬ì¡°
- ì°¨ì´ì : **OODì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ”ì§€**

<br>

| í•­ëª©     | Entropy                      | Energy                                          |
| -------- | ---------------------------- | ----------------------------------------------- |
| ì…ë ¥     | softmax í™•ë¥                  | logit ì›ê°’                                      |
| scale    | í•­ìƒ 0~log(C) ë²”ìœ„ (bounded) | unbounded (logit í¬ê¸°ì— ë”°ë¼ ë‹¤ì–‘)              |
| gradient | softmaxì˜ gradient           | logitì˜ gradient (softmaxë³´ë‹¤ ì•ˆì •ì )           |
| í‘œí˜„ë ¥   | í™•ë¥ ì  í¼ì§ë§Œ ë°˜ì˜           | logit ê°’ í¬ê¸° + í¼ì§ ëª¨ë‘ ë°˜ì˜                  |
| ì¥ì      | ì§ê´€ì , ê°„ë‹¨                 | ë” ì •êµí•˜ê³  í‘œí˜„ë ¥ ë†’ìŒ                         |
| ë‹¨ì      | bounded â†’ sensitivity ë‚®ìŒ   | logit scale ë¯¼ê° â†’ normalization í•„ìš”í•  ìˆ˜ ìˆìŒ |

<br>

# 4. Mahalanobis distance

(Softmaxë‚˜ energyì²˜ëŸ¼) **logit ê¸°ë°˜ì˜ confidence**ê°€ ì•„ë‹ˆë¼...

$$\rightarrow$$ **"Feature spaceì—ì„œì˜ í†µê³„ì  ê±°ë¦¬"**ë¥¼ ì´ìš©í•´ì„œ OOD ì—¬ë¶€ë¥¼ íŒë‹¨!

https://papers.nips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf (NeurIPS 2018)

<br>

## (1) Key Idea

- feature embeddingì€ í´ë˜ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¸ ì–‘ìƒì„ ë³´ì„!
- ë”°ë¼ì„œ,
  - (1) embeddingì˜ ë¶„í¬ë¥¼ **í´ë˜ìŠ¤ë³„ Gaussian ë¶„í¬**ë¡œ ê·¼ì‚¬
  - (2) ìƒˆë¡œìš´ ì…ë ¥ì˜ featureê°€ **ì–´ëŠ í´ë˜ìŠ¤ ì¤‘ì‹¬ì—ì„œ ì–¼ë§ˆë‚˜ ë¨¼ê°€?**ë¥¼ ì¸¡ì •í•´ì„œ OOD ì—¬ë¶€ë¥¼ íŒë‹¨í•¨
- í•œ ì¤„ ìš”ì•½: **Mahalanobis distance**ë¥¼ í™œìš©í•´ *â€œì–´ëŠ ë¶„í¬ì—ë„ ì˜ ì†í•˜ì§€ ì•ŠëŠ”ë‹¤â€*ëŠ” ê²ƒì„ ìˆ˜ì¹˜í™”

<br>

## (2) Mahalanobis Distance

$$D_M(x)=\sqrt{(f(x)-\mu)^T \Sigma^{-1}(f(x)-\mu)}$$.

- $$f(x)$$ : ì…ë ¥ x ì˜ feature (ì—: penultimate layer ì¶œë ¥)
- $$\mu$$ : ë“ì • í´ë˜ìŠ¤ì˜ í‰ê·  feature
- $$\Sigma$$ : í´ë˜ìŠ¤ ê³µëŒ±ì˜ ê³µë¶„ì‚° í—ë ¬

$$\rightarrow$$ ì¼ë°˜ì ì¸ Euclidean distanceë³´ë‹¤ ***feature correlationê¹Œì§€ ê³ ë ¤***

<br>

How? 

- ì‘ì„ìˆ˜ë¡ ID
- í´ìˆ˜ë¡ OOD

<br>

## (3) Procedure

1. **ì‚¬ì „ í•™ìŠµëœ ë¶„ë¥˜ ëª¨ë¸** ì¤€ë¹„ (ì˜ˆ: ResNet-50)

2. Training setì˜ featureë¥¼ ë½‘ì•„ í´ë˜ìŠ¤ë³„ í‰ê·  & ê³µë¶„ì‚° ê³„ì‚°

3. Test setì˜ $$x$$ ì— ëŒ€í•´  $$f(x)$$ ì¶”ì¶œ

4. Mahalanobis distanceë¥¼ ëª¨ë“  í´ë˜ìŠ¤ $$c$$ì— ëŒ€í•´ ê³„ì‚°í•œ ë’¤ **ìµœì†Ÿê°’** ì‚¬ìš©

   - $$\operatorname{score}(x)=-\min _c D_M\left(f(x), \mu_c\right)$$.

   â†’ ìŒìˆ˜ë¥¼ ë¶™ì—¬ì„œ **ë†’ì„ìˆ˜ë¡ ID**, ë‚®ì„ìˆ˜ë¡ OOD

<br>

## (4) Mahalanobis vs Softmax-based

| í•­ëª©           | Mahalanobis             | MSP / Energy                |
| -------------- | ----------------------- | --------------------------- |
| ê¸°ë°˜           | Feature space (ì€ë‹‰ì¸µ)  | Logit space                 |
| íŠ¹ì§•           | í†µê³„ì  ê±°ë¦¬             | í™•ë¥ /ì—ë„ˆì§€ ê¸°ë°˜ confidence |
| ì„¤ëª…ë ¥         | í†µê³„ì  í•´ì„ ëª…í™•        | ëª¨ë¸ confidence í•´ì„        |
| ì¶”ê°€ í•™ìŠµ í•„ìš” | âŒ ì—†ìŒ                  | âŒ ì—†ìŒ                      |
| ê³„ì‚°ë¹„ìš©       | ê³µë¶„ì‚° ì—­í–‰ë ¬ ê³„ì‚° í•„ìš” | ë¹„êµì  ê°„ë‹¨                 |

<br>

### Code

```python
# assume: feature_extractor, class_means, inv_cov (Î£â»Â¹)

def mahalanobis_score(x):
    f = feature_extractor(x)  # feature: [B, D]
    distances = []
    for mu in class_means:  # list of [D]
        delta = f - mu
        d = torch.einsum('bi,ij,bj->b', delta, inv_cov, delta)
        distances.append(d)
    return -torch.stack(distances, dim=1).min(dim=1).values  # high = ID-like
```

<br>

## (5) ì¶”ê°€ ìš”ì†Œ

ì•„ë˜ì˜ ìš”ì†Œë“¤ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ!

- **ì…ë ¥ perturbation** (ODINì²˜ëŸ¼ gradientë¡œ input ì¡°ê¸ˆ ë³€ê²½)
- **multi-layer fusion**: í•˜ë‚˜ì˜ layer featureë§Œ ì“°ì§€ ì•Šê³  ì—¬ëŸ¬ layerì˜ Mahalanobis distanceë¥¼ ensemble

<br>

## (6) ì¥ì  & ë‹¨ì 

ì¥ì 

- **confidence ê¸°ë°˜ logitì˜ í•œê³„**ë¥¼ ë²—ì–´ë‚¨
- Adversarial attackê¹Œì§€ robustí•˜ê²Œ ë°©ì–´ ê°€ëŠ¥
- **í•´ì„ë ¥**ì´ ë†’ê³ , Gaussian class-conditional ê°€ì •ì´ ì§ê´€ì 

<br>

ë‹¨ì 

- **ê³µë¶„ì‚° ê³„ì‚°ì´ í•„ìš”** $$\rightarrow$$ ì°¨ì›ì´ ë†’ìœ¼ë©´ ëŠë¦´ ìˆ˜ ìˆìŒ
- í´ë˜ìŠ¤ ìˆ˜ ë§ê±°ë‚˜ ëª¨ë¸ì´ ë³µì¡í• ìˆ˜ë¡ ê³„ì‚° ë¹„ìš© ì¦ê°€
- í•™ìŠµì´ ì•„ë‹ˆë¼ post-hoc ë°©ì‹ì´ê¸´ í•˜ì§€ë§Œ **feature extractor**ê°€ ì¤‘ìš”

<br>

## (7) Summary

| í•­ëª©      | ì„¤ëª…                                           |
| --------- | ---------------------------------------------- |
| ì ‘ê·¼ ë°©ì‹ | í´ë˜ìŠ¤ë³„ feature ë¶„í¬(Gaussian) ê¸°ë°˜ ê±°ë¦¬ ì¸¡ì • |
| í•µì‹¬ í•¨ìˆ˜ | Mahalanobis distance                           |
| ì…ë ¥      | ì€ë‹‰ì¸µ feature                                 |
| ê²°ê³¼      | ê±°ë¦¬ ì‘ìœ¼ë©´ ID, í¬ë©´ OOD                       |
| í™•ì¥      | multi-layer, perturbation                      |

<br>

# 5. Generative ëª¨ë¸ ê¸°ë°˜ì˜ OOD detection

ì§€ê¸ˆê¹Œì§€ ë´¤ë˜ **discriminative (ë¶„ë¥˜ ê¸°ë°˜)** ë°©ë²•ê³¼ëŠ” ë‹¬ë¦¬...

$$\rightarrow$$ ì…ë ¥ ë°ì´í„°ë¥¼ **ì§ì ‘ ëª¨ë¸ë§**í•˜ëŠ” ì ‘ê·¼!

<br>

## (1) Key Idea

- **ID ë°ì´í„°**ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” **ìƒì„± ëª¨ë¸**ì„ í•™ìŠµí•´ë†“ìœ¼ë©´...
- **OOD ë°ì´í„°**ëŠ” **"low likelihoodë¥¼ ê°€ì§ˆ ê²ƒ"**ì´ë¼ê³  ê¸°ëŒ€!

â†’ í•™ìŠµëœ ìƒì„± ëª¨ë¸ì´ ì…ë ¥ $$x$$ì— ëŒ€í•´ ê³„ì‚°í•œ log-likelihood $$\log p(x)$$ ë¥¼ ì´ìš©í•´ì„œ ..

- **í™•ë¥ ì´ ë‚®ì€ ìƒ˜í”Œ = OOD**
- **í™•ë¥ ì´ ë†’ì€ ìƒ˜í”Œ = ID**

ë¼ê³  íŒë‹¨í•¨

<br>

## (2) ëŒ€í‘œ ëª¨ë¸ ì¢…ë¥˜

### a) **Normalizing Flow**

- $$p(x)=p(z) \mid \operatorname{det} \frac{\partial z}{\partial x} \mid ^{-1}$$.
- Exact likelihood ê³„ì‚° ê°€ëŠ¥
- Ex) Glow, RealNVP, Flow++, etc.

<br>

### b) **Autoregressive Model**

- $$p(x)=\prod_i p\left(x_i \mid x_{<i}\right)$$.
- ê° í”½ì…€(ë˜ëŠ” ì°¨ì›)ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•´ ì „ì²´ ë¶„í¬ ëª¨ë¸ë§
- Ex) PixelCNN, WaveNet

<br>

### c) **Variational Autoencoder (VAE)**

- $$x \sim p(x \mid z)$$.
- ELBO (Evidence Lower Bound)ë¥¼ log-likelihood ê·¼ì‚¬ë¡œ ì‚¬ìš©

<br>

### Code (NF)

```python
# x: input image or data
log_likelihood = flow_model.log_prob(x)
score = -log_likelihood  # ë‚®ì„ìˆ˜ë¡ OOD ê°€ëŠ¥ì„± ë†’ìŒ
```

â†’ ì´ ê°’ì„ thresholding í•´ì„œ OOD ì—¬ë¶€ íŒë‹¨

<br>

## (3) í•œê³„ì : "Likelihood â‰  Semantic ID-ness"

- ì˜ˆ: CIFAR-10ë¡œ í•™ìŠµí•œ Flow ëª¨ë¸ì´ SVHNë³´ë‹¤ CIFAR-10ì— **ë‚®ì€ likelihood**ë¥¼ ì£¼ëŠ” í˜„ìƒ (Nalisnick et al., 2019)
- Why? LikelihoodëŠ” **ì €ì°¨ì› í†µê³„ì  êµ¬ì¡°**(í…ìŠ¤ì²˜, í•´ìƒë„ ë“±)ì— ë¯¼ê°
  - ì¦‰, ìš°ë¦¬ê°€ ì›í•˜ëŠ” "ì´ê²Œ ì´ ë¶„í¬ì—ì„œ ì˜ë¯¸ ìˆëŠ” ìƒ˜í”Œì¸ê°€?"ëŠ” ë°˜ì˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸

<br>

## (4) Solution

### a) **Likelihood Ratio**

- Flow + classifierë¥¼ ê°™ì´ í•™ìŠµ
  - class-conditional likelihood p(xâˆ£y)ì™€ class-marginal p(x) ë¹„êµí•œë‹¤!
- $$\text{score}(x) = \frac{p(x|y)}{p(x)}$$.

<br>

### b) **Input complexity regularization**

- likelihoodê°€ ë†’ì€ **"ì‰¬ìš´"** ì…ë ¥ë“¤ì„ penalize
- ì˜ˆ: IDë³´ë‹¤ í”½ì…€ê°’ì´ í‰íƒ„í•œ SVHNì´ ë” ë†’ì€ likelihood ë°›ëŠ” ë¬¸ì œë¥¼ ë³´ì™„

<br>

### c) **Latent space í™œìš© (e.g., VAE)**

- encoderì˜ latent spaceì—ì„œì˜ distance, reconstruction error ë“±ì„ OOD scoreë¡œ ì‚¬ìš©

<br>

## (5) ì¥ì  & ë‹¨ì  

ì¥ì 

| í•­ëª©        | ì„¤ëª…                                              |
| ----------- | ------------------------------------------------- |
| í•™ìŠµ        | label ì—†ì´ ê°€ëŠ¥ (unsupervised)                    |
| ì ìš© ë²”ìœ„   | ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ì…ë ¥             |
| í•´ì„ ê°€ëŠ¥ì„± | í™•ë¥  ëª¨ë¸ì´ë¯€ë¡œ log-likelihoodì˜ ì§ê´€ì  ì˜ë¯¸ ìˆìŒ |

<br>

ë‹¨ì 

| í•­ëª©                     | ì„¤ëª…                                           |
| ------------------------ | ---------------------------------------------- |
| semantic OODì— ì·¨ì•½      | ì§„ì§œ ì˜ë¯¸ì˜ "ë¶„í¬ ë°”ê¹¥"ì„ ê°ì§€ ëª»í•  ìˆ˜ ìˆìŒ    |
| ì´ìƒí•˜ê²Œ ë†’ì€ likelihood | SVHNì²˜ëŸ¼ IDë³´ë‹¤ ë” ë†’ì€ likelihood ë°›ëŠ” ê²½ìš°   |
| ê³„ì‚° ë¹„ìš©                | íŠ¹íˆ high-dim ì´ë¯¸ì§€ì—ì„œëŠ” ëŠë¦¼ (Flow, VAE ë“±) |

<br>

# 6. SSL ê¸°ë°˜ OOD detection

Supervised signal ì—†ì´ë„ **OODì— ê°•ì¸í•œ feature**ë¥¼ ë½‘ì„ ìˆ˜ ìˆë‹¤!

<br>

## (1) Key Idea

- SSLì„ í†µí•´ I(D ë°ì´í„°ì— íŠ¹í™”ëœ) feature spaceë¥¼ í•™ìŠµ

- Test inputì´ ê·¸ ê³µê°„ì—ì„œ **ì¼ë°˜ì ì¸ ID featureì™€ ìœ ì‚¬í•œê°€?**ë¥¼ íŒë‹¨í•´ì„œ OOD ì—¬ë¶€ íŒë‹¨

  ( í•™ìŠµëœ feature extractorë¥¼ ê³ ì • )

<br>

Summary

- Training: label (X)
- Detection: feature similarity / classifier confidence ë“±

<br>

## (2) Examples

- Rotation prediction, Jigsaw ë“± Pretext Task ê¸°ë°˜
- Contrastive Learning ê¸°ë°˜ (ì˜ˆ: SimCLR, MoCo ë“±)

<br>

## (3) OOD Score ì˜ˆì‹œ:

- Nearest neighbor distance (k-NN)
- Feature ì¤‘ì‹¬ê³¼ì˜ cosine similarity
- Linear classifierì˜ softmax confidence

<br>

## (4) Examples

| ë…¼ë¬¸           | ë°©ì‹                                   | ì„¤ëª…                                                     |
| -------------- | -------------------------------------- | -------------------------------------------------------- |
| CSI (2020)     | Contrastive + augmentation + one-class | Self-supervised contrastive learning + novel OOD scoring |
| SSD (2021)     | Self-supervised detection              | Rotation, jigsaw ë“± pretext lossë¡œ OOD ê°ì§€              |
| KNN-OOD (2020) | k-NN in contrastive space              | Nearest neighbor distanceë¡œ score ê³„ì‚°                   |

<br>

## (5) Procedure

1. ID ë°ì´í„°ë§Œ ì‚¬ìš©í•´ì„œ SSLë¡œ encoder $$f(x)$$ í•™ìŠµ
2. í•™ìŠµëœ encoderë¥¼ ê³ ì •í•œ ì±„, **ID feature ì €ì¥**
3. Test input $$x$$ ì— ëŒ€í•´ feature $$f(x)$$ ê³„ì‚°
4. OOD score ê³„ì‚°:
   - ê°€ì¥ ê°€ê¹Œìš´ ID featureì™€ì˜ ê±°ë¦¬ (k-NN)
   - cosine similarity
   - classifierë¥¼ ë¶™ì˜€ë‹¤ë©´ softmax-based score

<br>

## (6) ì¥ì  & ë‹¨ì 

ì¥ì 

| í•­ëª©                    | ì„¤ëª…                                   |
| ----------------------- | -------------------------------------- |
| label ë¶ˆí•„ìš”            | ID ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥              |
| generalization          | ë‹¤ì–‘í•œ downstream taskì—ì„œë„ í™œìš© ê°€ëŠ¥ |
| ë‹¤ì–‘í•œ ì…ë ¥ì— ì ìš©      | ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ì‹œê³„ì—´ ë“± ë²”ìš©ì„± ìˆìŒ  |
| ë‹¤ë¥¸ ë°©ì‹ë“¤ê³¼ ê²°í•© ê°€ëŠ¥ | MSP, ODIN ë“±ê³¼ hybrid ê°€ëŠ¥             |

<br>

ë‹¨ì 

| í•­ëª©                     | ì„¤ëª…                                             |
| ------------------------ | ------------------------------------------------ |
| representation í’ˆì§ˆ ì˜ì¡´ | contrastive í•™ìŠµì´ ì˜ ì•ˆ ë˜ë©´ OOD ì„±ëŠ¥ë„ ì €í•˜    |
| score ê¸°ì¤€ ì„¤ê³„ í•„ìš”     | distance, classifier ë“± ì¶”ê°€ ì„¤ê³„ ìš”ì†Œ í•„ìš”      |
| feature drift ìœ„í—˜       | training/test domain ì°¨ì´ ë°œìƒ ì‹œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ |

