# 교훈 정리

## (1) WaveNet (2016)

- https://seunghan96.github.io/ts/Wavenet(2016)/
- **dilated causal convolution**



## (2) TCK & TKAE (2019)

- Learning Representations of multivariate time series with missing data (2019)
- https://seunghan96.github.io/ts/TKAE(2019)/
- **가변 길의의 input & missing data 잘다룸**
- TKAE ( = Temporal Kernelized Autoencoder )



## (3) TLAE (2021)

- Temporal Latent AutoEncoder : A Method For Probabilistic MTS Forecasting
- https://seunghan96.github.io/ts/TLAE(2021)/
- **MTS를 low rank matrix로 factorize하자**
- TRMF ( Temporal Regularized Mactrix Factorizion ) : $\mathbf{Y}$ = $\mathbf{F} \mathbf{X}$ 로 분해
- TLAE 한 줄 요약 : $\mathbf{Y} \rightarrow \mathbf{X} \rightarrow \hat{\mathbf{X}} \rightarrow \hat{\mathbf{Y}}$.
  - (1) $\mathbf{Y}_{B}=\left[\mathbf{y}_{1}, \ldots, \mathbf{y}_{b}\right] \in \mathbb{R}^{n \times b}$  임베딩하고,
  - (2) $\mathbf{X}_{B}=\left[\mathbf{x}_{1}, \ldots, \mathbf{x}_{b}\right] \in \mathbb{R}^{d \times b}$ 를 둘로 나누고 (앞/뒤)
  - (3) (앞)으로 (뒤) 예측하고  ( $\hat{\mathbf{x}}_{i+1}=h_{\mathrm{W}}\left(\mathrm{x}_{i-L+1}, \ldots, \mathbf{x}_{i}\right)$ )
  - (4) $\mathbf{x}_{1}, \ldots, \mathbf{x}_{L}$ & $\hat{\mathbf{x}}_{L+1}, \ldots, \hat{\mathbf{x}}_{b}$ 로 $\hat{\mathbf{Y}}_{B}$ 예측
- 이때 위의 (3) 과정에서 probabilistic forecasting ( mu, sigma 예측 )
  - VAE와 유사



## (4) HIVAE (2020)

- Handling Incomplete Heterogeneous Data using VAEs
- https://seunghan96.github.io/ts/HIVAE(2020)/
- VAE의 문제점 : heterogeneous ( cont + disc ) & missing data 취급못해



## (5) GP-VAE (2020)

- GP-VAE ; Deep Probabilistic Time Series Imputation
- https://seunghan96.github.io/ts/GPVAE(2020)/

- 목적 : 차원 축소 & imputation

<br>

## (6) AST (2020)

- Adversarial Sparse Transformer (=AST) for TS Forecasting
- https://seunghan96.github.io/ts/AST(2020)/
- **AST** based on **GANs**
  - generator : sparse transformer
  - discriminator : improve prediction

- 목적 : horizon 만큼 multi-step forecast, for **each quantile**
- X : 1~15 , Y : 16~20이라 했으면,
  - generator loss : Y & Y hat
  - discriminator loss : X+Y & X+Yhat
- **Sparse Transformer : 집중 안헐거는 참조 X**

<br>

## (7) Transformer + MTS (2020)

- A Transformer-based Framework for Multivariate Time Series Representation Learning
- https://seunghan96.github.io/ts/TransformerTS(2020)/
- **UNsupervised** 러닝 : missing value imputation
  - masking하고, 그 부분 맞추기

<br>

## (8) Informer (2021)

- Informer ; Beyond Efficient Transformer for Long Sequence Time-Series Forecasting
- https://seunghan96.github.io/ts/Informer(2021)/
- 핵심 : Transformer를 더 efficient하게!

<br>

## (9) IQN-RNN (2021) 

- Probabilistic TS forecasting with Implicit Quantile Networks
- **univariate** method 
- probabilistic output by **IQN**
- loss function : **CRPS ( = Continuous Ranked Probability Score )**

<img src="https://seunghan96.github.io/assets/img/ts/img17.png" width="400"/>,

<br>

## (10) SCINet (2021)

- Time Series is a Special Sequence ; Forecasting with Sample Convolution and Interaction
- https://seunghan96.github.io/ts/SCINET(2021)/
- SCINet ( Sample Convolution and Interaction Network )
  - hierarchical TSF framework ( binary tree )
  - SCI-Block
- step1) Input을 두 개의 sub-sequence로 쪼개기 (splitting)
- Step2) 각각 처리한뒤
- step3) 나중에 incorporate (interactive learning)

<img src="https://seunghan96.github.io/assets/img/ts/img19.png" width="400"/>,



## (11) Deep AR (2019)

- https://seunghan96.github.io/ts/DeepAR(2019)/
- probabilistic forecasting



## (12) Nbeats & NbeatsX

- 생략



## (13) Split Networks = Multi-task learning TS (2019)

- Multi-step Forecasting via Multi-task Learning 
- https://seunghan96.github.io/ts/Multi-step-Forecasting-via-Multi-task-Learning/
- 총 task의 수 : KxH
  - K : 시계열 개수 ( 그닥 확장 불가능할 듯 )
  - H : 예측 시계열 길이
- loss에 다른 가중치 : **exponential weighting**
  - 뒤에 있는거 맞추기 더 어려우므로

<br>

## (14) MTL-Trans (2021)

- Multi-Task Time Series Forecasting with Shared Attention
- https://seunghan96.github.io/ts/Multi-Task-Time-Series-Forecasting-with-Shared-Attention/
- **attention기반의 MTL**
- propose 2 different attention sharing architectures

<img src="https://seunghan96.github.io/assets/img/ts/img59.png" width="400"/>



## (15) Dilated RNN (2017)

- dilated recurrent skip-connection



## (16) Multi-task + MTS ( Imputation & Forecast ) ( 2021 )

- End-to-end Multi-task Learning of Missing Value Imputation and Forecasting in Time-Series Data
- https://seunghan96.github.io/ts/End-to-end-Multi-task-Learning-of-Missing-Value-Imputation-and-Forecasting-in-Time-Series-Data/
- 복잡해서 블로그 참조



## (17) TimeGAN (2019)

- https://seunghan96.github.io/ts/TimeGAN/
- UNsupervised + supervised 둘 다 활용
- TimeGAN = **Autoregressive + GAN**
- 2 objectives
  - (1) global : $\min _{\hat{p}} D\left(p\left(\mathbf{S}, \mathbf{X}_{1: T}\right) \| \hat{p}\left(\mathbf{S}, \mathbf{X}_{1: T}\right)\right)$ ……… UNSUPERVISED
    - (1-1) GAN discriminator loss
    - (1-2) Reconstruction loss

  - (2) local : $\min _{\hat{p}} D\left(p\left(\mathbf{X}_{t} \mid \mathbf{S}, \mathbf{X}_{1: t-1}\right) \| \hat{p}\left(\mathbf{X}_{t} \mid \mathbf{S}, \mathbf{X}_{1: t-1}\right)\right)$  ……… SUPERVISED




## (18) DTCR ( Deep Temporal Clustering Representation ) (2019)

- Learning Representations for Time Series Clustering

- https://seunghan96.github.io/ts/Learning-Representations-for-Time-Series-Clustering/
- Unsupervised method
- DTCR = (1) + (2)
  - (1) **temporal reconstruction**
  - (2) **K-means objective**
- able to obtain **cluster-specific** temporal representation
- encoder = **bidirectional Dilated RNN**
- <img src="https://seunghan96.github.io/assets/img/ts/img116.png" width="400"/>.



## (19) TS Clustering (2020)

- Clustering Time Series Data through Autoencoder-based Deep Learning Models
- https://seunghan96.github.io/ts/Clustering-Time-Series-Data-through-Autoencoder-based-Deep-Learning-Models/

<img src="https://seunghan96.github.io/assets/img/ts/img118.png" width="400"/>.





## (20) DeTSEC (Deep TS Embedding Clustering) (2020)

- Deep MTS Embedding Clustering via Attentive-Gated Autoencoder 
- https://seunghan96.github.io/ts/Deep-MTS-Embedding-Clustering-via-Attentive-Gated-Autoencoder/

- 가변 길이의 MTS 처리 가능
- 2개의 step
  - step 1) attention & gating mechanism
  - step 2) clustering refinement

<img src="https://seunghan96.github.io/assets/img/ts/img121.png" width="400"/>.



(1) autoencoder network

$\begin{aligned}
L_{a e}=& \frac{1}{|X|} \sum_{i=1}^{|X|}\left\|X_{i}-\operatorname{dec}\left(\operatorname{enc}\left(X_{i}, \Theta_{1}\right), \Theta_{2}\right)\right\|_{2}^{2} \\
&+\frac{1}{|X|} \sum_{i=1}^{|X|}\left\|\operatorname{rev}\left(X_{i}\right)-\operatorname{dec}_{b a c k}\left(\operatorname{enc}\left(X_{i}, \Theta_{1}\right), \Theta_{3}\right)\right\|_{2}^{2}
\end{aligned}$.

<br>

(2) regularizer term

$\frac{1}{|X|} \sum_{i=1}^{|X|} \sum_{l=1}^{n \text { Clust }} \delta_{i l} \|$ Centroids $l-\operatorname{enc}\left(X_{i}, \Theta_{1}\right) \|_{2}^{2}$

<br>

(3) Total loss : (1) + (2)



## (21) LogSparse Transformer (2020)

- https://seunghan96.github.io/ts/LogSparseTransformer/

- **convolutional self-attention** 을 제안함

<img src="https://seunghan96.github.io/assets/img/ts/img123.png" width="400"/>

<img src="https://seunghan96.github.io/assets/img/ts/img124.png" width="400"/>



## (22) TFT (Temporal Fusion Transformer) (2020)

- Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting

- https://seunghan96.github.io/ts/TFT/
- 3종류의 input을 받음
  - (1) 시간 불변 - static covariates
  - (2) 시간 가변 - known future inputs ( ex. 요일 )
  - (3) 시간 가변 - exogenous TS ( 미래꺼 몰라 )

<img src="https://seunghan96.github.io/assets/img/ts/img128.png" width="400"/>.

- 2가지 핵심
  - (1) multi-horizon forecasting
  - (2) interpretable …….. quantile regression
- 사용하는 알고리즘
  - (local) RNN
  - (Long-term = global) interpretable self-attention

<br>

## (23) DeepTCN (2020)

- Probabilistic Forecasting with Temporal Convolutional Neural Network 
- https://seunghan96.github.io/ts/DeepTCN/

<img src="https://seunghan96.github.io/assets/img/ts/img126.png" width="400"/>.

- Probabilistic Forecasting 하는 2가지 방법
  - (1) non-parametric ……… quantile loss
  - (2) parametric …….. mu & sigma 추정

<br>

## (24) STRIPE (2020)

- Probabilistic Time Series Forecasting with Structured Shape and Temporal Diversity
- https://seunghan96.github.io/ts/STRIPE/

- DTW처럼, “모양” 기반으로 similarity 측정하는 방법론 제안

<br>

## (25) Uber (2017)

- Deep and Confident Prediction for Time Series at Uber

- https://seunghan96.github.io/ts/BNN/

- 그냥 monte-carlo dropout 사용
- **pre-training** 방법
- **외부 변수 있을때 사용하기 좋은 방법**

<img src="https://seunghan96.github.io/assets/img/ts/img135.png" width="400"/>.

<br>

## (26) ForGAN (2019)

- Probabilistic Forecasting of Sensory Data with Generative Adversarial Networks - ForGAN 

- https://seunghan96.github.io/ts/ForGAN/
- **probabilistic Forecasting with CGAN**

<img src="https://seunghan96.github.io/assets/img/ts/img137.png" width="400"/>.

<br>

## (27) DPSOM (2020)

- 생략

<br>

## (28) TripletLoss (2019)

- Unsupervised Scalable Representation Learning for Multivariate Time Series

- https://seunghan96.github.io/ts/TripletLoss/

- unsupervised 학습을 위해, 새로운 task & 그로 인한 loss 제시

<img src="https://seunghan96.github.io/assets/img/ts/img141.png" width="400"/>.

<br>

## (29) SOM-VAE (2019)

- 생략

<br>

## (30) MQ-R(C)NN (2017)

- A Multi-Horizon Quantile Recurrent Forecaster

- https://seunghan96.github.io/ts/ts2/

$p\left(y_{t+k, i}, \cdots, y_{t+1, i} \mid y_{: t, i}, x_{: t, i}^{(h)}, x_{t: i}^{(f)}, x_{i}^{(s)}\right)$.
$y_{\cdot, i}: i$ th target TS
1) $x_{: t, i}^{(h)}$ : temporal covariates ( available in history )
2) $x_{t:, i}^{(f)}:$ knowledge about the future
3) $x_{i}^{(s)}$ : static, time-invariant features

<img src="https://seunghan96.github.io/assets/img/ts/img162.jpg" width="400"/>.



## (31) AE based CPD

- 생략

<br>

## (32) RNN + imputation (2018)

- Recurrent Neural Networks for MTS with Missing Values
- https://seunghan96.github.io/ts/ts7/

- **GRU-D** 를 제안함
- 2 representations of **informative missingness patterns**
  - (1) masking ( 미싱인지 아닌지 0/1 )
  - (2) time interval ( 얼마나 오랫동안 미싱이 아니었는지 )

<img src="https://seunghan96.github.io/assets/img/ts/img168.png" width="400"/>.

<br>

## (33) DAIN (Deep Adaptive Input Normalization) (2019)

- Deep Adaptive Input Normalization for Time Series Forecasting
- https://seunghan96.github.io/ts/ts10/
- 배경 : **DL degenerate, if data are not “properly normalized”**
- 제안 : **adaptively normalizing the input TS**
- 세 가지 sub-layer
  - (1) shift the data ( centering )
  - (2) scaling ( standardization )
  - (3) gating ( suppressing features that are irrelevant )

<img src="https://seunghan96.github.io/assets/img/ts/img180.png" width="800"/>.

## (34) Transformer + MTS 2 (2020)

- Deep Transformer Models for Time Series Forecasting ; The Influenza Prevalence Case

- https://seunghan96.github.io/ts/ts13/
- 

<img src="https://seunghan96.github.io/assets/img/ts/img185.png" width="400"/>.

