---
title: LIMA; Less Is More for Alignment
categories: [LLM, NLP]
tags: []
excerpt: NeurIPS 2023
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# LIMA: Less Is More for Alignment

```
Zhou, Chunting, et al. "Lima: Less is more for alignment." NeurIPS 2023
```

( https://arxiv.org/pdf/2305.11206 )

참고: 

- https://aipapersacademy.com/lima/

<br>

### Contents

1. Abstract
2. LLM Training Stages
3. How LIMA can improve LLMs training process?
4. Small datasets with 1000 samples
5. Experiments

<br>

# 1. Abstract

LIMA = ***Less Is More*** for Alignment ( by Met AI)

- Fine-tune the LLaMa model on only ***1000 samples***

  $$\rightarrow$$ Achieve competitive results with top large language models (such as GPT-4, Bard and Alpaca)

<br>

# 2. LLM Training Stages

Stage 1) **Pre-training** stage: NSP task

Stage 2) **Alignment** stage

- Not very good with helping in concrete tasks that LLMs are often used for

  $$\rightarrow$$ Need alignment! 

- Pretrained model is being **fine-tuned on a specific task dataset** 
  - e.g., instructions dataset  human feedback with reinforcement learning (RLHF)

![figure2](/assets/img/llm/img102.png)

<br>

# 3. How LIMA can improve LLMs training process?

Proposal: **Alignment stage** can be replaced with a ***much more lightweight process*** of fine-tuning on just a ***small dataset***

$$\rightarrow$$ Still achieve remarkable and competitive results!

<br>

Why does it work well?

$$\rightarrow$$ ***Superficial alignment hypothesis***

<br>

## Superficial Alignment Hypothesis

Key point: A model has learned ***almost entirely*** during the **pretraining** stage!

Thus, alignment stage = simple! only requires to learn..

- ***What part of knowledge to use***
- ***What is the correct format***

<br>

$$\rightarrow$$ **SHORT fine-tuning** can ruin less of the **pretraining knowledge** ( & avoid catastrophic forgetting )

<br>

# 4. Small datasets with 1000 samples

![figure2](/assets/img/llm/img103.png)

<br>

# 5. Experiments

![figure2](/assets/img/llm/img104.png)

<br>

- Alpaca = LLaMa + fine-tune on **LARGE** instructions dataset
- LIMA= LLaMa + fine-tune on **SMALL** instructions dataset
- DaVinci003 = (based on InstructGPT) trained with **RLHF**

<br>
