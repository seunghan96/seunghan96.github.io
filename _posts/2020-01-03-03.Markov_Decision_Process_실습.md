---
title: 3.Markov Decision Process (MDP) 실습
categories: [RL]
tags: [Reinforcement Learning, Value Function, Bellman Equation, MDP]
excerpt: Value Function, Bellman Equation, Markov Decision Process
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

( 참고 : Fastcampus 강의 )

# [ 3. Markov Decision Process (MDP) 실습 ]

<br>

# 1. 환경 설정하기

## 1-1. Import Packages

```python
import sys; sys.path.append('..') 
import numpy as np 

from src.common.gridworld import GridworldEnv 
```

<br>

## 1-2. Make Environment

```python
num_y, num_x = 4, 4
env = GridworldEnv(shape=[num_y, num_x])
```

===========
T  x  o  o
o  o  o  o
o  o  o  o
o  o  o  T

===========

- T: 도착점 (종결상태, Terminal state) 
- x: 현재 위치 $s_t$
- o: 다른 환경의 점

<br>

# 2. Environment 소개

```python
observation_space = env.observation_space
action_space = env.action_space
P = env.P_tensor
R = env.R_tensor
```

- `observation_space`의 크기 : 16 ( 4x4의 grid )
- `action_space`의 크기 : 4( 상/하/좌/우 )

- `P` (Transition Matrix)의 크기 : 4x16x16
  - 4 : a (상하좌우)
  - 16 : 현재 상태 (4x4=16)
  - 16 : 다음 상태 (4x4=16)
- `R` (Reward)의 크기 : 16x4
  - 16 : 16개의 state에서
  - 4 : 4가지 행동을 했을 때의 return값들

<br>

ex) "상"으로 행동할 경우의 transition probability는?

```python
action_up_prob = P[0, :, :]
```

- 값을 확인해보면, 전부 1 or 0이다 ( deterministic이다 )

- 현실성이 없다

  ( 현실은 "상"으로 행동한다고 반드시 "상"으로 가게 되지 않을 수도 )

<br>

# 3. Markov Decision Process

## 3-1. Episode

MDP의 Episode는 

$$<(s_0, a_0, r_0, s_1),..., (s_{t}, a_{t}, r_{t}, s_{t+1}),..., (s_{T-1}, a_{T-1}, r_{T-1}, s_{T})>$$ 으로 구성

<br>

4가지 행동 (dictionary)

````python
action_mapper = {
    0: 'UP',
    1: 'RIGHT',
    2: 'DOWN',
    3: 'LEFT'
}
````

<br>

한 번의 Episode 수행하기

- terminal state에 도달하면, 하나의 epsiode가 끝나게 된다.

```python
_ = env.reset()

num_step = 0
while True:
    print("t = {}".format(num_step))
    
    # (1) environment 세우기
    env._render()
    cur_state = env.s
    
    # (2) random하게 상/하/좌/우 중 1개 고르기
    action = np.random.randint(low=0, high=4)
    
    # (3) action하기
    next_state, reward, done, info = env.step(action)    
    
    print("state : {}".format(cur_state))
    print("aciton : {}".format(action_mapper[action]))
    print("reward : {}".format(reward))
    print("next state : {} \n".format(next_state))
    print('---------------------------------------')
    num_step += 1
    if done:
        break
```

<br>

## 3-2. 여러 번의 Epsiode

`run_episode` : initial state와 environment를 주어주면, 1번의 episode를 수행하는 함수

```python
def run_episode(env, s0):
    _ = env.reset() 
    env.s = s0
    num_step = 0
    
    while True:
        action = np.random.randint(low=0, high=4)
        next_state, reward, done, info = env.step(action)
        num_step += 1
        if done:
            break
    return num_step
```

<br>

Settings

- 총 10번의 epsiode 수행
- initial state : 6

```python
num_episodes = 10
s0 = 6

for i in range(num_episodes):
    len_ep = run_episode(env, s0)
    print("Episode {} | Length of episode : {}".format(i, len_ep))
```

```
Episode 0 | Length of episode : 5
Episode 1 | Length of episode : 101
Episode 2 | Length of episode : 29
Episode 3 | Length of episode : 7
Episode 4 | Length of episode : 7
Episode 5 | Length of episode : 5
Episode 6 | Length of episode : 11
Episode 7 | Length of episode : 9
Episode 8 | Length of episode : 28
Episode 9 | Length of episode : 3
```

