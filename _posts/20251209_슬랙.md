TSCMamba 관련 아래의 세 부분에 대해 전달드립니다!

------------------------------------------------------------------------

- (1) Inference set 추출

- (2) 다양한 Channel 개수 (C)를 활용한 training
- (3) TSCMamba의 ROCKET 활용 여부 판단

------------------------------------------------------------------------

(1) Inference set 추출

아래의 문제들은 해결되었는가? 

- (1) 중복: O
  - 해결 방식: 추출하면서 중복 체크하기

- (2) 누락: O
  - 특별히 누락이 되는 이슈가 없었음

- (3) 터짐: X

  - 180K중, 135K가 중복없이 저장되었다가 터짐

- (4) 예상 소요시간의 정확성
  - Progress bar는 현재 시간을 제대로 측정못하고 있음.
  - 분수의 분자가 a)가 아니라 b)여야 정확한 시간 측정하는 꼴
  - a) `f"[PROGRESS] {done}/{total} "`
  - b) `f"[PROGRESS] {processed_user_matches}/{total} "`

- 해결 방식
  - 기존) `02.json_to_TS_inference.ipynb` 
    - 마찬가지로 3.5h에서 또 죽음
    - 또한, 중복 (+Maybe 누락) 저장 issue

  - 수정) `02.json_to_TS_inference_dedup.ipynb` 
    - 해결의 핵심: **deduplication**
    - 의심 원인: Multi-processing을 사용하다보니, process간에 본인들이 뽑은 user match가 process들간에 서로 공유되지 않아서, 중복저장 되는 것은 아닐지?
    - 결론: **성공**

- 저장 경로

  - (Before) `anticheat_log_TS_inference`
  - (After) `anticheat_log_TS_inference_dedup`

  ```
  dir_path = '/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/timeseries_dl/anticheat_log_TS_inference_dedup/2025-11-20/user_match/'
  pkl_files = [f for f in os.listdir(dir_path) if f.endswith('.pkl')]
  ```

- 



(2) 다양한 Channel 개수 (C)를 활용한 training


기타: 다양한 C의 개수 (=feature/channel의 개수)로 Training을 진행 중

- Caching 이슈 X: C=20,30,40,50,60
- Caching 이슈 O: C=167 (전부 사용한 버전)

<br>

정확히 아래의 코드에서 죽음: `data_factory.py`

- issue: kernel unhealthy (약 1시간 10분 돌아가다가 아래의 위치에서 죽음)
- mean & std의 broadcasting 과정에서 발생한듯

```os.makedirs(cache_file, exist_ok=True)
X_train = (X_train - mean) / (std + 1e-8)
X_val   = (X_val   - mean) / (std + 1e-8)
X_test  = (X_test  - mean) / (std + 1e-8)
```

해결 방안: 

- (1) caching을 안하는 방법 
- (2) 여러개로 나눠서 caching

=> 다만, 이렇게까지 할 필요가 있을지 우선 보류. 이유?

- 현재 C=20,30,40,50,60에 대해서 진행 중이고, 이에 대한 labeled test set의 F1 score은 전부 비슷한 상황. 
  - 물론 EBR의 양상은 다를 수 있음. 현재 아직 측정은 안해본 상태 (Inference set이 이제 막 추출)

- 따라서, C=20~60에 대해서 우선 EBR을 측정을 해본 뒤, 그 성능을 본 뒤에 C=167까지 해볼 필요가 있는지를 판단하기
- (+ C=167 -> C=100으로 줄여서 현재 돌려놓은 상황)



(3) TSCMamba의 ROCKET 활용 여부 판단

- ROCKET: TSCMamba내의 module로써, TS embedding extraction을 위해 convolution kernel을 활용
  - *ROCKET generates **random convolutional kernels**, including random length and dilation. It transforms the time series with two features per kernel*
  - *The substantial accuracy drop indicates that **ROCKET’s non-learnable convolutional features provide rich and robust representations that enhance classification**.*
  - 참고) 여기서의 CNN은 learnable X, random initialize
  - (1=Full 사용) 512 dim = MLP_512(TS)
  - (0=사용 안함) 512 dim = CNN_512(TS)
  - (0.5=절반 사용) 512 dim = MLP_512(TS) +  CNN_512(TS)
- 한계점: 다만, ROCKET의 loading이 불가능
  - 이유 1) 너무 오래 걸림 (1개의 channel 당 20분)
  - 이유 2) memory issue (20~30개 channel 사이에서 터짐)
- (참고) 여기서의 loading이란?
  - `z = filter (x)`에서, `z`를 loading하는 것이 아니라, 
  - `filter`를 로딩하고 `x`를 적용하는 과정을 말함 
- 기타 사항
  - (실제로 TSCMamba official github도) `no_rocket = 1`로써, ROCKET 사용안하는게 default
  - 또한, 논문에서 ablation study를 완벽하게 진행하지 않아서, ROCKET O vs. X에 따른 성능은 없음.
- 결론: 해당 module은 사용안하는게 현실적!



결론 세줄 요약

- (1) Inference set은 잘 추출되었고
- (2) Channel(feature)의 개수 관련해서는 
  - 20/30/40/50/60개에서는 문제없이 잘 진행되나, 
  - 167개 (전체)에 대해서는 죽어서, 32bit -> 16bit로 변환하여 caching하고
- (3) ROCKET 모듈은 사용하지 않음 
