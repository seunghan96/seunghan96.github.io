---
title: 27.DQN
categories: [RL]
tags: [Reinforcement Learning]
excerpt: Problems of RL & DQN
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

( 참고 : Fastcampus 강의 )

# [ 27.DQN ]

### Contents

1. Problems of RL
2. Non-stationary Target
3. Idea of DQN
4. DQN (Deep Q-Network)
   1. Replay Memory
   2. Fixed Q-target
   3. 기타
5. DQN Summary

<br>

# 1. Problems of RL

(1) sparse reward

- (일반적) DL : labeled training dataset을 통해 학습

- RL : 오로지 reward를 통해서만 학습되고, 심지어 reward도 sparse!

(2) iid assumption 불가

- 현재 state & 다음 state간의 correlation이 크다

(3) Non-stationary Target

<br>

# 2. Non-stationary Target

***우리의 타겟값($$y$$) 조차 fixed된 값이 아니다!***

최적의 action-value function을 근사하기 위한 loss function(MSE)를 적으면 아래와 같다.

- $$L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a, r, s^{\prime}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]$$.

즉, 근사하고자 하는 대상을 다시 적자면 아래와 같다.

- $$y_{i}=r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}\right)$$

위 식에서 알 수 있다시피, $$Q\left(s, a ; \theta_{i}\right)$$ 가 $$Q$$ 함수에 dependent하므로, $$Q$$ 함수가 update됨에 따라 target 값 또한 계속 변화하는 상황이다.

<br>

# 3. Idea of DQN

DQN은 위의 문제를, 아래와 같은 방법으로 해결한다.

Problem 1 : correlation between samples

$$\rightarrow$$ Solution 1  : **Experience Replay**

Problem 2 : non-stationary target

$$\rightarrow$$ Solution 2  : **fixed Q-target**

<br>

HOW?

- 1) raw pixel 그대로 input data로 사용 (전처리 X)
- 2) function approximator : CNN
- 3) 하나의 agent가 여러 종류의 Atari game을 학습하도록
- 4) Experience replay로 효율성 $$\uparrow$$

<br>

# 4. DQN (Deep Q-Network)

## (1) Replay Memory

Replay Memory를 통해 correlation을 줄이기!

1. **기억 저장**

   - agent의 경험 ( $$e_{t}=\left(s_{t}, a_{p}, r_{p}, s_{t+1}\right)$$  )을 time step 단위로 $$D_{t}=\left\{e_{1}, \ldots, e_{t}\right\}$$에 저장

2. **학습 진행**

   - 저장된 기억(=$$D_t$$)으로부터 sampling 하여 구성된 mini-batch 통해 학습 진행

     ( uniform sample : $$(s, a, r, s) \sim U(D))$$ )

   - mini-batch가 sequential 하지 않기 때문에, decorrelated

   - 과거 경험에 대한 반복학습 OK

   - (paper) replay memory size = 1,000,000

<br>

## (2) Fixed Q-target

$$Q(s, a ; \theta)$$ 와 동일한 네트워크 구조를 가진 ( parameter는 다른 ) 독립적인 target network $$\hat{Q}\left(s, a ; \theta^{-}\right)$$ 생성

$$\rightarrow$$ Q-learning $$\operatorname{target} y_{i}$$ 에 이용한다.

<br>

$$y_{i} =r+\gamma \max _{a} \hat{Q}\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right) \\$$

$$\begin{aligned} L_{i}\left(\theta_{i}\right) &=\mathbb{E}_{(s, a, r s) \sim U(D)}\left[\left(y_i -Q\left(s, a ; \theta_{i}\right)\right)^{2}\right] \\ &=\mathbb{E}_{(s, a, r s) \sim U(D)}\left[\left(r+\gamma \operatorname{rmax}_{a} \hat{Q}\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right] \end{aligned}$$.

<br>

Target Network $$\hat{Q}\left(s, a ; \theta^{-}\right)$$의 parameter $$\theta^{-}$$는 $$C$$ step 마다, Q Network $$Q(s, a ; \theta)$$의 parameter $$\theta$$로 update된다. ( paper : set $$C=10, 000$$ )

<br>

## (3) 기타

gradient exploration 방지 위해 gradient clipping 사용

<br>

# 5. DQN Summary

<img src="https://miro.medium.com/max/998/1*Du1AnMnIEq85EJYh6GmFCA.png" width="750" />.







