---
title: All about Mistral
categories: [LLM, MULT, NLP]
tags: []
excerpt: Mistral 7B, Mixtral 8x7b 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# All about "Mistral"

(Reference: https://www.youtube.com/watch?v=UiX8K-xBUpE)

<br>

## Contents

1. Introduction to Mistral
   1. Transformer vs. Mistral
   2. Mistral vs. LLaMA
2. Mistral vs. Mixtral
3. Sliding Window Attention (SWA)
   1. w/o SWA vs. w/SWA
   2. Details
4. KV-Cache
   1. KV-Cache
   2. Rolling Buffer Cache
   3. Pre-fill & Chunking
5. Sparse MoE
6. Model Sharding
7. Optimizing inference with multiple prompts

<br>

# 1. Introduction to Mistral

## (1) Transformer vs. Mistral

- (naive) Transformer = Encoder + Decoder
- Mistral = ***Decoder-only*** model (like LLaMA)

<br>

## (2) Mistral vs. LLaMA

- (1) Attention $$\rightarrow$$ **Sliding window attention**
- (2) **Rolling Buffer**
- (3) FFN $$\rightarrow$$ **MoE (for Mixtral)**
- Both methods uses
  - (1) **GQA (Grouped query attention)**
  - (2) **RoPE (Rotary Positional Embedding)**


![figure2](/assets/img/llm/img713.png)

<br>

# 2. Mistral vs. Mixtral

Depends on the usage of MoE!

- Mi**s**tral: MoE (X) ... 7B
- Mi**x**tral: MoE (O) ... 8 experts of 7B

![figure2](/assets/img/llm/img714.png)

<br>

# 3. Sliding Window Attention (SWA)

## (1) w/o SWA vs. w/ SWA

**( sliding window size = 3 )**

![figure2](/assets/img/llm/img715.png)

<br>

## (2) Details

1. **[Efficiency]** Reduce the \# of dot products

2. **[Trade-off]** May lead to degradation, as less interaction btw tokens

   $$\rightarrow$$ But still, much more efficient! 

3. **[Receptive field]** Can still allow one token to watch outside the window (due to multiple layers)

![figure2](/assets/img/llm/img716.png)

<br>

# 4. KV-Cache

## (1) KV-Cache

Goal: ***Faster inference!***

At each step of the inference, only interested in the **last token**!

- As ***ONLY the last token*** is projected to linear layer (to predict the next token)

Nonetheless, ***model needs all the previous tokens*** to constitute its context

$$\rightarrow$$ Solution: KV Cache

<br>

### a) Inference w/o KV Cache

![figure2](/assets/img/llm/img717.png)

<br>

### b) Inference w/ KV Cache

![figure2](/assets/img/llm/img718.png)

<br>

## (2) Rolling Buffer Cache

**"KV-Cache" + "Sliding window attention"**

$$\rightarrow$$ **No need to keep ALL** the previous tokens in the cache!

( only limit to the "latest $$W$$ tokens")

![figure2](/assets/img/llm/img719.png)

<br>

Example: 

- Sentence: **"The cat is on a chair"** 
- **Window size ($$W$$) = 4**
- Current token: $$t=4$$ (**chair**)

<br>

$$t=3$$ : [The, cat, is, on]

should become

$$t=4$$ : [cat, is, on, a]

$$\rightarrow$$ By **"unrolling"** ( or unrotating )

![figure2](/assets/img/llm/img720.png)

<br>

## (3) Pre-fill & Chunking

### a) Inference with LLM

Infernce with LLM

- Use a prompt & Generate tokens ***"ONE BY ONE"*** (using the previous tokens)

Inference with LLM  + **"KV-Cache"**

- **Add all the prompt tokens to the KV-Cache**

<br>

### b) Motivation

[Motivation] We know all the prompts in advance! ( = no need to generate )

$$\rightarrow$$ Why not ***"PREFILL" the KV-Cache*** using the ***"tokens of the PROMPT"***?

( + What if the prompt is toooo long? )

<br>

Solution:

- **(1) Prefilling**: prefill the kv-cache using the tokens of the prompt
- **(2) Chunking**: divide the prompt into chunkks (of size $$W$$ = window size)

<br>

### c) Example

Setting:

- Large (Long) prompt + $$W=4$$

- Sentence = **"Can you tell me who is the richest man in history?"**

<br>

**Step 1) First prefill**

- Fill the first $$W$$ tokens in the KV-Cache

![figure2](/assets/img/llm/img721.png)

<br>

**Step 2) Subsequent prefill**

- Fill the next $$W$$ tokens in the KV-Cache

- Attention masked is calculated using ....

  - (1) KV-Cache (Can, you, tell, me)
  - (2) Current chunk (who is the richest)

  $$\rightarrow$$ $$\therefore$$ Size of attention mask can be bigger than $$W$$

![figure2](/assets/img/llm/img722.png)

<br>

**Step 3) Generation**

- Size of attention mask = $$W$$

![figure2](/assets/img/llm/img723.png)

<br>

# 5. Sparse MoE

### Mixture of Experts: Ensemble technique

- Multiple “expert” models
  - Each trained on a subset of the data
  - Each model specializes on it
- Output of the experts are combined 

<br>

### Mistral 8x7B: **Sparse Mixture of Experts (SMoE)**

- Only 2 out of 8 experts are used for every token
- Gate: Produces logits $$\rightarrow$$ Used to select the **top-k** experts

![figure2](/assets/img/llm/img724.png)

<br>

Details

- Experts = FFN layers
- Architectures: Each Encoder layer is comprised of ...
  - (1) Single Self-Attention mechanism
  - (2) Mixture of experts of 8 FFN
    - Gate function selects the top 2 experts 

![figure2](/assets/img/llm/img725.png)

![figure2](/assets/img/llm/img726.png)

<br>

# 6. Model Sharding

Example) **Pipeline parallelism (PP)**

- Mistral = 32 Encoder layers
  - 4 GPUs x 8 layers

![figure2](/assets/img/llm/img727.png)

$$\rightarrow$$ ***Not very efficient! Only one GPU is working at a time!***  How to solve?

<br>

### Before

![figure2](/assets/img/llm/img728.png)

<br>

### After

![figure2](/assets/img/llm/img729.png)

- Divide batch into smaller microbatches!
- **Gradient accumulation**: Gradients for each microbatch is accumulated!

<br>

# 7.Optimizing inference with multiple prompts

## (1) Problem

Example) Prompt of 3 different users

- Prompt 1: “Write a poem” (3 tokens) 
- Prompt 2: “Write a historical novel” (4 tokens) 
- Prompt 3: “Tell me a funny joke” (5 tokens)

( Note that we cannot use )

<br>

![figure2](/assets/img/llm/img730.png)

![figure2](/assets/img/llm/img731.png)

<br>

## (2) Solution

Into a ***SINGLE sequence***!

( + Keep track of the **"length of each prompt"** when we calculate the output )

$$\rightarrow$$ by using **xformers BlockDiagonalCausalMask**

![figure2](/assets/img/llm/img732.png)
