---
title: LongNet; Scaling Transformers to 1,000,000,000 Tokens
categories: [LLM, NLP, CV, TS]
tags: []
excerpt: arxiv 2023
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# LongNet: Scaling Transformers to 1,000,000,000 Tokens

```
Ding, Jiayu, et al. "Longnet: Scaling transformers to 1,000,000,000 tokens." arXiv preprint arXiv:2307.02486 (2023).
```

( https://arxiv.org/pdf/2307.02486 )

참고: 

- https://aipapersacademy.com/longnet/

<br>

### Contents

1. Background
2. Improving Attention Mechanism
   1. Standard Attention
   2. Dilated Attention
   3. Mixture of Dilated Attentions
   4. Multi-head Dilated Attention


<br>

# 1. Background

Modeling **long sequences** is crucial!

Limitation: **High computational complexity**

$$\rightarrow$$ Difficult to **scale up** the context length.

<br>

# 2. Improving Attention Mechanism

## (1) Standard Attention

**Quadratic dependency**

![figure2](/assets/img/llm/img105.png)

<br>

## (2) Dilated Attention

**Sparsification** = Remove rows from each segment based on a hyperparameter $$r$$

- Controls the distance between each removed row
- Each segment can be calculated **in parallel** $$\rightarrow$$ Distributed training on multiple GPUs.

<br>

![figure2](/assets/img/llm/img106.png)

![figure2](/assets/img/llm/img107.png)

<br>

## (3) Mixture of Dilated Attentions

Q) **Information loss** by dilation?

$$\rightarrow$$ Use **mixture of dilated attentions**

<br>

![figure2](/assets/img/llm/img109.png)

- All of the different dilated attentions can be computed **in parallel**
- Provide the model with **diverse and full information** that captures both **short-range** and **long-range** information.

<br>

## (4) Multi-head Dilated Attention

To ***further diverse*** the captured information ( in addition to the mixture of dilate attentions )

$$\rightarrow$$ Use **multi-head dilated attention blocks**

- Choose different rows to remove in each block!

<br>

![figure2](/assets/img/llm/img110.png)
