---
title: DALL-E
categories: [LLM, MULT, CV]
tags: []
excerpt: arxiv 2021
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# DALL-E (arxiv 2021)

*Zero-Shot Text-to-Image Generation (https://arxiv.org/pdf/2102.12092)*

<br>

### 간단 요약

- **"dVAE"**로 image를 **"1024개의 discrete token"**으로 압축해 **"visual codebook"**을 만든 뒤
- Text token + Image token을 **"하나의 sequence"**로 결합해 
- 12B Transformer가 **"Autoregressive"**로 **"joint modeling"**
  - **"text token을 condition으로"**
  - **"256×256 이미지의 zero-shot 생성"**이 가능하도록 
- **"2.5B image-text pair"**로 학습된 초기 대규모 text-to-image 모델

<br>

## Contents

1. Abstract
2. Introduction
3. Method
   1. Stage 1: dVAE로 Image 압축 (Visual Codebook 학습)
   2. Stage 2: Text + Image Tokens Joint Modeling (Prior 학습)

<br>

# 1. Abstract

**기존 text-to-image (t2i)** 모델은 **"복잡한 구조"**

- e.g., multi-scale GAN, segmentation mask, auxiliary loss 등

<br>

**DALL-E**

- (1) Data: **Text+ Image token**을 ***"하나의 sequence로"***
- (2) Model: Transformer
- (3) Task: **Autoregressive** modelin

<br>

Results

- **12B** model을 **2.5B**개의 Image-Text pairs로 학습
- 결과: **Zero-shot**에서도 기존 domain-specific 모델들과 경쟁력 확보.

<br>

# 2. Introduction

Previous works (**초기 t2i** 연구)

- DRAW (VAE 기반) → 이후 GAN 기반
- 이후 attention, multi-scale·object-conditioning 등 다양한 기법
- 최근 GPT계열이 다양한 분야에서 큰 발전

<br>

Motivation

- **기존 t2i 모델**들은 (MS-COCO와 같은) **small-scale dataset**에 국한
- 모델 구조보다 ***dataset 규모와 model 규모가 한계였던 것 아니냐?*** 라는 의문

<br>

DALL-E

- Arch) **12B Transformer**

- Data) **250M image-text pairs**

- How) Pixel 대신 **dVAE로 tokenize 후** 

- Task) *Autoregressive* modeling

  → 고품질 Image를 **zero-shot**으로 생성

<br>

# 2. Method

DALL-E의 Two-stage 구조

- (1) Stage 1: dVAE로 Image 압축 (Visual Codebook 학습)
- (2) Stage 2: Text + Image Tokens Joint Modeling (Prior 학습)

<br>

## 전체 목표

- a) Text token + Image token을 **"하나의" autoregressive sequence**로 모델링
- b) **"Text를 조건"**으로 **"Image token을 생성"**하는 Transformer를 학습

<br>

문제점? Pixel 단위로 직접 모델링하면 context 길이가 **너무 커짐** 

→ Transformer 불가

→ **Image를 discrete tokens로 압축하는 별도의 모듈 (dVAE)** 가 필요!

<br>

## (1) Stage 1: dVAE로 Image 압축 (Visual Codebook 학습)

***Image를 (32×32 = 1024개의) discrete tokens으로 변환하는 encoder–decoder (dVAE)를 학습***

<br>

### a) Quantize 과정

**(1) Encoder**

- Input pixel: 256x256 RGB

- Patch size = 8x8

- Grid 개수 = 32x32 =1,024개의 token

  → **32×32 grid** of logits (8192 vocab)

**(2) Vector quantization**

- (Gumbel-softmax Relaxation 사용으로) discrete token 선택

**(3) Decoder** 

- Token grid → 복원 Image

<br>

### b) 특징

- Codebook 크기 $$K=8192$$
- Spatial downsample factor = 8
  - (i.e., 8x8 pixel이 1개의 patch/token)

- Gumbel-softmax $$\tau$$ 값 annealing

<br>

### c) Summary

- Pixel 대신 “Image token”을 만드는 Image 전처리 VQ 모듈을 만듬
- 참고: 이 단계에서는 Text는 사용되지 않는다!

<br>

## (2) Stage 2: Text + Image Tokens Joint Modeling (Prior 학습)

### a) Input

(1) Text: BPE로 최대 256 tokens (vocab=16,384)

- **BPE**로 문자 시퀀스를 16k vocab의 token으로 변환

(2) Image: dVAE로 1024 tokens (vocab=8,192)

- **dVAE encoder**가 256×256 이미지를 **32×32 grid의 discrete codebook index(8192 vocab)**로 변환

$$\rightarrow$$ 둘을 **하나의 시퀀스로 concatenate**

```
[ TEXT_1, TEXT_2, ..., TEXT_T,   IMG_1, IMG_2, ..., IMG_K ]
```

<br>

### b) Model Architecture

- **12B Autoregressive Transformer**
  - 특징: **Decoder-only**
  - Details: 총 64 layers, 62 attention heads per layer
- Attention
  - Image token: Text token을 자유롭게 attend 가능
  - Image 내부: row / column / convolutional sparse mask 사용

<br>

### c) Training

- NTP (with CE loss)

- Cross-entropy to predict next token
- Text loss 1/8, Image loss 7/8 가중
- Image token은 argmax로 선택 (sampling noise 제외)
