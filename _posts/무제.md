LLM의 Inference를 위한 대표적인 패키지 세 가지를 소개하면 다음과 같다.

1. **Hugging Face Transformers**
   - 가장 널리 사용되는 라이브러리 중 하나로, 다양한 LLM을 쉽게 로드하고 Inference할 수 있도록 지원한다.
   - PyTorch와 TensorFlow를 모두 지원하며, `AutoModelForCausalLM` 등을 활용해 편리하게 사용할 수 있다.
   - `accelerate`, `bitsandbytes`와 같은 라이브러리와 함께 사용하면 메모리 최적화도 가능하다.
2. **vLLM**
   - NVIDIA TensorRT-LLM 및 FasterTransformer 등과 경쟁하는 고성능 Inference 엔진으로, 특히 Serving에 최적화되어 있다.
   - PagedAttention을 활용한 효율적인 KV 캐싱 기법을 제공해 속도와 메모리 사용량을 개선한다.
   - `vllm.engine.AsyncEngine` 등을 사용하여 빠르고 확장성 있는 Inference를 수행할 수 있다.



```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

input_text = "What is the capital of France?"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

```



```
from vllm import LLM, SamplingParams

model_name = "meta-llama/Llama-2-7b-chat-hf"
llm = LLM(model=model_name, dtype="bfloat16")

sampling_params = SamplingParams(max_tokens=50)
output = llm.generate("What is the capital of France?", sampling_params)

print(output[0].outputs[0].text)

```


