---
title: ìµœì í™” ê¸°ë²• ì‹¬í™”1 - GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”
categories: [DLF, LLM, PYTHON, MULT]
tags: []
excerpt: 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”

<br>

## Contents

1. ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©
2. GPU ë©”ëª¨ë¦¬ ê³„ì‚° - êµ¬ì„±ìš”ì†Œ
   1. Model Parameter
   2. Gradient
   3. Optimizer State
3. Optimizer ë¹„êµ
   1. AdamW
   2. Lion
   3. 8-bit Adam
   4. Adafactor
4. Accumulated Step & Gradient Checkingpoint & Context Length
   1. Accumulated Step
   2. Gradient Checkingpoint
   3. Context Length

5. Summary

<br>

# 1. ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©

1. Optimizer
2. Accumulated Steps
3. Gradient Checkingpoint
4. ìì› ìµœì í™”

<br>

# 2. GPU ë©”ëª¨ë¦¬ ê³„ì‚° - êµ¬ì„±ìš”ì†Œ

## (1) Model Parameter

- íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ $$P$$ë¼ê³  í•˜ë©´,
- **FP32 (32-bit) ì‚¬ìš© ì‹œ:** $$4P$$ ë°”ì´íŠ¸
- **BF16 / FP16 ì‚¬ìš© ì‹œ:** $$2P$$ ë°”ì´íŠ¸
- ex) **70B ëª¨ë¸, FP16** â†’ 2Ã—70B=140GB

<br>

## (2) **Gradient**

- í•™ìŠµ ì¤‘ì—ëŠ” ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ gradientë¥¼ ì €ì¥í•´ì•¼ í•¨!
- **FP32 ì‚¬ìš© ì‹œ:** $$4P$$ ë°”ì´íŠ¸
- **BF16 / FP16 ì‚¬ìš© ì‹œ:** $$2P$$ ë°”ì´íŠ¸
- ex) **70B ëª¨ë¸, FP16** â†’ 2Ã—70B=140GB

<br>

## (3) **Optimizer State (Adam ê¸°ì¤€)**

- Adam: 2ê°œì˜ ëª¨ë©˜í…€ ë²¡í„° (m,v)ë¥¼ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ,
- **FP32 ì‚¬ìš© ì‹œ:** $$8P$$ ë°”ì´íŠ¸
- **BF16 / FP16 ì‚¬ìš© ì‹œ:** $$4P$$ ë°”ì´íŠ¸
- ex) **70B ëª¨ë¸, FP16** â†’ 4Ã—70B=280G

<br>

### **ì´í•© ì •ë¦¬ (FP16 + Adam ê¸°ì¤€)**

|          | ëª¨ë¸ íŒŒë¼ë¯¸í„° | Gradient | Optimizer ìƒíƒœ | í•©ê³„      |
| -------- | ------------- | -------- | -------------- | --------- |
| 70B ëª¨ë¸ | 140GB         | 140GB    | 280GB          | **560GB** |

<br>

# 3. Optimizer ë¹„êµ

## **(1) AdamW (ê¸°ë³¸ Adam í¬í•¨)**

- AdamWëŠ” 1ì°¨, 2ì°¨ ëª¨ë©˜í…€ ë²¡í„° $$(m,v)$$ë¥¼ ì €ì¥í•¨.
- **FP32 ê¸°ì¤€**: $$8P$$ ë°”ì´íŠ¸ ($$4P$$ for $$m$$, $$4P$$ for $$v$$)
- **FP16 / BF16 ê¸°ì¤€**: $$4P$$ ë°”ì´íŠ¸
- âœ… **ì¥ì **: ì•ˆì •ì ì¸ í•™ìŠµ
- âŒ **ë‹¨ì **: ë©”ëª¨ë¦¬ ì†Œëª¨ê°€ í¼

<br>

## **(2) Lion (Less Memory Intensive Optimizer)**

- AdamWì™€ ë‹¬ë¦¬ 2ì°¨ ëª¨ë©˜í…€ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
- **FP32 ê¸°ì¤€**: $$4P$$ ë°”ì´íŠ¸ ($$4P$$ for $$m$$)
- **FP16 / BF16 ê¸°ì¤€**: $$2P$$ ë°”ì´íŠ¸
- âœ… **ì¥ì **: ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ
- âŒ **ë‹¨ì **: ì¼ë¶€ ëª¨ë¸ì—ì„œ AdamW ëŒ€ë¹„ í•™ìŠµ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥

<br>

## **(3) 8-bit Adam (Quantized Adam)**

- **ëª¨ë©˜í…€ ë²¡í„°ë¥¼ 8-bitë¡œ ì–‘ìí™”**í•˜ì—¬ ì €ì¥í•¨.
- ì¼ë°˜ì ìœ¼ë¡œ $$2P$$ ë°”ì´íŠ¸ë¡œ ì¤„ì–´ë“¦.
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:
  - $$2P$$ ë°”ì´íŠ¸ (8-bitë¡œ ì €ì¥ëœ $$m,v$$)
  - ì¶”ê°€ì ì¸ ë³´ì • ë³€ìˆ˜ í•„ìš” ì‹œ ì•½ê°„ì˜ ì¶”ê°€ ë©”ëª¨ë¦¬
- âœ… **ì¥ì **: ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼ í¼
- âŒ **ë‹¨ì **: ì¼ë¶€ ëª¨ë¸ì—ì„œ ì•ˆì •ì„± ë¬¸ì œ ë°œìƒ ê°€ëŠ¥

<br>

## **(4) Adafactor (Extreme Memory Efficiency)**

- ëª¨ë©˜í…€ì„ **í–‰ë ¬ ë¶„í•´ ë°©ì‹**ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê·¹ë„ë¡œ ì ˆê°.
- ì¼ë°˜ì ìœ¼ë¡œ **$$O(\sqrt{P})$$ ìˆ˜ì¤€ì˜ ë©”ëª¨ë¦¬ë§Œ í•„ìš”**í•¨.
- âœ… **ì¥ì **: ì´ˆëŒ€í˜• ëª¨ë¸ í•™ìŠµ ì‹œ ë§¤ìš° íš¨ìœ¨ì 
- âŒ **ë‹¨ì **: ì„±ëŠ¥ ìµœì í™”ê°€ ì–´ë µê³ , íŠ¹ì • ì„¤ì •ì—ì„œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ

<br>

## (5) LoMo (Low-Memory Optimizer)

- 1ì°¨ ëª¨ë©˜í…€ ($$m$$)ë§Œ ì €ì¥í•˜ê³  2ì°¨ ëª¨ë©˜í…€ì„ ì œê±°í•˜ì—¬ **ìµœì†Œí•œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©**
- **FP32 ê¸°ì¤€**: $$2P$$ ë°”ì´íŠ¸ ($$2P$$ for $$m$$)
- **FP16 / BF16 ê¸°ì¤€**: $$P$$ ë°”ì´íŠ¸
- âœ… **ì¥ì **: ëª¨ë“  ì˜µí‹°ë§ˆì´ì € ì¤‘ ê°€ì¥ ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- âŒ **ë‹¨ì **: AdamWë³´ë‹¤ í•™ìŠµ ì•ˆì •ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŒ

<br>

![figure2](/assets/img/llm/img621.png)

![figure2](/assets/img/llm/img622.png)

<br>

### **Optimizer ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (FP16 ê¸°ì¤€)**

| ì˜µí‹°ë§ˆì´ì €     | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰     |
| -------------- | ----------------- |
| **AdamW**      | $$4P$$            |
| **Lion**       | $$2P$$            |
| **8-bit Adam** | $$2P$$ (ì–‘ìí™”ë¨) |
| **Adafactor**  | $$O(\sqrt{P})$$   |
| **LoMo**       | $$P$$             |

**ğŸ’¡ ê²°ë¡ :**

- **ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì“¸ ìˆ˜ ìˆë‹¤ë©´** â†’ **AdamW** ì‚¬ìš© (ì•ˆì •ì ì¸ í•™ìŠµ)
- **ë©”ëª¨ë¦¬ ì ˆì•½ì´ í•„ìš”í•˜ë‹¤ë©´** â†’ **8-bit Adam, Lion**
- **ê·¹ë‹¨ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½ì´ í•„ìš”í•˜ë‹¤ë©´** â†’ **Adafactor, LoMo**

<br>

# 4. Accumulated Step & Gradient Checkingpoint & Context Length

## (1) Accumulated Step 

- ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš°, Batch sizeë¥¼ ëŠ˜ë¦¬ëŠ” ëŒ€ì‹  Accumulated stepë¥¼ ëŠ˜ë ¤ë„ ë¨!
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  & ë” ë§ì€ ë°°ì¹˜ë¥¼ í†µí•´ weightë¥¼ ì—…ë°ì´íŠ¸

- ë‹¨ì /ìœ ì˜í• ì 
  - (1) ë” ê¸´ í›ˆë ¨ ì‹œê°„
  - (2) lr, schedulerë“±ì„ ì˜ ì¡°ì •í•´ì¤˜ì•¼!

<br>

## (2) Gradient Checkingpoint

ìœ„ì—ì„œë„ ì–¸ê¸‰í–ˆë“¯, **Gradients**ë˜í•œ í° ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•¨!

ì¼ë°˜ì ì¸ NN í•™ìŠµ ì‹œ, forwardì—ì„œ gradientsë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ **ê° ë…¸ë“œì˜ ê³„ì‚°ê°’ì„ ì €ì¥**í•˜ê²Œ ë¨

- ì¥ì ) ë¹ ë¦„ (backpropì‹œ í™œìš©í•  ë•Œ, ì¬ê³„ì‚° ë¶ˆí•„ìš”)
- ë‹¨ì ) ë©”ëª¨ë¦¬ ì†Œëª¨ëŸ‰í•˜

$$\rightarrow$$ Grandient checking pointë¥¼ enable í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼!

<br>

ê¸°íƒ€ ì‚¬í•­

- QLoRAì˜ ê²½ìš°) `prepare_kbit_training()` ì ìš© ì‹œ gradient checking pointê°€ í’€ë¦¬ëŠ” ê²½ìš°ê°€ ìˆìŒ!
  - í•´ê²°ì±…: `SFTTrainer`ë¥¼ ì“°ë©´, ì•ë‹¨ì—ì„œ `prepare_kbit_training()`ì´ ë¶ˆí•„ìš”í•´ì„œ, ë©”ëª¨ë¦¬ì ì¸ ì´ì  ë³¼ ìˆ˜ ìˆìŒ

![figure2](/assets/img/llm/img623.png)

<br>

## (3) Context Length

ë¬´ì¡°ê±´ ê¸´ context lengthê°€ ì¢‹ì€ ê²ƒì€ ì•„ë‹ ìˆ˜ ìˆë‹¤!

($$\because$$ ë©”ëª¨ë¦¬ì ì¸ ì¸¡ë©´)

<br>

### KV Cache ë©”ëª¨ë¦¬!

2 x batch size x \# layers x \# heads x d_head x **CONTEXT LENGTH** x precision

<br>

# 5. Neft Tune

https://arxiv.org/pdf/2310.05914

- What? LLMì˜ fine-tuning ê¸°ë²• ì¤‘ í•˜ë‚˜
- Goal?  Modelì˜ **robustness**ë¥¼ ë†’ì´ì!
- How? ***Add random noise to the embedding vectors of the training data during the forward pass of fine-tuning***

- Training (O), Inference (X) ì‹œì— ë¶€ì—¬!

![figure2](/assets/img/llm/img627.png)

![figure2](/assets/img/llm/img628.png)

<br>

# 6. Summary

![figure2](/assets/img/llm/img624.png)

<br>

# Reference

https://fastcampus.co.kr/data_online_gpu
