---
title: 14.Policy Gradient
categories: [RL]
tags: [Reinforcement Learning]
excerpt: Policy Gradient
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 13. Policy Gradient ]

( 아래 내용은 SKplanet Tacademy의 강화학습 강좌를 참고하여 학습한 내용입니다 )

## 1. Introduction

<img src="https://camo.githubusercontent.com/8c08926ab1ceb83928c865d0bc15cee988b53bd1/68747470733a2f2f79616e70616e6c61752e6769746875622e696f2f696d672f746f7263732f6163746f722d6372697469632e706e67" width="550" /> 

https://camo.githubusercontent.com/

강화학습 (RL)는 위의 그림처럼 크게 세 가지로 나눌 수 있다.

- 1 ) **Value Based** : 
  최적의 행동을 찾기 위해 **Value Function**을 최대화"하는 간접적인 방향으로 모델 학습



- 2 ) **Policy Based** : 
  최적의 행동을 찾기 위해 직접적으로 최적의 **policy**를 학습



- 3 ) **Actor Critic** : 
  **Value Function과 Policy**를 모두 이용하여 최적의 행동을 찾음



지금 까지 우리가 앞에서 공부해온 방법들은 모두 "가치 함수(Value Function)을 최대화"함으로써 문제를 풀려고 한 **1) Value Based** approach라고 할 수 있다. 하지만, 앞으로 공부할 내용은, 보다 많이 이용되고 있는 **Policy Based & Actor Critic**

<br>

## 2. Why Policy Based RL?
***goal : "모든 state로부터 반환되는 값들을 최대화하는 최적의 정책 $$\pi$$ 학습"***


Value Function을 사용하지 않고 직접적으로 policy를 구하려는 것을 **Policy Based** 라고 한다. ( 그 중 대표적인 방법이 Policy Gradient이다. 이름에서도 알 수 있듯, 이는 neural net을 이용하여 gradient descent/ascent의 방법으로 optimize를 한다. )



그렇다면, 왜 Policy Based 방법이, 기존의 Value Based 방법보다 좋을까?

- 1 ) **연속적 행동 공간(continuous action space)**의 경우에는 value based approach로는 해결할 수 없다. 
  ( Policy Gradient는 연속 행동 공간으로 된 환경에도 적용 가능하다 )

- 2 ) **Stochastic Policy**를 배울 수 있다.

  ( 가위바위보를 생각해보자. Q-Learning에서는 가위/바위/보 중 딱 하나의 최적의 행동만 있지만, Policy Gradient에서는 각각의 가위/바위/보에 확률을 부여하여 행동할 수 있다. )

<br>

## 3. Policy Gradient

### 1 ) Objective Function of Policy Gradient?

Neural Net을 사용해서 Gradient Descent/Ascent 방법으로 푸는 것은 우리에게 너무 익숙하다. 그런데 생각해보자. Q-Learning에서는 Value Function이 있어서, Value-Function Approximation을 사용하여 Loss Function도 설정하고, 이를 minimize하는 방향으로 update를 할 수 있었다. 그런데 Policy Gradient는? ***무엇을 우리의 Loss Function(혹은 Objective Function)으로 잡아야하는가?***

<br>

그래서 우리는 우리의 정책 $$\pi$$의 성능을 나타낼 수 있는 함수를 설정해야한다. 이 성능을 나타내는 함수를 $$U(\theta)$$ 라고 해보자. $$U(\theta)$$는 어떤 식으로 정의해야할까?

<br>

직관적으로 생각해보자. 우리는 $$U(\theta)$$를 단순히 **"특정 policy에 따라 행동 했을 때, 모든 state에서 받게 되는 return들의 합"** 으로 정의할 수 있겠다. 식으로 나타내면 다음과 같다.

$$U(\theta) = E(\sum_{t=0}^{T}R(s_t,a_t) \mid \pi_{\theta})$$



하지만 뭔가 쉽지 않아보인다. 이를 미분해야하는데, 기대값이 씌워져있다. 이를 없애기 위해, 우리는 다음과 같이 새로운 변수 $$\tau$$를 도입할 것이다.

let $$R(\tau)$$ = $$\sum_{t=0}^{T}R(s_t,a_t)$$

then, $$U(\theta) = E(\sum_{t=0}^{T}R(s_t,a_t) \mid \pi_{\theta}) = \sum_{\tau}P(\tau \mid \theta)R(\tau)$$



### 2) Gradient of objective function, $$U(\theta)$$

기대값이 사라졌기 때문에, 우리는 이제 위 식에 $$\theta$$에 대해 gradient를 취할 수 있다.

$$\begin{align*}
\bigtriangledown_{\theta}U(\theta)&= \bigtriangledown_{\theta}\sum_{\tau}P(\tau \mid \theta)R(\tau) \\
&= \sum_{\tau}R(\tau) \bigtriangledown_{\theta} P(\tau \mid \theta) \\
&= \sum_{\tau}\frac{P(\tau \mid \theta)}{P(\tau \mid \theta)} R(\tau) \bigtriangledown_{\theta} P(\tau \mid \theta) \\
&= \sum_{\tau}P(\tau \mid \theta) R(\tau)\frac{\bigtriangledown_{\theta}P(\tau \mid \theta)}{P(\tau \mid \theta)} \\
&= \sum_{\tau}P(\tau \mid \theta)R(\tau)\bigtriangledown_{\theta}logP(\tau \mid \theta)
\end{align*}$$

<br>

위 식을 정리하면 다음과 같다.

$$\begin{align*}
\bigtriangledown_{\theta}U(\theta)&=  \sum_{\tau}P(\tau \mid \theta)R(\tau)\bigtriangledown_{\theta}logP(\tau \mid \theta) \\
&= E_{\tau}[R(\tau) \bigtriangledown_{\tau}logP(\tau \mid \theta)]
\end{align*}$$

<br>

복잡해보이는 위 식은 사실상 직관적으로 해석하면 매우 그럴 듯 하다.

만약 $$R(\tau)$$가 양수라면, 해당 경로의 log probability를 증가시키는 방향으로 update가 이루어질 것이다. 반대로,  음수라면 반대로 해당 경로의 log probability를 감소시키는 방향으로 update가 될 것이다.

<br>

여기서 끝이 아니다. 위 식 $$\bigtriangledown_{\theta}U(\theta) = E_{\tau}[R(\tau) \bigtriangledown_{\tau}logP(\tau \mid \theta)]$$ 에서, 우리는 변수 $$\tau$$를 제거하고 싶다. 우선 먼저  $$\bigtriangledown_{\theta}logP(\tau \mid \theta)$$에 대해서 $$\tau$$를 제거해보자.



$$ \bigtriangledown_{\theta}logP(\tau \mid \theta)= \bigtriangledown_{\theta}log[\prod_{t=0}^{T}P(s_{t+1} \mid s_t,a_t) \ast \pi_{\theta}(a_t \mid s_t)]  $$

( 위 식에서, $$P(s_{t+1} \mid s_t,a_t)$$ 는 "**모델에 관한 부분**", $$\pi_{\theta}(a_t \mid s_t)$$ 는 "**정책에 관한 부분**"이다. )



$$\begin{align*}
\bigtriangledown_{\theta}logP(\tau \mid \theta)&= \bigtriangledown_{\theta}log[\prod_{t=0}^{T}P(s_{t+1} \mid s_t,a_t) \ast \pi_{\theta}(a_t \mid s_t)] \\
&= \bigtriangledown_{\theta}[\sum_{t=0}^{\tau}logP(s_{t+1} \mid s_t, a_t) + \sum_{t=0}^{\tau}log \pi_{\theta}(a_t \mid s_t)]\\
&= \bigtriangledown_{\theta}\sum_{t=0}^{\tau}log \pi_{\theta}(a_t \mid s_t)\\
&= \sum_{t=0}^{\tau}\bigtriangledown_{\theta}log \pi_{\theta}(a_t \mid s_t)\\
\end{align*}$$

로 정리됨을 알 수 있다.

보면, 모델에 관한 부분인 $$P(s_{t+1} \mid s_t,a_t)$$는 $$\theta$$와 무관하기 때문에 사라진 것을 확인할 수 있다. 이를 통해, 우리는 위 방법이 **model-free**한 경우에도 적용될 수 있음을 다시 한번 확인할 수 있다!



자 이제 다시 위 식을  $$\bigtriangledown_{\theta}U(\theta)$$에 넣고 다시 정리해보자.

$$\begin{align*}
\bigtriangledown_{\theta}U(\theta)&=   E_{\tau}[R(\tau) \bigtriangledown_{\tau}logP(\tau \mid \theta)] \\
&= E_{\tau}[R(\tau)(\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log \pi_{\theta}(a_t \mid s_t))] \\
&= E_{\tau}[(\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log \pi_{\theta}(a_t \mid s_t))(\sum_{t=0}^{\tau}R(s_t,a_t))] \\
&= E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t)(\sum_{k=0}^{t-1}R(s_k,a_k) + \sum_{k=t}^{\tau}R(s_k,a_k))]\\
&= E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) \sum_{k=t}^{\tau}R(s_k,a_k)]\\
&= E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t]
\end{align*}$$



위 식에서 기댓값은, sampling을 한 뒤 그것들의 sample mean을 이용할 수 있다.

$$\bigtriangledown_{\theta}U(\theta) = E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t] = \frac{1}{m} \sum_{t=1}^{m} \sum_{t=0}^{\tau} \bigtriangledown_{\theta} log \pi_{\theta}(a_t \mid s_t)G_t$$



<img src="https://miro.medium.com/max/2132/1*94EI9DpoXnWa6oLHvh14pw.jpeg" width="850" /> https://miro.medium.com/max/2132/1*94EI9DpoXnWa6oLHvh14pw.jpeg



다음 포스트에서는, 위에서 구한 objective function의 gradient를 사용한 "Monte Carlo Policy Gradient인 **REINFORCE**에 대해 배우고 코드로 구현해볼 것이다.

