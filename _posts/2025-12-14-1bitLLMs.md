---
title: The Era of 1-bit LLMs; All Large Language Models are in 1.58 Bits
categories: [LLM, NLP]
tags: []
excerpt: arxiv 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

```
Ma, Shuming, et al. "The era of 1-bit llms: All large language models are in 1.58 bits." arXiv preprint arXiv:2402.17764 (2024).
```

참고: 

- https://aipapersacademy.com/the-era-of-1-bit-llms/
- https://arxiv.org/abs/2402.17764

<br>

### Contents

1. Post-training Quantization
2. Abstract of BitNet b1.58
3. Benefits of BitNet b1.58
   1. Additions Instead Of Multiplications
   2. Feature Filtering
   3. Reduce Cost Without Performance Penalty
4. Model Architecture
5. Experiments

<br>

# 1. Post-training Quantization

LLM is getting larger and larger!

$$\rightarrow$$ How to **efficiently** run LLMs?

<br>

**Quantization**

- Process of **reducing the precision** of the model weights

- e.g., Converting the model weights from **float16 to int8** 

  $$\rightarrow$$ So each weight is one byte in memory instead of four

- Limitation: ***Decrease in the model accuracy***

<br>

![figure2](/assets/img/llm/img159.png)

<br>

# 2. Abstract of BitNet b1.58

![figure2](/assets/img/llm/img160.png)

Propose **BitNet b1.58**

<br>

### Three key points

- (1)**Reduce cost, while maintaining performance**

- (2) **Ternary weights**

  - Every weight is either -1, 0 or 1

    $$\rightarrow$$ Need less than 16 bits to represent the three possible values!

  - How many bits are required? **$$\log_2(3) \approx 1.58$$**

    $$\rightarrow$$ model weights are a bit more than 1 bit!!

- (3) **Trained from scratch** 

  - NOT adapted after the training

    $$\rightarrow$$ The model learns **during training** how to work with **ternary weights**

<br>

# 3. Benefits of BitNet b1.58

## (1) Additions Instead Of Multiplications

![figure2](/assets/img/llm/img161.png)

<br>

## (2) Feature Filtering

Variant of the original BitNet model

- (Original) BitNet = Each weight is either -1 or 1

- (Proposed) BitNet 1.58 = Addition of 0 

  $$\rightarrow$$ Allows the model to **filter out features** & significantly improve the latency!

<br>

## (3) Reduce Cost Without Performance Penalty

Can match **full precision models performance**

( while dramatically **reducing the cost** to tun the models )

<br>

# 4. Model Architecture

![figure2](/assets/img/llm/img162.png)

<br>

Same layout as transformers

- Stacking blocks of self-attention
- Feed-forward networks. 

<br>

Difference?

- Instead of the regular matrix multiplication, use **BitLinear**

  $$\rightarrow$$ Limit the model weights to the **possible values of (-1,0,1)**

<br>

### Constrain weights to ternary values

$$\begin{gathered}
\widetilde{W}=\operatorname{RoundClip}\left(\frac{W}{\gamma+\epsilon},-1,1\right), \\
\operatorname{RoundClip}(x, a, b)=\max (a, \min (b, \operatorname{round}(x))), \\
\gamma=\frac{1}{n m} \sum_{i j} \mid W_{i j} \mid  .
\end{gathered}$$.

How to ensure that the weights will only be **-1, 0 or 1**?

$$\rightarrow$$ Use **absolute mean quantization**. 

- Step 1) Scale the weight matrix by its average absolute value.
- Step 2) Round each weight to the nearest number among the three possible options

<br>

# 5. Experiments

![figure2](/assets/img/llm/img163.png)

![figure2](/assets/img/llm/img164.png)

![figure2](/assets/img/llm/img165.png)
