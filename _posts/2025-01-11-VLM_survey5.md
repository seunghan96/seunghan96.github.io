---
title: (VLM survey) (Part 5; VLM Knowledge Distillation)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024

---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey

https://arxiv.org/pdf/2304.00685

<br>

# Contents

- (7) VLM Knowledge Distillation



# 7. VLM Knowledge Distillation

As VLMs capture generalizable knowledge that covers a wide range of visual and text concepts, several studies explore how to distil the general and robust VLM knowledge while tackling complex dense prediction tasks such as object detection and semantic segmentation. This section presents the motivation of distilling knowledge from VLMs as well as two groups of knowledge distillation studies on the tasks of semantic segmentation and object detection.

<br>

## (1) Motivation of Distilling Knowledge from VLMs

Different from VLM transfer that generally keeps the original VLM architecture intact in transfer [31], [132], [136], VLM knowledge distillation distils general and robust VLM knowledge to task-specific models without the restriction of VLM architecture, benefiting task-specific designs while tackling various dense prediction tasks [36], [173], [174]. For example, knowledge distillation allows transferring the general VLM knowledge to tackle detection tasks while taking the advantages of state-of-the-art detection architectures such as Faster R-CNN [55] and DETR [62].

<br>

## (2) Common Knowledge Distillation Methods

As VLMs are generally pre-trained with architectures and objectives designed for image-level representation, most VLM knowledge distillation methods focus on transferring image-level knowledge to region- or pixel-level tasks such as object detection and semantic segmentation. Table 5 shows a list of VLM knowledge distillation methods.

### P1) for Object Detection

Open-vocabulary object detection [193] aims to detect objects described by arbitrary texts, i.e., objects of any categories beyond the base classes. As VLMs like CLIP are trained with billion-scale image-text pairs that cover very broad vocabulary, many studies explore to distill VLM knowledge to enlarge the detector vocabulary. For example, ViLD [36] distills VLM knowledge to a two-stage detector whose embedding space is enforced to be consistent with that of CLIP image encoder. Following ViLD, HierKD [186] explores hierarchical global-local knowledge distillation, and RKD [187] explores region-based knowledge distillation for better aligning region-level and image-level embeddings. ZSD-YOLO [198] introduces self-labelling data augmentation for exploiting CLIP for better object detection. OADP [201] preserves proposal features while transferring contextual knowledge. BARON [200] uses neighborhood sampling to distill a bag of regions instead of individual regions. RO-ViT [199] distills regional information from VLMs for open-vocabulary detection.

<br>

Another line of research explores VLM distillation via prompt learning [165]. For example, DetPro [37] introduces a detection prompt technique for learning continuous prompt representations for open-vocabulary object detection. PromptDet [188] introduces regional prompt learning for aligning word embeddings with regional image embeddings. Additionally, several studies [180], [181], [189], [194], [197] explore VLM-predicted pseudo labels to improve object detectors. For example, PB-OVD [189] trains object detectors with VLM-predicted pseudo bounding boxes while XPM [194] introduces a robust cross-modal pseudo-labeling strategy that employs VLM-generated pseudo masks for open-vocabulary instance segmentation. P3OVD [197] exploits prompt-driven self-training that refines the VLMgenerated pseudo labels with fine-grained prompt tuning.

<br>

### P2) for Semantic Segmentation

**KD for open-vocabulary semantic segmentation**

 leverages VLMs to enlarge the vocabulary of segmentation models, aim to segment pixels described by arbitrary texts (i.e., any categories of pixels beyond base classes). For example, [35], [180], [181] achieve openvocabulary semantic segmentation by first class-agnostic segmentation by grouping pixels into multiple segments and then segment recognition with CLIP. CLIPSeg [175] introduces a lightweight transformer decoder to extend CLIP for semantic segmentation. LSeg [176] maximizes the correlation between CLIP text embeddings and pixel-wise image embedding encoded by segmentation models. ZegCLIP [174] employs CLIP to generate semantic masks and introduces a relationship descriptor to mitigate overfitting on base classes. MaskCLIP+ [163] and SSIW [177] distillknowledge with VLM-predicted pixel-level pseudo labels. FreeSeg [185] generates mask proposals firstly and then performs zero-shot classification for them. 



**KD for weakly-supervised semantic segmentation**

 aims to leverage both VLMs and weak supervision (e.g., image-level labels) for semantic segmentation. For example, CLIP-ES [184] employs CLIP to refine the class activation map by deigning a softmax function and a class-aware attention-based affinity module for mitigating the category confusion issue. CLIMS [183] employs CLIP knowledge to generate high-quality class activation maps for better weakly-supervised semantic segmentation.

<br>

## (3) Summary & Discussion

In summary, most VLM studies explore knowledge distillation over two dense visual recognition tasks, namely, object detection and semantic segmenting, where those for the former aim to better align image-level and objectlevel representations while those for the latter focus on tackling the mismatch between image-level and pixel-level representations. They can also be categorized based on their methodology, including feature-space distillation that enforces embedding consistency between VLMâ€™s encoder and the detection (or segmentation) encoder and pseudolabelling distillation that employs VLM-generated pseudo labels to regularize detection or segmentation models. Moreover, compared with VLM transfer, VLM knowledge distillation has clearly better flexibility of allowing different downstream networks regardless of the original VLMs.

<br>

# 8. Performance Comparison

## (1) Performance of VLM Pretraining

## (2) Performance of VLM Transfer Learning

## (3) Performance of VLM Knowledge Distillation

## (4) Summary

<br>

# 9. Future Directions

VLM enables effective usage of web data, zero-shot prediction without any task-specific fine-tuning, and openvocabulary visual recognition of images of arbitrary categories. It has been achieving great success with incredible visual recognition performance. In this section, we humbly share several research challenges and potential research directions that could be pursued in the future VLM study on various visual recognition tasks

