---
title: (VLM survey) (Part 5; VLM Knowledge Distillation)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024

---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey

https://arxiv.org/pdf/2304.00685

<br>

# Contents

- (7) VLM Knowledge Distillation



# 7. VLM Knowledge Distillation

VLMs capture **generalizable** knowledge!

-  Covers a **wide range** of visual and text concepts

<br>

How to distill such **general** knowledge, while tackling complex **dense** prediction tasks?

- e.g., Object detection and semantic segmentation

<br>

## (1) Motivation of Distilling Knowledge from VLMs

VLM transfer vs. VLM knowledge distillation 

- (1) VLM **transfer** 
  - Generally ***keeps*** the original VLM ***architecture***

- (2) VLM **knowledge distillation** 
  - Distills general knowledge to task-specific models ***without the restriction of VLM architecture***

<br>

## (2) Common Knowledge Distillation Methods

VLMs = Pre-trained with architectures and objectives designed for **image-level (=coarse) representation**

$\therefore$ Most VLM knowledge distillation methods focus on ...

- Transferring "**image-level**" knowledge $\rightarrow$ "**region- or pixel-level**" tasks
  - e.g., Object detection, Semantic segmentation

![figure2](/assets/img/llm/img460.png)

<br>

### P1) for Object Detection

(Task introduction) **Open-vocabulary object detection**

- Aims to detect objects described by **arbitrary texts**

  ( i.e., objects of any categories **beyond the training/base class vocabulary** )

- CLIP-based models: Cover ***very broad vocabulary***

  $\rightarrow$ Many studies explore to distill VLM knowledge to ***enlarge the detector vocabulary***

![figure2](/assets/img/llm/img461.png)

<br>

Examples

- **ViLD** (https://arxiv.org/pdf/2104.13921) (ICLR 2022)

  - Title: *Open-vocabulary object detection via vision and language knowledge distillation*

  - Goal: Advancing open-vocabulary **two-stage** object detection

  - Proposal: **ViLD** = **Vision and Language knowledge Distillation**

    - Distills (teacher) VLM knowledge to a (student) two-stage detector, 

      whose **embedding space is enforced to be consistent with that of CLIP image encoder**

  - How? Distills the knowledge from (teacher) to (student)

    - (Teacher) Pretrained open-vocabulary image classification model
    - (Student) Two-stage detector 

  - Teacher encoder: Encode category texts & image regions of object proposals. 

  - Student detector: Region embeddings of detected boxes are aligned with the text and image embeddings **inferred by the teacher**

![figure2](/assets/img/llm/img462.png)

<br>

- **HierKD** (https://arxiv.org/pdf/2203.10593) (CVPR 2022)

  - Title: *Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation*

  - Goal: Advancing open-vocabulary object **one-stage** detection 

  - Previous works) 

    - (1) Two-stage detectors

      - Employ instance-level visual-to-visual knowledge distillation to align the visual space of the detector with the semantic space of pretrained VLM

    - (2) One-stage detector

      - Absence of class0agnostic object proposals 

        $\rightarrow$ Hinders the knowledge distillation on unseen objects

  - Proposal: **HierKD** = **Hierarchical** visual-language knowledge distillation

    - Key Idea: Explores hierarchical **global-local** KD

  - Details

    - Explores hierarchical **global-local** KD of unseen categories

    - Combine the (proposed) **global-level** KD & (common) **instance-level** KD

      $\rightarrow$ To learn the knowledge of both **seen** and **unseen** categories

  ![figure2](/assets/img/llm/img463.png)

  ![figure2](/assets/img/llm/img464.png)

<br>

- **RKD** (https://arxiv.org/pdf/2207.03482) (NeurIPS 2022)

  - Title: *Bridging the gap between object and imagelevel representations for open-vocabulary detection*

  - Previous works) Open-vocabulary detection (OVD)

    - Typically enlarge their vocabulary sizes by leveraging different forms of weak supervision
    - This helps generalize to novel objects at inference. 
    - Two popular forms of weak-supervision used in OVD
      - (1) Pretrained CLIP
      - (2) Image-level supervision

    - Limitation
      - (1) CLIP: Lacks precise localization of objects
      - (2) Image-level supervision: Do not accurately specify local object regions

  - Solution: ***Object-centric alignment*** of the language embeddings from CLIP

  -  model. Furthermore, we visually ground the objects with only imagelevel supervision using a pseudo-labeling process that provides high-quality object proposals and helps expand the vocabulary during training. We establish a bridge between the above two object-alignment strategies via a novel weight transfer function that aggregates their complimentary strengths. In essence, the proposed model seeks to minimize the gap between object and image-centric representations in

  - Proposal: **RKD** = 

    - Key Idea: Explores **"region-based"** KD for aligning region-level and image-level embeddings 

- ZSD-YOLO (https://arxiv.org/pdf/2109.12066) (ICDMW 2022)

  - Title: *Zero-shot Object Detection Through Vision-Language Embedding Alignment*
  - Proposal: **ZSD-YOLO** = 
  - introduces self-labelling data augmentation for exploiting CLIP for better object detection. 

- OADP () (CVPR 2023)

  - Title: *Object-aware distillation pyramid for openvocabulary object detection*
  - Proposal: **OADP** = 
  - preserves proposal features while transferring contextual knowledge. 

- BARON [200] (CVPR 2023)

  - Title: *Aligning bag of regions for open-vocabulary object detection*
  - Proposal: **BARON** = 
  - uses neighborhood sampling to distill a bag of regions instead of individual regions. 

- RO-ViT () (CVPR 2023)

  - Title: *Region-aware pretraining for open-vocabulary object detection with vision transformers*
  - Proposal: **RO-ViT** = 
  - distills regional information from VLMs for open-vocabulary detection.

<br>

Examples (distillation via **prompt learning**)

- DetPro () (CVPR 2022)
  - Title: *Learning to prompt for open-vocabulary object detection with vision-language model*
  - Proposal: **DetPro** = 
  - introduces a detection prompt technique for learning continuous prompt representations for open-vocabulary object detection. 
- PromptDet [188] (ECCV 2022)
  - Title: *Promptdet: Towards open-vocabulary detection using uncurated images*
  - Proposal: **PromptDet** = 
  - introduces regional prompt learning for aligning word embeddings with regional image embeddings. 

<br>

Examples (distillation via **pseudo labeling**)

- PB-OVD () (ECCV 2022)
  - Title: *Open vocabulary object detection with pseudo bounding-box labels*
  - Proposal: **PB-OVD** = 
  - trains object detectors with VLM-predicted pseudo bounding boxes
- XPM [194] (CVPR 2022)
  - Title: *Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling*
  - Proposal: **XPM** = 
  - introduces a robust cross-modal pseudo-labeling strategy that employs VLM-generated pseudo masks for open-vocabulary instance segmentation. 
- P3OVD [197] (arxiv 2022)
  - Title: *P3ovd: Fine-grained visual-text prompt-driven self-training for open-vocabulary object detection*
  - Proposal: **P3OVD** = 
  - exploits prompt-driven self-training that refines the VLM generated pseudo labels with fine-grained prompt tuning.

<br>

### P2) for Semantic Segmentation

**KD for open-vocabulary semantic segmentation**

 leverages VLMs to enlarge the vocabulary of segmentation models, aim to segment pixels described by arbitrary texts (i.e., any categories of pixels beyond base classes). For example, [35], [180], [181] achieve openvocabulary semantic segmentation by first class-agnostic segmentation by grouping pixels into multiple segments and then segment recognition with CLIP. CLIPSeg [175] introduces a lightweight transformer decoder to extend CLIP for semantic segmentation. LSeg [176] maximizes the correlation between CLIP text embeddings and pixel-wise image embedding encoded by segmentation models. ZegCLIP [174] employs CLIP to generate semantic masks and introduces a relationship descriptor to mitigate overfitting on base classes. MaskCLIP+ [163] and SSIW [177] distillknowledge with VLM-predicted pixel-level pseudo labels. FreeSeg [185] generates mask proposals firstly and then performs zero-shot classification for them. 



**KD for weakly-supervised semantic segmentation**

 aims to leverage both VLMs and weak supervision (e.g., image-level labels) for semantic segmentation. For example, CLIP-ES [184] employs CLIP to refine the class activation map by deigning a softmax function and a class-aware attention-based affinity module for mitigating the category confusion issue. CLIMS [183] employs CLIP knowledge to generate high-quality class activation maps for better weakly-supervised semantic segmentation.

<br>

## (3) Summary & Discussion

In summary, most VLM studies explore knowledge distillation over two dense visual recognition tasks, namely, object detection and semantic segmenting, where those for the former aim to better align image-level and objectlevel representations while those for the latter focus on tackling the mismatch between image-level and pixel-level representations. They can also be categorized based on their methodology, including feature-space distillation that enforces embedding consistency between VLMâ€™s encoder and the detection (or segmentation) encoder and pseudolabelling distillation that employs VLM-generated pseudo labels to regularize detection or segmentation models. Moreover, compared with VLM transfer, VLM knowledge distillation has clearly better flexibility of allowing different downstream networks regardless of the original VLMs.

<br>

# 8. Performance Comparison

## (1) Performance of VLM Pretraining

## (2) Performance of VLM Transfer Learning

## (3) Performance of VLM Knowledge Distillation

## (4) Summary

<br>

# 9. Future Directions

VLM enables effective usage of web data, zero-shot prediction without any task-specific fine-tuning, and openvocabulary visual recognition of images of arbitrary categories. It has been achieving great success with incredible visual recognition performance. In this section, we humbly share several research challenges and potential research directions that could be pursued in the future VLM study on various visual recognition tasks

