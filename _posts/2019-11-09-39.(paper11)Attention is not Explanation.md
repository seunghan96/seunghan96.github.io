---
title: 39.(paper) 11.Attention is not Explanation
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: Paper Review by Seunghan Lee
---

# 11.Attention is not Explanation (2019)

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Abstract

Attention is widely used nowadays!

But, unclear what **relationship** exists between **(1) attention weights & (2) model outputs**

perform extensive experiments across a variety of NLP taks.....

$\rightarrow$ **No meaningful explanations**