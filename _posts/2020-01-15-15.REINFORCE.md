---
title: 15.Policy Gradient (1) REINFORCE ( + pytorch 구현 )
categories: [RL]
tags: [Reinforcement Learning]
excerpt: Policy Gradient, REINFORCE
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 14. REINFORCE ]

( 아래 내용은 SKplanet Tacademy의 강화학습 강좌를 참고하여 학습한 내용입니다 )



# 1. Introduction

REINFORCE는 **Policy Gradient**의 가장 간단한 (***MC를 사용한***) 방법이다. 

우리는 이전 포스트의 끝 부분에서, $$U(\theta)$$의 gradient를 다음과 같이 구했었다.

- $$\bigtriangledown_{\theta}U(\theta) = E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t]$$.

<br>

# 2. Setting
코드로 구현하기 전에, 우리가 구하고자 하는 것에 대해서 알아보자.

우리가 구한 기울기는 $$E_{\tau}[\sum_{t=0}^{\tau}\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t]$$ 로, pytorch를 통해 $$\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t$$를 계산해야 한다. 

하지만 알다시피, 우리는 input으로 직접 gradient를 넣지 않고, 그것을 미분하기 전 단계인 loss function 혹은 objective function을 넣는다.

$$G_t \ast log \pi_{\theta}(a_t \mid s_t)$$를 미분하면 코드에 $$\bigtriangledown_{\theta}log\pi_{\theta}(a_t \mid s_t) G_t$$가 되기 때문에, 우리는 $$G_t \ast log \pi_{\theta}(a_t \mid s_t)$$를 넣을 것이다.

여기서 조심해야 할 것이 있다. pytorch에서 objective function으로 넣어야 하는 default 값은 "LOSS" function으로, minimize해야하는 대상을 이 곳에 넣는다. 하지만 우리는 알다시피 위 $$U(\theta)$$를 maximize하고 싶은 것이기 때문에, 앞에 (-)를 붙여서 넣어야 한다. (Gradient Descent가 아니라 Gradient Ascent다)

<br>

자 이제 그럼 구현을 위한 모든 준비는 끝났다.  이제 구현을 해볼 것이다.

<br>

# 3. Implementation of REINFORCE

( 아래 코드는 SKplanet Tacademy의 강화학습 강좌를 참고로 하여 작성된 코드입니다. )

### 1) Import Libraries

```python
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
```



### 2) Set Hyperparameters


```python
lr = 0.0002
gamma = 0.98
```



### 3) REINFORCE

- CartPole 환경을 이용하여 테스트해볼 것이다.


```python
def main():
    env = gy.make('CartPole-v1')
    pi = Policy()
    score = 0
    print_int = 20
    
    for episode in range(2000): 
        state = env.reset()
        done = False
        
        while not done:
            prob = pi(torch.from_numpy(s).float()) # into probability
            actions = Categorical(prob).sample() # random sample ( prob에 따라 )
            state_, returns, done, info = env.step(actions.item())
            pi.put_data((r,prob[actions]))
            state = state_ # 다음 state로 넘어감
            score += returns # return을 누적해서 더함
            
        pi.train()
        
        if episode%print_int==0 & episode!=0:
            print('episode : {}, score : {}'.format(episode, score/print_int))
            score =0
            
    env.close()      
```

<br>


```python
class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.data = []
        self.fc1 = nn.Linear(4,128) 
        self.fc2 = nn.Linear(128,2) # hidden layer의 unit 개수 : 128개
        self.opt = optim.Adam(self.parameters(), lr=lr) # Adam Optimizer 사용
    
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.softmax(self.fc2(x), dim=0) # 각 행동에 대한 probability 반환
        return x
    
    def put_data(self,item):
        self.data.append(item)
    
    def train(self):
        total_return = 0
        for returns,prob in self.data[::-1]:
            total_return = returns + gamma*total_return
            loss = -torch.log(prob)*returns 
            self.opt.zero_grad()
            loss.backward()
            self.opt.step()
        self.data = []
```



