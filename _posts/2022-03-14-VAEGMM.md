---
title: VAE + GMM
categories: [GAN]
tags: [GAN]
excerpt: 2017
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Deep Unsupervised Clustering with Gaussian Mixture Gaussian Mixture VAE

<br>

![figure2](/assets/img/gan/img96.png)

## Contents

0. Abstract
1. 

<br>

# 0. Abstract

variant of **VAE** with **GMM** as prior

- goal : ***unsupervised clustering via DGM***

<br>

Problem of regular VAE : over-regularisation

$\rightarrow$ leads to cluster degeneracy

<br>

**Minimum information constraint** 

- mitigate these problems in VAE
- improve unsupervised clustering performance

<br>

# 1. Introduction

Unsupervised clustering 

- (conventional) K-means, GMM

  - limitation : similarity measures are limited to local relations in the data space

- DGM (Deep Generative Model)

  - can encode rich latent structures

  - can be used for dimensionality reduction

  - try to estimate the **density of observed data** under some **assumptions**

    ( assumption about about its latent structure )

    $\rightarrow$ They allow us to **reason about data** in more complex ways

     than in models trained purely through supervised learning. However, inference in models with complicated latent structures can be difficult. Recent breakthroughs in approximate inference have provided tools for constructing tractable inference algorithms. As a result of combining differentiable models with variational inference, it is possible to scale up inference to datasets of sizes that would not have been possible with earlier inference methods (Rezende et al., 2014). One popular algorithm under this framework is the variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014). In this paper, we propose an algorithm to perform unsupervised clustering within the VAE framework. To do so, we postulate that generative models can be tuned for unsupervised clustering by making the assumption that the observed data is generated from a multimodal prior distribution, and, correspondingly, construct an inference model that can be directly optimised using the reparameterization trick. We also show that the problem of over-regularisation in VAEs can severely effect the performance of clustering, and that it can be mitigated with the minimum information constraint introduced by Kingma et al. (2016).



# 2. Related Works

