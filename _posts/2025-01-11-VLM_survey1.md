---
title: Vision-Language Models for Vision Tasks; A Survey (Part 1)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey (Part 1)

https://arxiv.org/pdf/2304.00685

<br>

# Abstract

Vision-Language Models (VLMs) 

- Goal: Learn rich vision-language correlation
- Dataset: Web-scale image-text pairs 
- Results: Enables zero-shot predictions on various visual recognition tasks

<br>

This paper:

Review of VLMs for various visual recognition tasks

- (1) Background of visual recognition paradigms
- (2) Foundations of VLM
  - Widely-adopted network architectures
  - Pre-training objectives
  - Downstream tasks
- (3) Datasets in VLM pre-training and evaluations
- (4) Review & Categorization of existing ....
  - VLM pre-training methods
  - VLM transfer learning methods
  - VLM knowledge distillation methods
- (5) Benchmark
- (6) Research challenges & Future works

<br>

![figure2](/assets/img/llm/img381.png)

<br>

# 1. Introduction

## P1) Visual recognition

Visual recognition

- Image classification
- Object detection
- Semantic segmentation

<br>

Challenges? (Traditional ML $$\rightarrow$$ E2E DL)

- (1) Slow convergence of DNN training 
  - under learning from scratch
- (2) Laborious collection of datasets

<br>

![figure2](/assets/img/llm/img377.png)

<br>

## P2--3) VLM training paradigm

- Previous) Pretraining - Finetuning - Prediction
- Recent) Pretraining - ***Zero-shot*** Prediction
  - Step 1) VLM is pre-trained with large-scale image-text pairs
  - Step 2) Pretrained VLM can be directly applied to downstream visual recognition tasks ***w/o fine-tuning***

![figure2](/assets/img/llm/img378.png)

<br>

VLM pre-training

- By vision-language objectives [10], [18], [19]

  $$\rightarrow$$ Enable to learn image-text correspondences

-  e.g., CLIP [10]: Employs contrastive learning

  - Pretrained CLIP has achieved superior zero-shot performance on 36 visual recognition tasks 

<br>

## P4) Two line of research

1. VLMs with **transfer learning**

   - Aim for effective adaptation of pre-trained VLMs towards various tasks!

   - e.g., prompt tuning [31], [32]
   - e.g, visual adaptation [33], [34]

2. VLMs with **knowledge distillation** [35], [36], [37]
   - Explores on how to distill knowledge from VLMs to tasks

<br>

## P6) Main contributions of this work

1. Systematic review of VLMs (for visual recognition tasks)
2. Up-to-date progress of VLMs 
3. Research challenges & Potential research directions

<br>

# 2. Background

1. Development of the training paradigm of visual recognition
2. Paradigm of VLM Pre-training & Zeroshot Prediction
3. Development of the VLMs for visual recognition.

<br>

## (1) Training Paradigms for Visual Recognition

### P1) Traditional Machine Learning and Prediction

Rely heavily on feature engineering with hand-crafted features

$$\rightarrow$$ Requires domain experts 

<br>

### P2) Deep Learning from Scratch and Prediction

Enables end-to-end trainable DNNs

Two new challenges: 

- (1) Slow convergence of DNN training (under from scratch)
- (2) Laborious collection of large-scale, task-specific, and crowd-labelled data

<br>

### P3-6) Change in paradigms

Paradigm 1) Scratch + Prediction

Paradigm 2) Supervised Pre-training +  Fine-tuning + Prediction

Paradigm 3) Unsupervised Pre-training +  Fine-tuning + Prediction

- Various SSL have been proposed

**Paradigm 4) Unsupervised Pre-training + Zero-shot Prediction**

- Enables effective use of large-scale web data for pretraining
- Zero-shot predictions without task-specific fine-tuning

<br>

Improve VLMs from 3 perspectives

- (1) Collecting image-text data
- (2) Designing high-capacity models 
- (3) Designing new pre-training objectives

<br>

## (2) Development of VLMs for Visual Recognition

Great progresses since CLIP

![figure2](/assets/img/llm/img379.png)

<br>

(1) Pre-training objectives

***“single objective” $$\rightarrow$$ “multiple hybrid objectives”***

- Early VLMs: single objective
- Recent VLMs: multiple objectives 
  - Ee.g., contrastive, alignment and generative objectives

<br>

(2) Pre-training frameworks

***"multiple separate" networks $$\rightarrow$$"unified" network***

- Early VLMs: Two-tower pre-training frameworks

- Recent VLMs: One-tower pretraining framework

  $$\rightarrow$$ Encodes images and texts with a unified network 

  ( with less GPU memory & more efficient communications across data modalities )

<br>

(3) Downstream tasks

***"simple" tasks $$\rightarrow$$  "complex" tasks***

- Early VLMs: Image-level visual recognition tasks

- Recent VLMs: General-purpose 

  - Also work for dense prediction tasks 

    ( that are complex and require localization related knowledge )

<br>

# 3. VLM Foundations

VLM pre-training

- Aims to pretrain a VLM to learn image-text correlation
- For effective zero-shot predictions

<br>

Procedure

- Step 1) Text encoder & Image encoder to extract image and text features 
- Step 2) Learns the vision-language correlation with certain pre-training objectives
- Step 3) Evaluation on unseen data in a zero-shot manner

<br>

This section: **Foundations of VLM pre-training**

- a) Common network architectures 
  - For extracting image and text features
- b) Pre-training objectives 
  - For modelling vision-language correlation
- c) Frameworks 
  - For VLM pre-training
- d) Downstream tasks 
  - For VLM evaluations

<br>

## (1) Network Architectures

Notation

- Dataset $$\mathcal{D}=\left\{x_n^I, x_n^T\right\}_{n=1}^N$$, 
- Image encoder $$f_0$$ 
- Text encoder $$f_0$$, 
- Image embedding $$z_{\mathrm{n}}^I=f_\theta\left(x_n^I\right)$$ 
- Text embedding $$z_n^T=f_\phi\left(x_n^T\right)$$, 

<br>

### P1) Architectures for Image Features

(1) CNN-based

- ResNet [6]: Skip connections

(2) Transformer-based

- ViT [57]: Stack of Transformer blocks
  - Input image is first split into fixed-size patches

<br>

### P2) Architectures for Language Features

Most VLM studies (e.g., CLIP): 

- Employs Transformer ( + with minor modifications )

<br>

## (2) VLM Pretraining Objectives

Three categories:

- (1) Contrastive objectives
- (2) Generative objectives
- (3) Alignment objectives

<br>

### P1) Contrastive Objectives

**Image Contrastive Learning**

- InfoNCE and its variants
- $$\mathcal{L}_I^{\text{InfoNCE}}=-\frac{1}{B} \sum_{i=1}^B \log \frac{\exp \left(z_i^t \cdot z_{+}^t / \tau\right)}{\sum_{j=1, j \neq i}^{B+1} \exp \left(z_i^t+z_j^t / \tau\right)}$$.
  - $$z_i^I$$ : Query embedding
  - , $$\left\{z_j^I\right\}_{j=1, j \neq 1}^{B+1}$$ : Key embeddings

<br>

**Image-Text Contrastive Learning**

- Symmetrical image-text infoNCE loss 

  ( i.e., $$\mathcal{L}_{\text {mfoNCE }}^{\prime /}=$$ $$\mathcal{L}_{I \rightarrow T}+\mathcal{L}_{T \rightarrow I}$$ )

  - $$\mathcal{L}_{I \rightarrow T}$$ : Contrasts the query image with the text keys
  -  $$\mathcal{L}_{T \rightarrow I}$$: Contrasts the query text with image keys

- $$\begin{aligned}
  & \mathcal{L}_{I \rightarrow T}=-\frac{1}{B} \sum_{i=1}^B \log \frac{\exp \left(z_i^I \cdot z_i^T / \tau\right)}{\sum_{j=1}^B \exp \left(z_i^I \cdot z_j^T / \tau\right)}, \\
  & \mathcal{L}_{T \rightarrow I}=-\frac{1}{B} \sum_{i=1}^B \log \frac{\exp \left(z_i^T \cdot z_i^I / \tau\right)}{\sum_{j=1}^B \exp \left(z_i^T \cdot z_j^I / \tau\right)} .
  \end{aligned}$$.

<br>

**Image-Text-Label Contrastive Learning**

- SupCon + Image-text contrastive learning

  (i.e., $$\mathcal{L}_{\text {infoNCE }}^{I T L}=\mathcal{L}_{I \rightarrow T}^{I T L}+\mathcal{L}_{T \rightarrow I}^{I T L}$$ )

- $$\begin{aligned}
  & \mathcal{L}_{I \rightarrow T}^{I T L}=-\sum_{i=1}^B \frac{1}{ \mid \mathcal{P}(i) \mid } \sum_{k \in \mathcal{P}(i)} \log \frac{\exp \left(z_i^I \cdot z_k^T / \tau\right)}{\sum_{j=1}^B \exp \left(z_i^I \cdot z_j^T / \tau\right)}, \\
  & \mathcal{L}_{T \rightarrow I}^{I T L}=-\sum_{i=1}^B \frac{1}{ \mid \mathcal{P}(i) \mid } \sum_{k \in \mathcal{P}(i)} \log \frac{\exp \left(z_i^T \cdot z_k^I / \tau\right)}{\sum_{j=1}^B \exp \left(z_i^T \cdot z_j^I / \tau\right)},
  \end{aligned}$$.

- where $$k \in \mathcal{P}(i)=\left\{k \mid k \in B, y_k=y_i\right\}$$ 

<br>

### P2) Generative Objectives

a) **Masked Image Modeling**

- $$\mathcal{L}_{M I M}=-\frac{1}{B} \sum_{i=1}^B \log f_\theta\left(\bar{x}_i^I \mid \hat{x}_i^I\right)$$.

<br>

b) **Masked Language Modeling**

- $$\mathcal{L}_{M L M}=-\frac{1}{B} \sum_{i=1}^B \log f_o\left(\vec{x}_i^T \mid \hat{x}_i^T\right)$$.

<br>

c) **Masked Cross-Modal Modeling**

- MCM = MIM + MLM
- Details) 
  - Step 1) Given an image-text pair
  - Step 2) Randomly masks 
    - a subset of image patches 
    - a subset of text tokens
  - Step 3) Learns to reconstruct them conditioned on ..
    - unmasked image patches 
    - unmasked text tokens

- $$\mathcal{L}_{M C M}=-\frac{1}{B} \sum_{i=1}^B\left[\log f_\theta\left(\bar{x}_i^I \mid \hat{x}_i^I, \hat{x}_i^T\right)+\log f_\phi\left(\bar{x}_i^T \mid \hat{x}_i^I, \hat{x}_i^T\right)\right]$$.

<br>

d) **Image-to-Text Generation**

- Aims to predict text $$x^T$$ autoregressively 
  - based on the **image paired with $$x^T$$**

- $$\mathcal{L}_{I T G}=-\sum_{l=1}^L \log f_\theta\left(x^T \mid x_{<l,}^T z^l\right)$$.
  - where $$L$$ denotes the number of tokens to be predicted

<br>

### P3) Alignment Objectives

Align the image-text pair via ..

- (1) Global image-text matching [7], [72]
- (2) Local region-word matching [45], [67] 

on embedding space.

<br>

**Image-Text Matching**

- Models ***global*** correlation (between images and texts)
- $$\mathcal{L}_{I T}=p \log \mathcal{S}\left(z^I, z^T\right)+(1-p) \log \left(1-\mathcal{S}\left(z^I, z^T\right)\right)$$.
  - Score function $$\mathcal{S}(\cdot)$$ : Measures the **alignment probability** between the image and text
  - $$p$$ is 1 if the image and text are paired and 0 otherwise. 

<br>

**Region-Word Matching**

- Model ***local*** cross-modal correlation (between "image regions" and "words")
- For ***dense*** visual recognition tasks
  - e.g., Object detection. It can be formulated as:

- $$\mathcal{L}_{R W}=p \log \mathcal{S}^r\left(r^I, w^T\right)+(1-p) \log \left(1-\mathcal{S}^r\left(r^I, w^T\right)\right)$$.
  - where $$\left(r^I, w^T\right)$$ denotes a region-word pair
  - $$p=1$$ if the region and word are paired otherwise

<br>

## (3) VLM Pretraining Frameworks

![figure2](/assets/img/llm/img380.png)

- **Two-tower** framework

  - Encoded with two **separate** encoders

- **Two-leg** framework

  - Introduces additional **multi-modal fusion layers**

- **One-tower** VLMs

  - **Unify** vision and language learning in a single encoder

    $$\rightarrow$$ Aming to facilitate **efficient communications across data modalities**

<br>

## (4) Evaluation Setups and Downstream Tasks

### P1) Zero-shot Prediction

Most common way of evaluating VLMs’ generalization capability

<br>

**a) Image Classification**

- *What?* Aims to classify Image
- *How?* By comparing the **embeddings of images and texts**
  - where “prompt engineering” is often employed to generate task-related prompts like **“a photo of a [label].”** 

**b) Semantic Segmentation**

- *What?* Assign a category label to each pixel in images
- *How?* By comparing the **embeddings of the given image pixels and texts** 

**c) Object Detection** 

- *What?* Localize and classify objects in images
- *How?* By comparing the **embeddings of the given object proposals and texts** 

**d) Image-Text Retrieval**

- *What?* Retrieve the demanded samples from one modality given the cues from another modality
- Text-to-image retrieval
- Image-to-text retrieval 

<br>

### P2) Linear Probing

pass

<br>

# 4. Datasets

## (1) Pretraining

![figure2](/assets/img/llm/img382.png)

<br>

## (2) Evaluation

![figure2](/assets/img/llm/img383.png)

