---
title: (paper 88) Novel Class Discovery; an Introduction and Key Concepts
categories: [CV, CL, SEMI]
tags: []
excerpt: 2023
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
# Novel Class Discovery: an Introduction and Key Concepts

<br>

![figure2](/assets/img/cl/img255.png)

## Contents

0. Abstract
1. Introduction
   


<br>

# 0. Abstract

Novel Class Discovery (NCD) is a growing field where we are given during training a labeled set of known classes and an unlabeled set of different classes that must be discovered. In recent years, many methods have been proposed to address this problem, and the field has begun to mature. In this paper, we provide a comprehensive survey of the state-of-the-art NCD methods. We start by formally defining the NCD problem and introducing important notions. We then give an overview of the different families of approaches, organized by the way they transfer knowledge from the labeled set to the unlabeled set. We find that they either learn in two stages, by first extracting knowledge from the labeled data only and then applying it to the unlabeled data, or in one stage by conjointly learning on both sets. For each family, we describe their general principle and detail a few representative methods. Then, we briefly introduce some new related tasks inspired by the increasing number of NCD works. We also present some common tools and techniques used in NCD, such as pseudo labeling, self-supervised learning and contrastive learning. Finally, to help readers unfamiliar with the NCD problem differentiate it from other closely related domains, we summarize some of the closest areas of research and discuss their main differences.



# 1. Introduction

In the past decade of machine learning research, many classification models have relied heavily on the availability of large amounts of labeled data for all relevant classes. The recent success of these models is due in part to the abundance of labeled data. However, it is not always possible to have labeled data for all classes of interest, leading researchers to consider scenarios where unlabeled data is available. This "open-world" assumption is becoming increasingly more common in practical applications, where instances outside the initial set of classes may emerge [1]. To illustrate, let's examine the scenario of Figure 1. Here, instances from classes never seen during training appear at test time. An ideal model should not only be able to classify the known classes (parrots and cats), but also to discover the new ones (tigers and horses).

What is the issue? - In this example, a standard classification model is likely to incorrectly classify instances that fall outside the known classes as belonging to one of the known classes. This is a wellknown phenomenon of neural networks, where they can produce overconfident incorrect predictions, even in the case of semantically related inputs [2]. Here, a tiger would be classified as a parrot or a cat. For this reason, researchers are now exploring scenarios where unlabeled data is also available $[3,4]$. In this survey, we will focus on one such scenario, where a labeled set of known classes and an unlabeled set of unknown classes are given during training. The goal is to learn to categorize the

unlabeled data into the appropriate classes. This is referred to as "Novel Class Discovery (NCD)"1 [5].
What is the usual setup of NCD? - Illustrated in Figure 2, the training data in NCD consists of two sets of samples: one from known classes and one from unknown classes. The test set is comprised solely of samples from unknown classes. The NCD scenario belongs to Weakly Supervised Learning $[3,4]$, where methods that require all the classes to be known in advance can be distinguished from those that are able to manage classes that have never appeared during training. As an example, in Open-World Learning (OWL) [1], methods seek to accurately label samples of classes seen during training, while identifying samples from unknown classes. However, the methods in OWL are generally not tasked with clustering the unknown classes and unlabeled data is left unused. Another example is Zero-Shot Learning (ZSL) [6], where the models are designed to accurately predict classes that have never appeared during training. But some kind of description of these unknown classes is needed to be able to recognize them. On the other hand, NCD has recently gained significant attention due to its practicality and real-world applications.

Why does clustering alone fail to produce good results? - Albeit naive, unsupervised clustering is a direct solution to the NCD problem as it can sometimes be sufficient for discovering classes in unlabeled data. For example, many clustering methods have obtained an accuracy larger than $90 \%$ on the MNIST dataset $[7,8,9]$. But in the case of complex datasets, the literature shows that clustering fails $[10,11]$ compared to more sophisticated approaches. Clustering can fail for many reasons due to the assumptions that the methods make: spherical clusters, mixture of Gaussian distributions, shape

