---
title: (sLM-4) Quantization ì‹¤ìŠµ
categories: [LLM, MULT, NLP]
tags: []
excerpt: 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Quantization ì‹¤ìŠµ

## Contents 

1. Quantization ì‹œì‘í•˜ê¸°
2. Quantization ì½”ë“œ
   1. Load Model
   2. Inference (Single)
   3. Inference (Batch)

<br>

# 1. Quantization ì‹œì‘í•˜ê¸°

**ìµœê·¼ íŠ¸ë Œë“œ**

- (X) ì‘ì€ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸°
- (O) í° ëª¨ë¸ì„ ì–‘ìí™”í•˜ì—¬ ì‚¬ìš©í•˜ê¸°

<br>

íŒ¨í‚¤ì§€ ì„¤ì¹˜í•˜ê¸°

- `bitsandbytes`: ì–‘ìí™”ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” íŒ¨í‚¤ì§€

```bash
!pip install bitsandbytes==0.43.1
!pip install accelerate==0.30.1
!pip install transformers==4.39.3
```

<br>

Huggingfaceì— ë¡œê·¸ì¸í•˜ê¸°

```python
from huggingface_hub import notebook_login

notebook_login()
```

<br>

# 2. Quantization ì½”ë“œ

## (1) Load Model

```python
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
```

```python
# ì‚¬ìš©í•  ëª¨ë¸: LLaMA-3-8B-Instruct
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
```

<br>

**ì–‘ìí™”ë¥¼ ìœ„í•œ configuration**

```python
config = BitsAndBytesConfig(
    load_in_4bit=True,                    
    bnb_4bit_quant_type="nf4",            
    bnb_4bit_use_double_quant=True,       
    bnb_4bit_compute_dtype=torch.bfloat16 
)
```

<br>

`AutoModelForCausalLM.from_pretrained`ì˜ ì¸ìë¡œ,

- `quantization_config=config`ë¥¼ ì„¤ì •í•´ì£¼ë©´ ëœë‹¤!

```python
# (1) Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# (2) Model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",        
    device_map="auto",         
    quantization_config=config  
)
```

<br>

## (2) Inference (Single)

**a) Prompt ë‚´ìš©**

```python
messages = [
    {"role": "system", "content": "You are a kind robot."},
    {"role": "user", "content": "ì´ìˆœì‹ ì´ ëˆ„êµ¬ì•¼?"},
]
```

<br>

**b) Promptë¥¼ tokenizingí•˜ê¸°**

- `add_generation_prompt`: ì£¼ì–´ì§„ promptì˜ ë’¤ì—, **ìƒˆë¡­ê²Œ ìƒì„±ì„ ìš”êµ¬í•˜ëŠ” token**ì„ ë„£ì„ì§€!

```python
input_ids = tokenizer.apply_chat_template(
    messages2,
    add_generation_prompt=True,
    return_tensors="pt",
    tokenize=True
).to(model.device)
```

<br>

**c) Terminator**: ë‹¨ì–´ ìƒì„± ì¢…ê²° ì¡°ê±´ ì§€ì •!

```python
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
```

<br>

**d) ìƒì„±í•˜ê¸°**

- `input_ids`: dictionary í˜•íƒœì´ë‹¤!
- `do_sample`
  - True: ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼ (sampling)
  - False: Greedy decoding

```python
outputs = model.generate(
    input_ids=input_ids,      
    max_new_tokens=300,       
    eos_token_id=terminators, 
    do_sample=True,           
    temperature=0.7,          
    no_repeat_ngram_size=2,   
    pad_token_id=tokenizer.eos_token_id 
)
```

<br>

**e) ê²°ê³¼ í™•ì¸**

```python
response = outputs[0][input_ids['input_ids'].shape[-1]:]
print("response : ", tokenizer.decode(response, skip_special_tokens=True))
```

```
response :  ! (Sejong the Great was the fourth king of the Joseon Dynasty in Korea. He is known for his efforts to promote education and culture, and is considered one of Korea's most important monarchs.)
```

<br>

## (3) Inference (Batch)

**a) Prompt ë‚´ìš©**

```python
messages1 = [
    {"role": "system", "content": "You are a kind robot."},
    {"role": "user", "content": "ì´ìˆœì‹ ì´ ëˆ„êµ¬ì•¼?"},
]

messages2 = [
    {"role": "system", "content": "You are a kind robot."},
    {"role": "user", "content": "ì„¸ì¢…ëŒ€ì™•ì´ ëˆ„êµ¬ì•¼?"},
]
```

<br>

**b) Promptë¥¼ tokenizingí•˜ê¸°**

```python
prompt1 = tokenizer.apply_chat_template(
    messages1,
    add_generation_prompt=True,
    return_tensors="pt",
    tokenize=False
)

prompt2 = tokenizer.apply_chat_template(
    messages2,
    add_generation_prompt=True,
    return_tensors="pt",
    tokenize=False
)
```

<br>

**c) Terminator**

```python
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
```

<br>

**d) Tokenizer ì„¸íŒ…**

```python
tokenizer.padding_side = 'left'
tokenizer.pad_token_id = tokenizer.eos_token_id
```

<br>

**e) Promptë¥¼ ë°°ì¹˜í™” (ì—¬ëŸ¬ ê°œ ë¬¶ê¸°)**

- ì¶”ê°€ë¡œ, ì—¬ëŸ¬ promptë¥¼ ë¬¶ì€ ì´í›„ì˜ "input_id"ë¥¼ ì¶”ì¶œí•œë‹¤

```python
prompt_batch = [prompt1, prompt2]
input_ids_batch = tokenizer(prompt_batch, return_tensors='pt',padding="longest")['input_ids']
```

<br>

**f) ìƒì„±í•˜ê¸°**

```python
outputs = model.generate(
    input_ids=input_ids_batch,
    max_new_tokens=30,        
    eos_token_id=terminators, 
    do_sample=True,           
    temperature=0.7,          
    no_repeat_ngram_size=2,   
    pad_token_id=tokenizer.eos_token_id  
)
```

<br>

**g) ê²°ê³¼ í™•ì¸í•˜ê¸°**

- for loopì„ ëŒë©°, ì—¬ëŸ¬ ì§ˆë¬¸(prompt)ì— ëŒ€í•œ ëŒ€ë‹µë“¤ì„ ê°ê° í™•ì¸í•œë‹¤!

```python
for i, output in enumerate(outputs):
    response = output[input_ids_batch[i].shape[-1]:]
    print(f"response {i + 1}: ", tokenizer.decode(response, skip_special_tokens=True))
```

```
response 1:  I think you're asking who Yi Sun-sin is!

Yi Sun-shin (1545-1598) was a Korean admiral and
response 2:  I'm happy to help! ğŸ˜Š

Sejong the Great (1396-1450) was the fourth king of the Joseon Dynasty in
```

<br>

### Reference

- [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤] 8ê°œì˜ sLMëª¨ë¸ë¡œ ëë‚´ëŠ” sLM íŒŒì¸íŠœë‹
