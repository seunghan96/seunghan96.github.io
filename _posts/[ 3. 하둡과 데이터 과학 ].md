# [ 3. 하둡과 데이터 과학 ]

# 3.1 하둡이란?

Apache Hadoop :

- 대규모 검색 색인을 구축하려고, java로 개발된 "오픈 소스 분산 컴퓨팅" 플랫폼

  ( 기존 목적뿐만 아니라, 보다 일반적인 목적으로 널리 사용됨 )

- 핵심 기능 : **장애 허용 (fault tolerance)**

  - 확장성을 높이려함
  - 장비/노드의 장애를 발생가능한 일로 간주

- 하둡의 기초 구성요소

  - 1) **분산 파일 시스템 (Distributed File System)**
  - **2) 리소스 관리자 & 스케줄러**
  - **3) 분산 데이터 처리 프레임워크**



## 3.1.1 분산 파일 시스템 (Distributed File System)

Hadoop의 스토리지로 선택할 수 있는 "분산 파일 시스템"은 다양함

- ex) **HDFS (Hadoop Distributed File System)**

  - 바탕 : **"데이터의 중복 저장"**

  - 일반적인 파일시스템을 가진 **"여러 노드를 묶어서" 하나의 분산파일 시스템**을 구축

    $$\rightarrow$$ 덕분에, 손 쉽게 확장 가능!

<br>

HDFS 설계의 가정

- 1) 데이터의 full 스캐닝을 지원하기 위해, **"파일 순차 읽기의 속도가 빨라야"**
- 2) 데이터의 계산이, "수행되는 곳으로 이동"되는 게 아니라, **"저장된 곳에서 계산이 수행"**
- 3) 노드의 장애를 **"소프트웨어 레이어에서 극복"**해야

<br>

데이터의 저장

- 데이터는 HDFS 내부에 **"블록" 형태**로 저장됨

- 블록은, HDFS에 의해 "투명하게 복제"되어 여러 노드에 분산됨

  - 단순히 여러 노드에 저장하는건 X
  - 데이터가 "여러 랙(rack)" 에 분산 저장되도록 보장

  $$\rightarrow$$ 단일 노드/랙의 장애가 있다해서, 데이터가 유실되지 X

- 데이터의 블록이 저장된 위치 파악 & 계산이 실행될 최적의 장소를 전체 시스템이 결정

<br>

위치 투명성

- 파일의 "물리적인 섹터" 가 여러 곳에 나눠서 저장되어도,

  파일 이름만을 가지고 접근 가능한 특징

<br>

클라이언트 & 네임노드

- 클라이언트 : HDFS의 데이터를 읽거나/쓰거나 하려는 대상

  - 파일 목록이 필요한 클라이언트 : 네임노드에 직접 접속 & 메타데이터 요청
  - 데이터를 읽거나/쓰려는 클라이언트 : 네임노드에 필요한 블록의 위치를 요청

- 보조 네임노드 ( 체크포인트 노드 )

  - 네임노드 설정에 필요하진 않지만, 클러스터에 포함하는 편이 좋다

    ( 이름 오해 가능성! 장애 극복을 위해 대체해주는 역할? NO )

  - 네임노드의 장애에 대비해, **장애 발생 직전 마지막 상태를 보존하는 checkpoint 저장**

<br>

![figure2](/assets/img/hadoop/img1.png)

<br>

## 3.1.2 리소스 관리자 & 스케줄러

분산 시스템의 "핵심 요소"

- 1) 스케줄링
- 2) 리소스 관리 기능

<br>

Hadoop에는 **YARN (Yet Another Resource Negotiator)**이라 불리는 애플리케이션이 존재

( for 스케줄링 & 리소스 관리 )

- 역할 1) 가장 효율적인 방법으로 계산 리소스 할당
- 역할 2) 사용자 애플리케이션을 스케줄링

<br>

YARN 상세 설명

- 클러스터의 리소스를 **container**로 분할

  ( 컨테이너 : 기본으로 CPU, Memory 등이 할당됨 )

- **실행 중인 container를 모니터링** & 할당량 초과 않도록 억제

- **데이터 지역성**도 리소스로 제공

  ( 즉, YARN 애플리케이션은 특정 container가 특정 data를 저장하고 있는 서버에서 실행되도록 요청 가능 )

$$\rightarrow$$ "컨테이너"로 관리함으로써, 분산 시스템을 원활하게 운영 가능 & 클러스터의 리소스를 다수의 애플리케이션에 공평한 방식으로 공유

<br>

![figure2](/assets/img/hadoop/img2.png)

<br>

## 3.1.3 분산 데이터 처리 프레임워크

- 효율적인 데이터 I/O 기능은 중요, but 그것만으로는 부족!

- YARN은 컴퓨터 클러스터 **"전체에 계산을 분산"**

  & HDFS에 **"보관된 데이터를 확장 가능한 방식으로 처리하는 방법을 추상화"**해 제공할 뿐

<br>

**Map Reduce (맵 리듀스)**

- Hadoop이 가장 먼저 지원한 "데이터 처리 모델"
- 매우 단순한 모델에 기반

<br>

Map Reduce의 단계 : 

- Map Reduce의 "병렬 처리 모델"은, 문제를

  - **1) map 단계**
  - **2) shuffle 단계**
  - **3) reduce 단계**

  로 나눠서 수행함.

- **"HDFS의 데이터 지역성" & "YARN의 작업 및 리소스 관리 기능"**덕분에,

  3) reduce 단계를 효율적으로 수행 가능

- 특징 : 위 3 단계는 거의 **stateless**

  - mapper/reducer들이 어떠한 node에서 실행됬는지 알 수 X

<br>

**1) map 단계**

- by "매퍼 (mapper) 함수"
- input data가 클러스터에서 **"병렬"**로 처리됨
- input 데이터를 **"key : value" 쌍**으로 변환

<br>

**2) shuffle 단계**

- 1)에서 변환된 데이터는, **"key값"을 기준으로 정렬되어, "버킷"으로 셔플링**

  ( 동일 key = 동일 reducer에 할당 )

<br>

**3) reduce 단계**

- by "reducer"
- 모든 key의 값을 처리하며, 결과를 **HDFS(혹은 다른 영구 저장소)에 저장**

<br>

example) **word-count**

- step 1) 텍스트 데이터가 **HDFS에 저장됨**

- step 2) Hadoop은 데이터를 **"블록 단위로 쪼갠 뒤"**, **"HDFS 서버로 분산"**해서 중복저장

- step 3) Map-Reduce 단계

  - step 3-1) 여러 개로 분산된 mapper가, 각 블록을 "병렬"로 읽어들임 & "key-value"쌍으로 변환

  - step 3-2) 같은 key를 모두 모아, shuffling한 후, reducer에 전달

  - step 3-3) reducer는 "같은 단어(key)"를 가지는 모든 값을 "단순히 더하는 계산"을 수행

    & 각 단어 &단어 빈도를 파일에 기록

<br>

![figure2](/assets/img/hadoop/img3.png)

<br>

Map reduce의 활용

- 여러 분야에서 사용 가능
- but, **반복이 많은 작업**에는 적합하지 않음
  - 반복의 중간 결과를 항상 저장해야

<br>

Map Reduce vs Spark

- Map Reduce : 대규모 일괄처리에는 우수

  ( but 저지연 application & 반복 연산은 효과적 X )

- Spark : 데이터를 **분산 메모리로 캐싱(caching)**하여, 빠른 연산 & 저지연 응답 속도

  ( ex. 메모리에 올려둔 데이터셋을 반복적으로 사용하여 ML 학습 good )

<br>

# 3.3 Data Scientist용 Hadoop 도구

Data Scientists가 Hadoop과 함꼐 사용하는 도구/framework는?

<br>

## 3.3.1 Apache Sqoop (스쿱)

- 목적 : **하둡과 정형 DB 간의 효율적인 대용량 데이터 전송**을 지원

<br>

## 3.3.2 Apache Flume (플럼)

- 서버에서 생성되는 **대용량 로그 데이터를 효율적으로 수집 & HDFS로 전송**하는 분산 서비스

  ( 여러 소스에서 데이터 수집 가능 )

- 견고한 장애 허용

- 단순 & 유연한 아키텍처 : "agent"로 구성

  - agent : 소스(source)에서 저장소(sink)로 데이터를 보냄
  - 최소 2개의 agent 필요 ( source에 1개, sink에 1개 )

<br>

소스 agent를 구동하는 명령

```bash
flume-ng agent -c conf -f web-server-source-agent.conf -n source_agent
```

<br>

콜렉터 agent를 구동하는 명령

- 소스 data를 수신해서, HDFS에 저장

```bash
flume-ng agent -c conf -f web-server-target-agent.conf -n collector
```

<br>

## 3.3.3 Apache Hive (하이브)

- 목적 : **SQL로 Hadoop 데이터를 분석**하기 위해
- 대화형 쿼리 & 실시간 쿼리 모두 지원

<br>

ex) word-count 문제를 Hive에서 구현

```sql
CREATE TABLE docs (line_text STRING);
LOAD DATA INPATH 'user/demo/text_file.txt' OVERWRITE INTO TABLE docs;
CREATE TABLE word_count AS
	SELECT word, count(1) AS count FROM
		(SELECT explode(split(line_text, '\\s')) AS word FROM docs) word
	GROUP BY word
	ORDER BY word;
```

<br>

## 3.3.4 Apache Pig (피그)

- 목적 : Hadoop의 복잡한 **ETL 작업을 손쉽게 구현**하고자
- ETL 작업을 지원하기 위해 설계된 "도메인 특화 언어"
- 관계형 기본 함수 제공

<br>

Hive vs Pg

- Hive : 단발적인 ad-hoc 쿼리에 적합

- Pig : 중간 결과물이 필요한, 보다 복잡한 쿼리에 적합

  ( 대량의 JOIN, 중간 중간 TABLE이 필요한 경우에 ! )

<br>

ex) word-count 문제를 Pig에서 구현

- input file을 "SENTENCES" relation 형태로 **HDFS에서 불러옴**
- 내장된 TOKENIZE함수 & foreach 연산자로, **각 줄의 단어를 분할**
- group by 연산자 & 내장된 COUNT 함수로, **각 단어의 출현 횟수를 집계**
- 집계된 결과를 **HDFS의 wordcount 폴더에 저장**

```sql
SENTENCES = load 'user/demo/text_file.txt';
WORDS = foreach SENTENCES generate flatten(TOKENIZE((chararray)$0)) as word;
WORD_GRP = group WORDS by word;
WORD_CNT = foreach WORD_GRP generate group as word, COUNT(WORDS) as count;
store WORD_CNT into '/user/deom/wordcount';
```

<br>

## 3.3.5 Apahce Spark (스파크)

- 분산 인-메모리 데이처 처리 프레임워크
- 스칼라 & 파이썬을 지원하는 "대화형 데이터 처리 기능" 제공
- 사용되는 추상화 객체는 **RDD (Resilient Distributed Dataset)**

<br>

ex) word-count with Spark의 scala 명령 셸

```scala
val file = sc.textFile("/user/demo/text_file.txt")
val counts = file.flatMap(line =>line.split(" "))
				 .map(word => (word,1))
				 .reduceByKey(_ + _)
counts.saveAsTextFile("/user/demo/wordcount")
```

<br>

![figure2](/assets/img/hadoop/img4.png)

<br>

Spark SQL

- Hive를 대체할 수 있는 분산 SQL 처리 엔진
- 기능 : 기존 SQL 쿼리문 + 관계형 대수 연산인 DataFrame 제공
- 장점 : SQL의 결과를, disk에 쓰지 않고 바로 spark로 가져올 수 있음

<br>

Spark MLlib

- 다양한 ML 알고리즘을 구현해 spark 툴셋과 연동 가능
- 다양한 ML 알고리즘을 제공함

<br>

Spark GraphX

- Spark 기반의 Graph Library
- PageRank, label propagation, triangle count등의 알고리즘 지원

<br>

# 3.4 Hadoop이 Data Scientists에게 유용한 이유

1. 저비용 스토리지

   - 페타바이트 규모의 데이터를 처리

   - 중간 데이터를 저장하는 ETL 파이프라인을 구축해, 분석을 효과적으로 지원

     

2. 스키마 온 리드 ( Schema on Read )

   - 기존 : Schema on Write

     - 데이터의 스키마를 완벽하게 호가정ㅎ나ㅡㄴ데 focus

   - Hadoop : NoSQL과 유사한 Schema on Read를 지향

     - 이미 입수된 데이터가 "실행 시점에 해석"

       ( 하이브, 피그, 스파크 SQL의 경우에는 "쿼리 시점" )

     - 데이터 저장 & 데이터 해석이 별개로 분리 가능

   - 데이터 입수 전에 마쳐야 했단 사전 작업/협상의 노력을 줄여줌

     & 입수부터 분석까지의 시간을 단축

<br>

3. 비정형 & 반정형 데이터

   - 비/반정형 데이터가 전통적으로는, 저장되기 전에 "정형 데이터"로 변환해야 했었음

     ( 이 과정에서, 오류/실수 발생 )

   - Hadoop은 "분산 컴퓨팅 환경"에 저장하므로, 변환/정제 작업 중 생기는 오류 교정 가능

<br>

4. 다양한 언어 지원

5. 견고한 스케줄링 & 리소스 관리

6. 분산 시스템 추상화 레벨

7. 대규모 데이터에 기반한 모델 구축



# [ 4. Hadoop을 활용한 데이터 입수 ]

# 4.1 Hadoop Data Lake

Hadoop 이전

- data architecture (=스키마)는 주로 FIXED ( 변경 hard )
- 사전에 정의된 스키마로 데이터를 ETL ( 많은 시간/노력이 들었음 )

<br>

Data Lake

- data architecture (=스키마)가 보다 유연해짐

- 모든 데이터를 "원시 형식"으로 저장하는 중앙 스토리지

- "데이터를 처리하는 시점"이 되어서야, ETL과 유사한 과정 수행

  ( = Schema on Read )

- Data Scientists는, 프로젝트에 어떠한 데이터가 중요할지 사전에 알 기 어렵기 때문에,

  ***"일단 쌓아놓고 보자"*** 마인드가 좋다!

<br>

Data Lake > Data Warehouse

- 1) 모든 데이터를 사용할 수 있다  
  - 어떻게 활용될 지 사전 추측 필요 X
- 2) 모든 데이터를 공유할 수 있다
- 3) 모든 데이터 접근 방식이 가능하다

<br>

# 4.2 HDFS

모든 Hadoop 애플리케이션은 **HDFS에 저장된 데이터**를 바탕으로 실행됨

사용자는 (1) $$\leftrightarrow$$ (2) 파일 저장

- (1) HDFS
- (2) 로컬 파일 시스템

<br>

HDFS

- 대용량 파일 읽기/쓰기에 최적화된 스트리밍 파일 시스템

- 파일의 데이터를 "슬라이스로 분할"해서, Hadoop Cluster의 여러 서버에 "이중으로 저장"

  $$\rightarrow$$ 여러 슬라이스를 병렬로 처리 ( FAST ) & 이중으로 저장 ( 유실 가능성 적음 )

  $$\rightarrow$$ HDFS에서 호스트 파일 시스템으로 전송할 때는, 분할된 것을 다시 합쳐서 하나로 만든 후 전송!

<br>

# 4.3 파일을 HDFS로 직접 전송

기본 HDFS 명령 사용하기 : `hdfs`

<br>

`get` : ( HDFS -> 로컬 파일시스템 ) 복사

```bash
$ hdfs dfs -get test
```

- 해석 : HDFS에 저장된 test 파일을  로컬 파일 시스템으로 복사

<br>

`put` : ( 로컬 파일시스템 -> HDFS  ) 복사

```shell
$ hdfs dfs -put test
```

- 해석 : 로컬 파일 시스템에 저장된 test 파일을  HDFS로 복사

<br>

`ls` : 목록 확인

```bash
$ hdfs dfs -ls
```

- HDFS에 저장된 파일 목록 확인

<br>

# 4.4 파일을 Hive 테이블로 가져오기

Hive : 

- SQL과 유사한 스크립트 사용하여 HDFS의 데이터를 분석
- 다양한 형식의 파일을 Hive로 가져와야 하는 경우가 종종있음
- 가져온 뒤, SQL 쿼리, Pig, Spark 등으로 데이터를 처리

<br>
Hive가 처리하는 2종류의 테이블

- 1) 내부 테이블 : Hive가 완전히 관리
  - 삭제 시, Hive의 table 메타 정의 & table에 있는 데이터도 모두 삭제됨
- 2) 외부 테이블 : Hive가 직접 관리 X
  - 오직 Hive의 메타 정의에만 사용해 원시 형태로 저장된 텍스트 데이터에 접근
  - 삭제 시, Hive의 table 메타 정의만 삭제됨 ( 데이터는 그대로 )

<br>

## 4.4.1 CSV 파일을 Hive 테이블로 가져오기

step 1) HDFS안에 names 경로 생성

```bash
$ hdfs dfs -mkdir names
```



step 2) `names.csv` 파일을 HDFS에 복사해 넣기

```bash
$ hdfs dfs -put names.csv names
```



step 3-1) 외부 Hive 테이블 (external table)로 데이터를 불러오기

( 테이블 명 : `Names_text` )

```SQL
hive> CREATE EXTERNAL TABLE IF NOT EXISTS Names_text(
	> EmployeeID INT, FirstName STRING, Title STRING,
	> State STRING, Laptop STRING)
	> COMMENT 'Employee Names'
	> ROW FORMAT DELIMITED
	> FIELDS TERMINATED BY ','
	> STORED AS TEXTFILE
	> LOCATION '/user/username/names';
```

- `LOCATION '/user/username/names'` : 테이블이 사용할 입력 파일의 경로 지정
- 특징 : `INSERT` 문이 없음
  - Hive 테이블은 파일의 데이터를 '틀'에 불과
  - HDFS에 저장한 이후, 이 파일을 읽는 외부 테이블만 정의해도, SELECT문으로 데이터 조회 가능

<br>

step 3-2) 내부 Hive 테이블 (internal table)로 데이터를 불러오기

( 테이블 명 : `Names` )

- ORC 형식으로

```SQL
hive> CREATE TABLE IF NOT EXISTS Names(
	> EmployeeID INT, FirstName STRING, Title STRING,
	> State STRING, Laptop STRING)
	> COMMENT 'Employee Names'
	> ROW FORMAT DELIMITED
	> FIELDS TERMINATED BY ','
	> STORED AS ORC;
OK
```

<br>

Hive Table은 5가지 파일 형식을 지원

( 압축률이 가장 좋고, 속도가 빠른 ORC나 Parquet을 주로 사용 )

- 1) 텍스트 파일
- 2) 시퀀스 파일
- 3) RC 파일
- 4) ORC 파일
- 5) Parquet 형식

<br>

step 4) 외부 테이블 ( `Names_text` )에서 내부 테이블 ( `Names` )로 복사

```SQL
hive> INSERT OVERWRITE TABLE Names SELECT * FROM Names_text
```

<br>

step 5) 복사된 결과 확인

```SQL
hive> Select * from Names limit 5;
```

<br>

step 6) Partition 테이블 생성

- Partition 사용 시, 테이블을 논리적으로 나눌 수 있음
- **쿼리를 일부 데이터에만 효율적으로 실행 가능**

- ex) **주(state) 필드에 파티션이 적용된** 내부 테이블 만들기

```SQL
hive> CREATE TABLE IF NOT EXISTS Names_part(
	> EmployeeID INT,
	> FirstName STRING,
	> Title STRING,
	> Laptop STRING)
	> COMMENT 'Employee names, partitioned by state'
	> PARTITIONED BY (State STRING)
	> STORED AS ORC;
OK
```

<br>

step 7) 펜실베니아 state 출신 직원들의 데이터를, "외부 테이블"에서 "내부 테이블"로 복사

```SQL
hive> INSERT INTO TABLE Names_part PARTITION(state='PA')
	> SELECT EmployeeID, FirstName, Title, Laptop FROM Names_text WHERE state='PA';
OK
```

- 일일히 insert로...? 저장할 파티션 개수가 많으면, 불편할수도!

  $$\rightarrow$$ 동적 파티션 삽입 (Dynamic partition insert) 기능 사용!

<br>

# 4.5 Spark를 사용해 데이터를 Hive 테이블로 가져오기

Spark : 인-메모리에 focus

- 주요 데이터 추상화 객체는 **RDD (Resilient Distributed Dataset)**
- RDD의 각 데이터셋은 논리적인 파타션으로 나뉨
- RDD의 연산은 cluster의 여러 node에서 투명하게 처리됨

<br>

Saprk의 또 다른 데이처 추상화 객체 : **DataFrame**

- RDD를 기반으로 개반
- RDD와 달리, "관계형 DB" 처럼 **"이름이 지정된 칼럼"**으로 데이터가 구성됨

<br>

PySpark ( Spark의 Python API ) 실행

```bash
$ pyspark
```

<br>

version 1.6.2 / 2.0.0의 pyspark 실행

```bash
$ export SPARK_MAJOR_VERSION=1
$ pyspark
```

```bash
$ export SPARK_MAJOR_VERSION=2
$ pyspark
```

<br>

## 4.5.1 Spark를 사용해 CSV 파일을 Hive로 가져오기

csv 파일을 **spark의 DataFrame**으로 가져와서, **Hive 테이블로 저장**할 수 있음

- 예제 1) **RDD**를 사용하는 방법
- 예제 2) **RDD를 DataFrame으로** 변환하는 방법
- 예제 3) **DataFrame을 Hive로** 저장하는 방법

( `spark-csv` 패키지를 사용해서 csv파일을 DataFrame으로 불러오기 )

<br>

step 1) **Spark DataFrame**에 필요한 함수들을 import

```python
from pyspark.sql import HiveContext
from pyspark.sql.types import *
from pyspark.sql import Row
```

<br>

step 2) raw data를 **spark RDD**로 불러오기 : `sc.textFile()`

```python
csv_data = sc.textFile("file:///home/username/names.csv")
```

<br>

step 3) `type()` 명렁으로, 생성한 RDD 확인하기

```python
type(csv_data)
```

<br>

step 4) 새 RDD를 생성

- `map()` 함수를 사용해서, 위 csv 데이터를 분리

```python
csv_data = csv_data.map(lambda p:p.split(","))
```

- 헤더 제거 ( `.first()` )

```python
header = csv_data.first()
csv_data = csv_data.filter(lambda p:p!=header)
```

<br>

step 5) RDD에 담긴 데이터를, **spark SQL DataFrame**에 저장

- `map()` 함수로, RDD 데이터를 DataFrame의 행으로 나타내는 **Row 객체로 변환 (`Row()`**) 해야!
- `int()` 함수로, 직원 ID를 정수로 변환

```python
df_csv = csv_data.map(lambda p:Row(EmployeeID = int(p[0]),
                                  FirstName = p[1],
                                  Title = p[2],
                                  State = p[3],
                                  Laptop = p[4])).toDF()
```

<br>

step 6) 앞의 5행 확인하기 ( `.show(n)` )

```python
df_csv.show(5)
```

<br>

step 7) dataframe의 스키마 확인하기 ( `.printSchema()` )

```python
df_csv.printSchema()
```

<br>

step 8) DataFrame을 **Hive 테이블에 저장**

```python
from pyspark.sql import HiveContext

hc = HiveContext(sc)
df_csv.write.format("orc").saveAsTable("employees")
```

<br>

## 4.5.2 Spark를 사용해 JSON 파일을 Hive로 가져오기

- 위와 매우 유사하다

```python
from pyspark.sql import HiveContext
hc = HiveContext(sc)

df_json = hc.read.json("file:///home/username/names.json")
df_json.show(5)
df_json.printSchema()
df_json.write.format("orc").saveAsTable("employees")
```

<br>

