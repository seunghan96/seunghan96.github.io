## 41. Pytorch CharRNN ( 텍스트 생성 )

- 1) import packages

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
```

<br>

- 2) dataset

```python
input_str = 'apple'
label_str = 'pple!'

vocab = sorted(list(set(input_str+label_str)))
vocab_size = len(char_vocab)
```

<br>

- 3) hyperparameter

```python
input_size = vocab_size 
hidden_size = 5
output_size = 5

learning_rate = 0.1
```

<br>

- 4) Integer Encoding ( + inverse 버전)

```python
char_to_index = dict((char, idx) for idx, char in enumerate(vocab))

index_to_char={}
for key, value in char_to_index.items():
    index_to_char[value] = key
```

```python
x_data = [char_to_index[c] for c in input_str]
y_data = [char_to_index[c] for c in label_str]
print(x_data)
print(y_data)
#---------------------------------------
[1, 4, 4, 3, 2] # a, p, p, l, e에 해당
[4, 4, 3, 2, 0] # p, p, l, e, !에 해당
```

<br>

( Pytorch는 **3차원** tensor를 input으로 받기 때문에 ( 맨 앞에 batch size) , batch dimension을 추가해준다 & one-hot encoding해준다 )

```python
x_data = [x_data]
y_data = [y_data]

x_one_hot = [np.eye(vocab_size)[x] for x in x_data]
print(x_one_hot)
#---------------------------------------
[array([[0., 1., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       [0., 0., 0., 0., 1.],
       [0., 0., 0., 1., 0.],
       [0., 0., 1., 0., 0.]])]
```

<br>

```python
X = torch.FloatTensor(x_one_hot)
Y = torch.LongTensor(y_data)
X.shape,Y.shape
#---------------------------------------
torch.Size([1, 5, 5]), torch.Size([1, 5])
```

<br>

- 5) 모델 생성

```python
class Net(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Net, self).__init__()
        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) 

    def forward(self, x): 
        x, _status = self.rnn(x)
        x = self.fc(x)
        return x
```

```python
net = Net(input_size, hidden_size, output_size)
outputs = net(X)

print(outputs.shape)
print(outputs.view(-1, input_size).shape)
#-----------------------------
torch.Size([1, 5, 5]) # 3차원 ( batch 차원 O ) 
torch.Size([5, 5])    # 2차원 ( batch 차원 X )
```

위에서 size(5,5)의 의미?

- 앞의 5 : time step 
- 뒤의 5 : 출력의 dimension

<br>

- 2) loss function & optimizer

```python
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), learning_rate)
```

<br>

- 3) train
  - `view` : Batch 차원 제거 위해

```python
for i in range(100):
    outputs = net(X)
    loss = loss_fn(outputs.view(-1, input_size), Y.view(-1)) 
    #----------------------
    # zbs
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
	#----------------------
    print(i, "loss: ", loss.item())
```

<br>

## 42. Pytorch Word RNN ( 텍스트 생성 )

RNN의 입력 단위가 "문자" 가 아닌 "단어" 로

$$\rightarrow$$ pytorch의 `nn.Embedding()`을 사용하기!

<br>

- 1) dataset

```python
sentence = "Repeat is the best medicine for memory".split()
vocab = list(set(sentence))
print(vocab)
#----------------------------------------------
['best', 'memory', 'the', 'is', 'for', 'medicine', 'Repeat']
```

<br>

- 2) Integer Encoding

```python
word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  
word2index['<unk>']=0
index2word = {v: k for k, v in word2index.items()}
```

<br>

- 3) `build_data` 함수
  - word $$\rightarrow$$ integer 인코딩
  - X,y반환
  - `unsqueeze(0)`하는 이유 : batch 차원 추가 위해
  - 정수 encoding이기 때문에, `torch.LongTensor`를 사용

```python
def build_data(sentence, word2index):
    encoded = [word2index[token] for token in sentence]
    X, Y = encoded[:-1], encoded[1:] 
    X = torch.LongTensor(input_seq).unsqueeze(0) 
    Y = torch.LongTensor(label_seq).unsqueeze(0) 
    return X, Y
```

```python
X, Y = build_data(sentence, word2index)
```

<br>

- 4) hyperparameter

```python
vocab_size = len(word2index)  # vocabulary 개수 ( 고유 단어 개수 + 1)
input_size = 5  
hidden_size = 20 
```

<br>

- 5) 모델

```python
class Net(nn.Module):
    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):
        super(Net, self).__init__()
        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, 
                                            embedding_dim=input_size)
        self.rnn_layer = nn.RNN(input_size, hidden_size,
                                batch_first=batch_first)
        self.linear = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        y = self.embedding_layer(x)
        y, h = self.rnn_layer(output)
        y = self.linear(y)
        return y.view(-1, y.size(2))
```

<br>

```python
model = Net(vocab_size, input_size, hidden_size, batch_first=True)

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(params=model.parameters())
```

<br>

- 6)  정수 list $$\rightarrow$$ 단어 list로 변환해주는 함수

```python
decode = lambda y: [index2word.get(x) for x in y]
```

<br>

- 7) train

```python
for step in range(201):
    Y_pred = model(X)
    loss = loss_fn(Y_pred, Y.view(-1))
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 40 == 0:
        print("[{:02d}/201] {:.4f} ".format(step+1, loss))
        pred = output.softmax(-1).argmax(-1).tolist()
        print(" ".join(["Repeat"] + decode(pred)))
        print()
```

```
[01/201] 2.0184 
Repeat the the the the medicine best

[41/201] 1.3917 
Repeat is the best medicine for memory

[81/201] 0.7013 
Repeat is the best medicine for memory

[121/201] 0.2992 
Repeat is the best medicine for memory

[161/201] 0.1552 
Repeat is the best medicine for memory

[201/201] 0.0964 
Repeat is the best medicine for memory
```

<br>

## 43. Pytorch Sentiment Classification (다대일)

- 1) import packages

```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchtext import data, datasets
import random

SEED = 5
random.seed(SEED)
torch.manual_seed(SEED)
```

<br>

- 2) hyperparameters

```python
BATCH_SIZE = 64
lr = 0.001
EPOCHS = 10

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
```

<br>

- 3) Field 정의하기

  - `torchtext.data`의 **Field**클래스 사용

    ( 전처리를 위한 객체 사용 )

```python
TEXT = data.Field(sequential=True, batch_first=True, lower=True)
LABEL = data.Field(sequential=False, batch_first=True)
```

<br>

- 4) dataset 불러오기

  - `torchtext.datasets`에서 제공하는 IMDB 데이터셋 아용

  - default 비율 = (8:2)

```python
trainset, testset = datasets.IMDB.splits(TEXT, LABEL)
```

<br>
( 첫 번째 sample 확인해보기 )

```python
vars(trainset[0])
#------------------------------------------------------------
{'text': ['if', 'you', 'like', 'jamie', 'foxx,(alvin', 'sanders),"date', 'from', 'hell",\'01,', 'you', 'will', 'love', 'his', 'acting', 'as', 'a', 'guy', 'who', 'never', 'gets', 'an', 'even', 'break', 'in', 'life', 'and', 'winds', 'up', 'messing', 'around', 'with', 'shrimp,', '(jumbo', 'size)', 'and', 'at', 'the', 'same', 'time', 'lots', 'of', 'gold', 'bars.', 'alvin', 'sanders', 'has', 'plenty', 'of', 'fbi', 'eyes', 'watching', 'him', 'and', 'winds', 'up', 'getting', 'hit', 'by', 'a', 'brick', 'in', 'the', 'jaw,', 'and', 'david', 'morse,(edgar', 'clenteen),', '"hack"', "'02", 'tv', 'series,', 'decides', 'to', 'zero', 'in', 'on', 'poor', 'alvin', 'and', 'use', 'him', 'as', 'a', 'so', 'called', 'fish', 'hook', 'to', 'attract', 'the', 'criminals.', 'there', 'is', 'lots', 'of', 'laughs,', 'drama,', 'cold', 'blood', 'killings', 'and', 'excellent', 'film', 'locations', 'and', 'plenty', 'of', 'expensive', 'cars', 'being', 'sent', 'to', 'the', 'junk', 'yard.', 'jamie', 'foxx', 'and', 'david', 'morse', 'were', 'outstanding', 'actors', 'in', 'this', 'film', 'and', 'it', 'was', 'great', 'entertainment', 'through', 'out', 'the', 'entire', 'picture.'],
'label': 'pos'}
```

<br>

- 5) Vocabulary 만들기

```python
TEXT.build_vocab(trainset, min_freq=5) # 단어 집합 생성
LABEL.build_vocab(trainset)

vocab_size = len(TEXT.vocab)
n_classes = 2
```

<br>

- 6) Data Loader 생성
  - train : val : test = 0.8x0.8 : 0.8x0.2 : 0.2

```python
trainset, valset = trainset.split(split_ratio=0.8)
```

( 단어를 index로 대체하여 사용하는 `BucketIterator` )

```python
train_iter, val_iter, test_iter = data.BucketIterator.splits(
    (trainset, valset, testset), 
    batch_size=BATCH_SIZE,
    shuffle=True, repeat=False)
```

( 크기 확인 )

```python
print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))
print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))
print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))
#----------------------------------------------------------------
훈련 데이터의 미니 배치의 개수 : 313
테스트 데이터의 미니 배치의 개수 : 391
검증 데이터의 미니 배치의 개수 : 79
```

<br>

- 7) 첫 번째 minibatch 확인하기
  - size = ( batch size x 해당 배치 내에서 최대 길이 )
  - "해당 배치 내에서 최대 길이"이므로, batch별로 크기가 다를 수 있다

```python
# 1번째
batch = next(iter(train_iter))
print(batch.text.shape)
#----------------------------------------------------------------
torch.Size([64, 968])
```

```python
# 2번째
batch = next(iter(train_iter))
print(batch.text.shape)
#----------------------------------------------------------------
torch.Size([64, 873])
```

<br>

- 8) 모델
  - `_init_state` : 특정 state를 0으로 초기화

```python
class GRU(nn.Module):
    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):
        super(GRU, self).__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.embed = nn.Embedding(n_vocab, embed_dim)
        self.dropout = nn.Dropout(dropout_p)
        self.gru = nn.GRU(embed_dim, self.hidden_dim,
                          num_layers=self.n_layers,
                          batch_first=True)
        self.out = nn.Linear(self.hidden_dim, n_classes)

    def forward(self, x):
        x = self.embed(x)
        h_0 = self._init_state(batch_size=x.size(0))
        x, _ = self.gru(x, h_0)  # GRU output의 shape : (batch size, seq length, hidden dim)
        h_t = x[:,-1,:] # 마지막 time step것만 가져오기
        h_t=self.dropout(h_t)
        logit = self.out(h_t)  # (배치 batch, hidden dim) -> (배치 batch, output dim)
        return logit

    def _init_state(self, batch_size=1):
        weight = next(self.parameters()).data
        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()
```

```python
model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
```

<br>

- 9) train & evaluate 함수

```python
def train(model, optimizer, train_iter):
    model.train()
    for _, batch in enumerate(train_iter):
        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)
        y.data.sub_(1)  # label을 0/1로 치환
        #-------------------------------
        logit = model(x)
        loss = F.cross_entropy(logit, y)
        #-------------------------------
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

```python
def evaluate(model, val_iter):
    model.eval()
    corrects, total_loss = 0, 0
    for batch in val_iter:
        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)
        y.data.sub_(1) 
        #-------------------------------
        logit = model(x)
        loss = F.cross_entropy(logit, y, reduction='sum')
        #-------------------------------
        total_loss += loss.item()
        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()
    size = len(val_iter.dataset) # batch의 개수
    avg_loss = total_loss / size
    avg_accuracy = 100.0 * corrects / size
    return avg_loss, avg_accuracy
```

<br>

- 10) 학습하기 & best weight 저장하기

```python
best_val_loss = None

for e in range(1, EPOCHS+1):
    # (1) train
    train(model, optimizer, train_iter)
    
    # (2) evaluate
    val_loss, val_accuracy = evaluate(model, val_iter)
    print("[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f" % (e, val_loss, val_accuracy))

	# (3) save models
    if not best_val_loss or val_loss < best_val_loss:
        if not os.path.isdir("snapshot"):
            os.makedirs("snapshot")
        torch.save(model.state_dict(), './snapshot/txtclassification.pt')
        best_val_loss = val_loss
```

<br>

- 11) best weight 불러오기

```python
# model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)
model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))

test_loss, test_acc = evaluate(model, test_iter)
```

<br>

## 44. Pytorch Sequence Labeling (다대다)

- 입력 시퀀스 $$\mathrm{X}=\left[x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right]$$ 
- 레이블 시퀀스 $$\mathrm{y}=\left[y_{1}, y_{2}, y_{3}\right.$$ $$\left.\ldots, y_{n}\right]$$ 를 각각 부여
  

<img src="https://wikidocs.net/images/page/33805/forwardrnn_ver2.PNG" width="300" /><img src="https://wikidocs.net/images/page/33805/bidirectionalrnn_ver2.PNG" width="450" />.



### task : 품사 태깅(PoS tagging)

- 1) packages 불러오기

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchtext import data
from torchtext import datasets
import time
import random

SEED = 1234
random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

<br>

- 2) **Field** 정의하기

```python
TEXT = data.Field(lower = True)
UD_TAGS = data.Field(unk_token = None)
PTB_TAGS = data.Field(unk_token = None)
```

<br>

- 3) Dataset 만들기
  - `torchtext.datasets`에서 제공하는 UDPOS 데이터셋
  - train/valid/test로 나눈다

```python
fields = (("text", TEXT), ("udtags", UD_TAGS), ("ptbtags", PTB_TAGS))

train_data, valid_data, test_data = datasets.UDPOS.splits(fields)
```

```python
print(f"훈련 샘플의 개수 : {len(train_data)}")
print(f"검증 샘플의 개수 : {len(valid_data)}")
print(f"테스트 샘플의 개수 : {len(test_data)}")
#-----------------------------------------
훈련 샘플의 개수 : 12543
검증 샘플의 개수 : 2002
테스트 샘플의 개수 : 2077
```

<br>

- 4) label ( = 품사 ) 확인하기

```python
vars(train_data.examples[0])['udtags']
#----------------------------------------------------
['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']
```

<br>

- 5) Vocabulary 만들기
  - pre-trained Glove 사용

```python
TEXT.build_vocab(train_data, min_freq = 5, vectors = "glove.6B.100d")
UD_TAGS.build_vocab(train_data)
PTB_TAGS.build_vocab(train_data)
```

<br>

- 6) Data Loader 만들기

```python
BATCH_SIZE = 64

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, valid_data, test_data), 
    batch_size = BATCH_SIZE,
    device = device)
```

<br>

- 7) 첫 번째 batch 확인
  - size = (46,64)
    - 46 : 해당 batch내의 최대 문장 길이
    - 64 : `batch_first=False`이므로, 두 번째 차원이 batch size(=64)이다

```python
batch = next(iter(train_iterator))
batch.text.shape
#----------------------------------------
torch.Size([46, 64])
```

```python
batch.text
#----------------------------------------
tensor([[ 732,  167,    2,  ...,    2,   59,  668],
        [  16,  196,  133,  ..., 2991,   46,    1],
        [   1,   29,   48,  ..., 1582,   12,    1],
        ...,
        [   1,    1,    1,  ...,    1,    1,    1],
        [   1,    1,    1,  ...,    1,    1,    1],
        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')
```



<br>

- 8) Model

```python
class RNNPOSTagger(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout): 
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional)
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)        
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        # (크기) text = [sent len, batch size]
        
        # (크기) embedded = [sent len, batch size, emb dim]
        embedded = self.dropout(self.embedding(text))

        # (크기) output = [sent len, batch size, hid dim * n directions]
        outputs, (hidden, cell) = self.rnn(embedded)

        # (크기) predictions = [sent len, batch size, output dim]
        predictions = self.fc(self.dropout(outputs))

        return predictions
```

<br>

- 9) Hyperparameters

```python
INPUT_DIM = len(TEXT.vocab) # vocabulary의 개수
EMBEDDING_DIM = 100
HIDDEN_DIM = 128
OUTPUT_DIM = len(UD_TAGS.vocab)

N_LAYERS = 2
BIDIRECTIONAL = True

DROPOUT = 0.25
```

```python
model = RNNPOSTagger(INPUT_DIM, 
                     EMBEDDING_DIM, 
                     HIDDEN_DIM, 
                     OUTPUT_DIM, 
                     N_LAYERS, 
                     BIDIRECTIONAL, 
                     DROPOUT)
```

<br>

- 10) pre-trained embedding 사용하기

```python
pretrained_embeddings = TEXT.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)

# unknown(0) & padding(1) 토큰 추가하기
UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) # 0번 임베딩 벡터에는 0값을 채운다.
model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM) #
```

<br>

- 11) Loss Function & Optimizer
  - ***padding token은 loss 계산에서 제외!***

```python
loss_fn = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)
optimizer = optim.Adam(model.parameters())
```

```python
model = model.to(device)
loss_fn = loss_fn.to(device)
```

<br>

- 12) example prediction
  - size (46,64,18)
    - 46 : 1번째 batch의 sequence 길이 ( = 최대 문장 길이 )
    - 64 : batch size
    - 18 : num_classes

```python
prediction = model(batch.text)

print(prediction.shape)
#------------------------------------
torch.Size([46, 64, 18])
```

```python
prediction = prediction.view(-1, prediction.shape[-1])

print(prediction.shape)
print(batch.udtags.view(-1).shape)
#------------------------------------
torch.Size([2944, 18])
torch.Size([2944])
```

<br>

- 13) accuracy 계산 함수

```python
def categorical_accuracy(y_pred, y_real_idx, tag_pad_idx):
    y_pred_idx = y_pred.argmax(dim = 1, keepdim = True)
    mask = (y_real != tag_pad_idx).nonzero()
    y_pred_idx=y_pred_idx[mask]
    y_real_idx=y_real_idx[mask]
    correct = y_pred_idx.squeeze(1).eq(y_real_idx)
    return correct.sum() / torch.FloatTensor([y_real_idx.shape[0]])
```

<br>

- 14) train & evaluation 함수

[size]

- text = [sent len, batch size]     
- predictions = [sent len, batch size, output dim] $$\rightarrow$$ predictions = [sent len * batch size, output dim]

- tags = [sent len, batch size] $$\rightarrow$$ [sent len * batch_size]

```python
def train(model, iterator, optimizer, loss_fn, tag_pad_idx):
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    for batch in iterator:
        X = batch.text
        Y = batch.udtags
		#-----------------------------------------
        Y_pred = model(X)
        Y_pred = Y_pred.view(-1, Y_pred.shape[-1])
        Y = Y.view(-1) 
        loss = loss_fn(Y_pred, Y)
		#-----------------------------------------
        optimizer.zero_grad()
		loss.backward()
		optimizer.step()
        #-----------------------------------------
        acc = categorical_accuracy(Y_pred, Y, tag_pad_idx)
        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)
```

```python
def evaluate(model, iterator, loss_fn, tag_pad_idx):
    epoch_loss = 0
    epoch_acc = 0

    model.eval()
    with torch.no_grad():

        for batch in iterator:
            X = batch.text
	        Y = batch.udtags
			#-----------------------------------------
            Y_pred = model(X)
            Y_pred = Y_pred.view(-1, Y_pred.shape[-1])
            Y = Y.view(-1)
            loss = loss_fn(predictions, tags)
			#-----------------------------------------
            # NO UPDATE
            #-----------------------------------------
            acc = categorical_accuracy(Y_pred, Y, tag_pad_idx)
            epoch_loss += loss.item()
            epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)
```

<br>

- 15) 학습하기

```python
N_EPOCHS = 10

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, TAG_PAD_IDX)
    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn, TAG_PAD_IDX)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut1-model.pt')

    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')
```

