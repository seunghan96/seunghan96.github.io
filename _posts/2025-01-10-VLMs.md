---
title: Vision-Language Models (VLMs)
categories: [LLM, CV, MULT, NLp]
tags: []
excerpt: 
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models (VLMs)

참고: https://encord.com/blog/vision-language-models-guide/

<br>

### Contents

- Overview
- (1) VLM architectures
- (2) VLM evaluation strategies
- (3) VLM mainstream datasets
- (4) Key challenges, primary applications, and future trends

<br>

# Overview

**Vision-language model (VLM)**

- Input: **Images** & Respective **textual descriptions**
- Goal:  Learns to associate the knowledge from the **two modalities**
  - **Vision model**: Captures spatial features from the **images**
  - **Language model**: Encodes information from the **text**

$$\rightarrow$$ Learns to understand **images** and transforms the knowledge into **text** (and vice versa)

<br>

**Training VLMS**

- (1) **Pre-training** foundation models
  - Contrastive Learning
  - Masked language-image modeling
- (2)  **Zero-shot** learning & **Transfer** Learning (w/ fine-tuning)

<br>

# 1. VLM Architectures

Mainstream models: [CLIP](https://openai.com/research/clip), [Flamingo](https://arxiv.org/abs/2204.14198), and [VisualBert](https://arxiv.org/abs/1908.03557)

<br>

## (1) CLIP

**Contrastive learning** in [CLIP](https://arxiv.org/pdf/2103.00020.pdf)

- Similarity between **text** and **image** embeddings

<br>

**3 Step process** (to enable **zero-shot** predictions)

- Step 1) **Pretrain**
  - Train a text & image encoder
- Step 2) Converts [training dataset](https://encord.com/glossary/training-data-definition/) classes into **captions**
- Step 3) **Zero-shot prediction**
  - Estimates the **best caption** for the given input image

![figure2](/assets/img/llm/img54.png)

<br>

[ALIGN](https://arxiv.org/pdf/2102.05918.pdf) : Also uses **image and textual encoders** to minimize the distance between similar embeddings with **contrastive learning**

<br>

## (2) SimVLM & VirTex & Frozen

### **PrefixLM**

NLP learning technique for model pre-training

- Input: **Part of the text (= prefix)** 
- Goal: Predict the **next word** in the sequence

<br>

### PrefixLM in **VLMs**

Enables the model to predict the **next sequence of words** based on ...

$$\rightarrow$$ an **image** & its respective **prefix text**

<br>

Model:  [Vision Transformer](https://encord.com/blog/vision-transformers/) (ViT) 

- (1) ***Vision part***

  - Divides an image into a **1d-patch sequence**

  - Applies **convolution or linear projection** over the processed patches

    $$\rightarrow$$ Generate contextualized **visual embeddings**

- (2) ***Text part***
  - Converts the text prefix relative to the patch into a **token embedding**

- Transformer's encoder-decoder blocks receive both **"visual"** and **"token"** embeddings

<br>

### a) [SimVLM](https://arxiv.org/pdf/2108.10904.pdf) 

- Popular architecture utilizing the **PrefixLM** 
- Simple Transformer architecture
  - **Encoder**: to learn **image-prefix pairs**
  - **Decoder**: to generate an **output sequence**
- Good generalization and [zero-shot learning ](https://encord.com/blog/zero-shot-learning-explained/)capabilities

![figure2](/assets/img/llm/img55.png)

<br>

### b) [VirTex](https://arxiv.org/pdf/2006.06666v3.pdf) 

- (1) Image: **CNN **

- (2) Text: **Textual head** with transformers

- Train the model end-to-end to predict the **image captions** 

  ( by feeding **image-text pairs** to the textual head )

![figure2](/assets/img/llm/img56.png)

<br>

### c) [Frozen](https://arxiv.org/abs/2106.13884) 

- **PrefixLM** vs. **Frozen PrefixLM**
  - (1) PrefixLM : Train visual and textual encoders from scratch 
  
  - (2) ***Frozen*** PrefixLM : Use [pre-trained](https://encord.com/glossary/pre-trained-model-definition/) networks
    - Only update the parameters of the **image encoders**
  
- Encoders
  - Text encoder: Any LLMs
  - Visual encoder: Any pre-trained visual foundation model


![figure2](/assets/img/llm/img57.png)

![figure2](/assets/img/llm/img58.png)

<br>

## (3) Flamingo

Architecture

- Text model: **[Chinchilla](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training) **
  - Freeze the **LLM**
- Vision model: **CLIP-like vision encoder**
  - Process the image through a [Perceiver Sampler](https://medium.com/analytics-vidhya/perceiver-io-a-general-architecture-for-structured-inputs-outputs-4ad669315e7f)
    - Results in faster inference & ideal for [few-shot learning](https://encord.com/glossary/few-shot-learning-definition/)

![figure2](/assets/img/llm/img59.png)

<br>

## (4) Multimodal Fusing with Cross-Attention

Pre-trained **LLM** for **visual representation learning**

$$\rightarrow$$ By adding **cross-attention layers**

<Br>

###  [VisualGPT](https://arxiv.org/pdf/2102.10407.pdf) 

- Key: Adaptation of an **LLM’s pre-trained encoder** for **visual tasks**

- How: Employs a novel **self-resurrecting** encoder-decoder attention mechanism 

  - To **quickly adapt** the LLM with a **small amount of in-domain image-text data**

- **Self-resurrecting** activation unit: produces **sparse activations**

  $$\rightarrow$$ Prevent accidental overwriting of linguistic knowledge 

  ( avoids the issue of vanishing gradients )

<br>

Procedure

- Step 1) Extract relevant objects from an image input

- Step 2) Feed them to a visual encoder

  $$\rightarrow$$ Obtain visual representations

- Step 3) Feed the representations to a **decoder**

  - Decoder: Initialized with weights according to **pre-trained LLM**

  - **Self-resurrecting activation unit (SRAU)**

    $$\rightarrow$$ Balances the visual and textual information 

<br>

![figure2](/assets/img/llm/img64.png)

<br>

## (5) Masked-Language Modeling (MLM) & Image-Text Matching (ITM)

Adapt the MLM and ITM techniques for visual tasks!

<br>

###  [VisualBERT](https://arxiv.org/pdf/1908.03557)

(Trained on the [COCO](https://cocodataset.org/#home) dataset)

![figure2](/assets/img/llm/img65.png)

- A simple and flexible framework for modeling **vision-and-language tasks**
- Stack of Transformer layers: **Implicitly align** elements of ...
  - **(1) An input text** 
  - **(2) Regions in an associated input image** 
- Propose two visually-grounded language model objectives for pre-training: **MLM & ITM**
  - ITM: Whether or not a caption matches the image

<br>

## (6) No Training

***Directly*** use large-scale, pre-trained VLMs ***without any fine-tuning***

- e.g.) [MAGIC](https://arxiv.org/pdf/2205.02655.pdf) and [ASIF](https://arxiv.org/pdf/2210.01738.pdf) : **Training-free** frameworks
  - Predict **text descriptions** with input image

<br>

### MAGIC 

- **"Specialized score"** based on **CLIP-generated image embeddings** to **guide LLMs' output**

<br>

### ASIF 

- Key idea: **similar images** have **similar captions**
- Step 1) Computes the similarities between the ...
  - Single Image) [training dataset's](https://encord.com/blog/an-introduction-to-data-labelling-and-training-data/) query 
  - Multiple Images)  candidate images
- Step 2) Compares the ...
  - Single Image) Query [image embedding](https://encord.com/blog/image-embeddings-to-improve-model-performance/) 
  - Multiple Texts) Text embeddings of the corresponding candidate images
- Step 3) Predicts a description whose embeddings are the most similar to those of the query image

![figure2](/assets/img/llm/img66.png)

![figure2](/assets/img/llm/img68.png)

![figure2](/assets/img/llm/img67.png)

<br>

## (7) Knowledge Distillation

Train VLMs from larger, [pre-trained models.](https://encord.com/glossary/pre-trained-model-definition/)

<br>

### [ViLD](https://arxiv.org/pdf/2104.13921.pdf)

- Teacher: Pre-trained open-vocabulary image [classification](https://encord.com/glossary/classification-definition/) model
- Student: Two-stage detector

$$\rightarrow$$ Matches **textual embeddings** from a textual encoder with **image embeddings**

![figure2](/assets/img/llm/img69.png)

![figure2](/assets/img/llm/img70.png)

<br>

# 2. Evaluating VLMs

Assess the quality of the relationships between the **image** and **text**

Example) Image captioning model

- Comparing the **generated captions** to the **[ground-truth](https://encord.com/glossar**

<br>

Various automated **n-gram-based **evaluation strategies to compare the **predicted labels**

-  in terms of accuracy, semantics, and information precision.

<br>

Examples:

- **[BLEU](https://aclanthology.org/P02-1040.pdf)** (Bilingual Evaluation Understudy)

  - [Originally proposed](https://en.wikipedia.org/wiki/BLEU#:~:text=BLEU ,one natural language to another) to evaluate machine translation tasks

  - How? **"Precision"** of the **target text** vs. **reference (ground truth)** 

    by considering ***how many words*** in the  **candidate sentence ** appear in the reference. 

- **[ROUGE](https://aclanthology.org/W04-1013.pdf)** (Recall-Oriented Understudy for Gisting Evaluation)

  - How? **"Recall"** by considering  ***how many words*** in the **reference sentence ** appear in the candidate.

- **[METEOR](https://aclanthology.org/W05-0909.pdf)** (Metric for Evaluation of Translation with Explicit Ordering)

  - How? **"Harmonic mean"** of precision and recall
    - More weight to recall and multiplying it with a penalty term

- **[CIDEr](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf)** (Consensus-based Image Description Evaluation)

  - How? **"TF-IDF scores"**: compares a **target sentence** to a set of **human sentences** by computing the **average similarity between reference and target sentences**
  - 
