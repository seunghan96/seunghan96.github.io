---
title: Vision-Language Models for Vision Tasks; A Survey (Part 2)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey (Part 2)

https://arxiv.org/pdf/2304.00685

<br>

# 5. VLM Pretraining

- (1) Contrastive Objectives
- (2) Generative Objectives
- (3) Alignment Objectives

![figure2](/assets/img/llm/img384.png)

<br>

## (1) Contrastive Objectives

- a) Image Contrastive Learning
- b) Image-Text Contrastive Learning
- c) Image-Text-Label Contrastive Learning

<br>

### P1) Image Contrastive Learning

Pass

<br>

### P2) Image-Text Contrastive Learning

- To learn vision-language correlation

- By contrasting image-text pairs

- CLIP [10] 

  - Symmetrical image-text infoNCE loss

  - Pre-trained VLM hence learns image-text correlation 

    $$\rightarrow$$ Allows zero-shot predictions 

![figure2](/assets/img/llm/img385.png)

<br>

(Inspired by CLIP)

Many studies improve the symmetrical image-text infoNCE loss

(1) Large-scale datasets

- ALIGN (https://arxiv.org/pdf/2102.05918)
  - Scales up the VLM pre-training with **large-scale (i.e., 1.8 billions)** 
  - But **noisy image-text pairs** with noise-robust CL

<br>

(2) Small-scale datasets

- DeCLIP (https://arxiv.org/pdf/2110.05208)
  - **Nearest-neighbor supervision** to utilize the information from similar pairs
  - Effective pre-training on limited data
- OTTER (https://arxiv.org/pdf/2112.09445)
  - **Optimal transport** to pseudo-pair images and texts
  - Reduce the required training data greatly
- ZeroVL (https://arxiv.org/pdf/2112.09331)
  - Limited data resource via debiased data sampling and data augmentation with coin flipping mixup.

![figure2](/assets/img/llm/img386.png)

![figure2](/assets/img/llm/img387.png)

<br>

(3) Across various semantic levels 

- FILIP (https://arxiv.org/pdf/2111.07783)

  - Region-word alignment into contrastive learning

    $$\rightarrow$$  Fine-grained vision-language corresponding knowledge!

- PyramidCLIP (https://arxiv.org/pdf/2204.14095)
  - Multiple semantic levels
  - Performs both cross-level and peer-level contrastive learning

![figure2](/assets/img/llm/img388.png)

![figure2](/assets/img/llm/img389.png)

<br>

(4) Augmenting image-text pairs

- LA-CLIP & ALIP

  (https://arxiv.org/pdf/2305.20088)

  (https://arxiv.org/pdf/2308.08428)

  - Employ LLM to augment synthetic captions for given images

- RA-CLIP (https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf)

  - Retrieves relevant image-text pairs for image-text pair augmentation. 

![figure2](/assets/img/llm/img390.png)

![figure2](/assets/img/llm/img391.png)

![figure2](/assets/img/llm/img392.png)

<br>

(5) Unified vision & language encoder

- To facilitate efficient communications across data modalities
- CLIPPO (https://arxiv.org/pdf/2212.08045)
- OneR (https://arxiv.org/pdf/2211.11153)

![figure2](/assets/img/llm/img393.png)

![figure2](/assets/img/llm/img394.png)

![figure2](/assets/img/llm/img395.png)

<br>

### P3) Image-Text-Label Contrastive Learning

![figure2](/assets/img/llm/img396.png)

- Encodes image, text and classification labels into a **shared space**
- Employs both ...
  - (1) Supervised pre-training (with image labels)
  - (2) Unsupervised VLM pretraining (with image-text pairs)

<br>

UniCL (https://arxiv.org/pdf/2204.03610)

- *Unified Contrastive Learning in Image-Text-Label Space*

![figure2](/assets/img/llm/img397.png)

![figure2](/assets/img/llm/img398.png)

<br>

### P4) Discussion

Two limitations:

- (1) Joint optimizing positive and negative pairs is challenging
- (2) Involves a heuristic temperature hyper-parameter

<br>

## (2)  VLM Pre-training with Generative Objectives

- a) Masked image modelling (MIM)
- b) Masked language modelling (MLM)
- c) Masked cross-modal modelling (MCM)
- d) Image-to-text generation

<br>

### P1) Masked Image Modelling

MAE [41] and BeiT [70]

![figure2](/assets/img/llm/img399.png)

![figure2](/assets/img/llm/img401.png)

<br>

FLAVA (https://arxiv.org/pdf/2112.04482)

( *Flava: A foundational language and vision alignment model* )

-  Adopts rectangular block masking as in BeiT [70]

![figure2](/assets/img/llm/img400.png)

<br>

KELIP (https://arxiv.org/pdf/2203.14463) and SegCLIP (https://arxiv.org/pdf/2211.14813)

- Follow MAE to mask out a large portion of patches (i.e., 75 %)

![figure2](/assets/img/llm/img402.png)

![figure2](/assets/img/llm/img403.png)

<br>

### P2) Masked Language Modelling

FLAVA 

- Masks out 15% text tokens
- Reconstructs them from the rest tokens for modelling cross-word correlation. 

<br>

FIBER (https://arxiv.org/pdf/2206.07643)

![figure2](/assets/img/llm/img404.png)

<br>

### P3) Masked Cross-Modal Modelling

Masks and reconstructs both (1) image patches and (2) text tokens

FLAVA [42]

- Masks âˆ¼40% image patches
- Masks 15% text tokens

<br>

### P4) Image-to-Text Generation

Procedure

- Step 1) Encodes an input image into embeddings
- Step 2) Decodes them into texts 

<br>

COCA [19], NLIP [123] and PaLI [83] 

![figure2](/assets/img/llm/img405.png)

<br>

### P5) Discussion

Generally adopted as **additional objectives** above other VLM pre-training objectives for learning rich context information

<br>

## (3) Alignment Objectives

Enforce VLMs to align paired images and texts by learning to predict ***whether the given text describes the given image correctly***

- a) (Global) image-text matching
- b) (Local) region-word matching

<br>

### P1) (Global) Image-Text Matching

Goal: Directly aligning paired images and texts 

<br>

Examples

- FLAVA: Matches the given image with its paired text via a **classifier** (feat. BCE loss)
- FIBER: Mine hard negatives with pair-wise similarities

<br>

### P2) Region-Word Matching

Goal: Models local **fine-grained** vision-language correlation

How? By aligning paired image **"regions"** and word **"tokens"**

$$\rightarrow$$ Benefit zero-shot dense predictions 

- e.g., in object detection and semantic segmentation.

<br>

Examples

- GLIP [67], FIBER [71] and DetCLIP [45]:
- Replace "object classification" logits by "region-word alignment" scores
  - i.e., Similarity between **"regional"** visual features and **"token-wise"** features

![figure2](/assets/img/llm/img406.png)

<br>

### P3) Discussion

Alignment objectives

$$\rightarrow$$ Learn to predict weather the given **image** and **text** data are **"matched"**

<br>

Pros & Cons

- Pros
  - Simple and easy-to-optimize
  - Can be easily extended to model **"fine-grained"** vision-language correlation

- Cons

  - Often learn little correlation information within vision or language modality

    $$\rightarrow$$ $$\therefore$$ Often adopted as **auxiliary losses** to other VLM pre-training objectives

<br>

## (4) Summary & Discussion

Pretraining with

- (1) Contrastive Objectives
  - 1-1) Image CL
  - 1-2) Image-Text CL
  - 1-3) Image-Text-Label CL
- (2) Generative Objectives
  - 2-1) MIM
  - 2-2) MLM
  - 2-3) MCMM
  - 2-4) Image-to-Text Generation
- (3) Alignment Objectives
  - 3-1) Image-Text Matching
  - 3-2) Region-Word Matching
