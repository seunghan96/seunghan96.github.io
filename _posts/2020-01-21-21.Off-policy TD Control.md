---
title: 21.Off-policy TD Control ( ex.Q-Learning )
categories: [RL]
tags: [Reinforcement Learning, SARSA]
excerpt: Off-policy TD Control
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

( 참고 : Fastcampus 강의 )

# [ 20. Off-policy TD Control  ( ex.Q-Learning ) ]

### Contents

1. Review
2. Off policy의 장점
3. Off policy 학습
   1. 목적
   2. Example
   3. Importance Sampling
   4. 구체적 알고리즘

<br>

# 1. Review

## (1) Off-policy

On-policy 대신 Off-policy를 통해서도 $$Q^{\pi}(s,a)$$를 추산할 수 있다는 것을 배웠다.

![figure2](/assets/img/RL/img42.png)

<br>

## (2) TD(0)

$$V(s) \leftarrow V(s)+\alpha\left(G_{t}-V(s)\right)$$.

- where $$G_{t} \stackrel{\text { def }}{=} R_{t+1}+\gamma V\left(S_{t+1}\right)$$

<br>

용어 소개

- $$R_{t+1}+\gamma V\left(S_{t+1}\right)$$ : TD target
- $$\delta_{t}=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)$$ : TD error

<br>

## (3) Off-policy MC

$$Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left(G_{t}^{\pi / \mu}-Q\left(s_{t}, a_{t}\right)\right)$$.

- where $$G_{t}^{\pi / \mu}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{\mu\left(A_{k} \mid S_{k}\right)} G_{t}$$
  <br>

(한계점) $$G_{t}^{\pi / \mu}$$ 의 분산이 클 수 있다는 점

<br>

## (4) Off-policy MC를 사용하기 어려운 이유

![figure2](/assets/img/RL/img43.png)

<br>

# 2. Q-Learning

**Importance Sampling** 을 사용하지 않고도, Off-policy 학습 가능!
<br>

$$\pi(\boldsymbol{a} \mid \boldsymbol{s})=\left\{\begin{array}{ll}
1, & \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q(s, a) \\
0, & \text { otherwise }
\end{array}\right.$$.

![figure2](/assets/img/RL/img44.png)

<br>

### Importance Sampling을 하지 않아도 되는 이유?

Hint : Value Iteration에서의 **Bellman Optimal Backup (Bellman 최적 방정식)**

(1) Bellman 최적 방정식

$$\begin{aligned}
Q^{*}(s, a) &=R_{S}^{a}+\gamma \sum_{s \in \mathcal{S}} P_{S S^{\prime}}^{a} \max _{a^{\prime} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \\
&=R_{S}^{a}+\gamma \mathbb{E}_{s^{\prime} \sim P_{s s^{\prime}}^{a}}\left[\max _{a^{\prime} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right]
\end{aligned}$$.

- 하지만, 현실 세계는 unknown이므로, 위 식의 $$R_{S}^{a}$$와 $$ P_{S S^{\prime}}^{a}$$는 unknown!

  ( $$R_{S}^{a}$$와 $$ P_{S S^{\prime}}^{a}$$ 둘 다 추산해야한다 )

<br>
(2) 샘플기반 추산 + Incremental Update

- $$R_{S}^{a}$$와 $$ P_{S S^{\prime}}^{a}$$ 모두 unknown이기 때문에, 

  $$Q$$는 아래 식과 같이 sample을 통해 추산하고, incremental하게 update함으로써 계산할 수 있다.

- $$Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)$$.

- 위 식을 보면, 추산치를 구하기 위해서 $$\mu$$나 $$\pi$$가 필요하지 않음을 알 수 있다.

  ( = Importance Sampling을 할 필요가 없다 )

- 그리고, 위 식이 곧 Q-learning의 update식이다!

<br>

# 3. Summary

![figure2](/assets/img/RL/img45.png)