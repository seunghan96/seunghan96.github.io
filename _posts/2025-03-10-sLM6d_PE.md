---
title: (sLM-6d) Prompt Engineering 실습 4
categories: [LLM, MULT, NLP]
tags: []
excerpt: Gemma 7B (Few-shot PE, Self-Ask PE, Chaining)
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
# Prompt Engineering 실습 4

1. LLaMA-3-8B (Multi-turn PE, Few-shot PE)
2. Mistral-7B (CoT PE, Zero-shot PE)
3. Phi-3-3.8B (Multi-turn PE, Generated Knowledge PE)
4. **Gemma 7B (Few-shot PE, Self-Ask PE, Chaining)**

<br>

## Contents

1. 모델 설정
2. Few-shot PE
3. Self-Ask PE
4. Prompt chaining

<br>

**관련 패키지 설치 및 환경 설정**

```bash
!pip install bitsandbytes==0.43.1
!pip install accelerate==0.30.1
!pip install transformers==4.39.3
!pip install gradio==4.29.0
```

```python
from huggingface_hub import notebook_login

notebook_login()
```

<br>

# 1. 모델 설정

```python
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
```

<br>

### a) 모델 설정: Gemma-7B

```python
model_id = "google/gemma-1.1-7b-it"
```

<br>

### b) Tokenizer 불러오기

```python
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

<br>

### c) Model 불러오기

Quantization 설정

```python
config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

<br>

(Quantize해서) 불러오기

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="cuda:0",
    trust_remote_code=True,
    quantization_config=config
)
```

<br>

# 2. Few-shot PE

### a) Prompt (message)

- Few-shot PE에 맞는 prompt 구성하기

```python
messages = [
    {"role" : "user", "content": "You are a robot that tells whether a review comment is positive or negative. \nreview comment : 다신 안먹어요"},
    {"role" : "assistant", "content" : "negative"},
    {"role" : "user", "content" : "review comment : 맨날 먹어요 ㅎㅎ"},
    {"role" : "assistant", "content" : "positive"},
    {"role" : "user", "content" : "review comment : 가격이 2배되도 시켜먹겠습니다."}
]
```

<br>

### b) Tokenize

```python
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)
```

<br>

### c) Generate

```python
outputs = model.generate(
    input_ids,
    max_new_tokens=512,
    top_p=0.9
)
```

<br>

### d) Result

```python
response = outputs[0][input_ids.shape[-1]:]
print("response : ", tokenizer.decode(response, skip_special_tokens=True))
```

```
positive
```

<br>

# 3. Self-Ask PE

### a) Prompt (message)

- Self-Ask PE에 맞는 prompt 구성하기

```python
messages = [
    { "role": "user", "content": """
    You are an English Teacher who teaches Korean Students.
    You always have to explain in the format of a conversation between a student and teacher.
    sententce : 나는 아버지가 방에 들어가는 모습을 보고 많이 후회하고 힘들어했다.
    """ },
    {"role": "assistant", "content": """
    Teacher: What is the verb in the sentence?
    Student: The verb is '후회하고 힘들어했다' which translates to 'regretted and struggled'.
    Teacher: What is the object of the sentence?
    Student: The object is '아버지가 방에 들어가는 모습' which translates to 'the sight of my father entering the room'.
    Teacher: Now, can you try to put it all together in English?
    Student: Yes, the sentence in English would be, "I regretted and struggled a lot after seeing my father entering the room."
    """},
    {"role": "user", "content": "sentence : 어제 밤에 일이 너무 힘들어서 나는 새벽에 깨서 엉엉 울었다."}
]
```

<br>

### b) Tokenize

위와 동일

<br>

### c) Generate

위와 동일

<br>

### d) Result

위와 동일

```
Teacher: What is the main verb in the sentence?
  Student: The main verb is '울었다' which translates to 'cried'.
  Teacher: What was the reason for crying?
  Student: The reason is '일이 너무 힘들어서' which translates to 'because the work was too difficult'.
  Teacher: Can you rephrase the sentence in English?
  Student: "I cried in the early morning because the work was too difficult the night before."
```

<br>

# 4. Prompt chaining

### a) Prompt (message)

- Prompt chaining에 맞는 prompt 구성하기

```python
import json

few_shot_context = "LG전자가 임직원들에게 무료로 사내식당 조식을 제공키로 했다.업계에 따르면 LG전자는 내달 1일부터 3만5000여 명에 달하는 국내 전 사업장 임직원들.."
few_shot_question = "LG전자의 국내 전 사업자 임직원들을 몇 명인가요?"
few_shot_answer = "LG전자의 국내 전 사업자 임직원 수는 약 3만 5000명입니다."

context = "예산군은 2024년도 여름방학 대학생 아르바이트 희망자 40명을 6월 24일부터 26일까지 모집한다고 밝혔다."

messages = [
    {"role" : "user", "content" : f"""You are a robot that generates question and answers using the given context. \n You MUST generate in Korean with JSON. \ncontext : {few_shot_context}"""},
    {"role" : "assistant", "content" : f"{{\"question\" : \"{few_shot_question}\", \"answer\" : \"{few_shot_answer}\"}}"},
    {"role" : "user", "content" : f"context : {context}"}
]
```

<br>

### b) Tokenize

위와 동일

<br>

### c) Generate

위와 동일

<br>

### d) Result

위와 동일

```
{"question": "예산군이 2024년도 여름방학 대학생 아르바이트 희망자를 언제 모집할까요?", "answer": "예산군은 2024년도 여름방학 대학생 아르바이트 희망자를 6월 24일부터 26일까지 모집합니다."}
```

<br>

### e) 추가 전처리 (json)

```python
question = json.loads(tokenizer.decode(response, skip_special_tokens=True))['question']
answer= json.loads(tokenizer.decode(response, skip_special_tokens=True))['answer']

print(answer)
```

```
예산군은 2024년도 여름방학 대학생 아르바이트 희망자를 6월 24일부터 26일까지 모집합니다.
```

<br>

### Reference

- [패스트캠퍼스] 8개의 sLM모델로 끝내는 sLM 파인튜닝
