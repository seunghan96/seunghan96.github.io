---
title: ReFT; Representation Finetuning for Language Models
categories: [LLM, NLP]
tags: []
excerpt: NeurIPS 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# ReFT: Representation Finetuning for Language Models

```
Wu, Zhengxuan, et al. "Reft: Representation finetuning for language models." NeurIPS 2024
```

참고: 

- https://aipapersacademy.com/reft/
- https://arxiv.org/pdf/2404.03592

<br>

### Contents

1. Introduction
   1. Motivation
   2. Proposal
2. ReFT
   1. Previous works: LoRA
   2. Idea of ReFT
   3. LoReFT
3. Experiments

<br>

# 1. Introduction

ReFT

- Promising novel direction for fine-tuning LLMs
- Excel at \# params & performance

<br>

## (1) Motivation

Finetuning a Pre-trained Transformer is **expensive**

$$\rightarrow$$ ***Parameter-efficient finetuning (PEFT)***

<br>

**Parameter-efficient finetuning (PEFT)**

- Only update a small number of weights!
- e.g., LoRA
  - Add small adapter weights to the model layers
  - Only update the added weights 

![figure2](/assets/img/llm/img166.png)

<br>

## (2) Proposal

ReFT = Representation Fine-Tuning

- **LoReFT**: Requires 10-50 times less parameters than LoRA

![figure2](/assets/img/llm/img167.png)

<br>

# 2. ReFT

## (1) Previous works: LoRA

***LoRA weights are baked into the Transformer***

- Train small number of adapter weights
- Once trained, the weights are baked into the model

$$\rightarrow$$ Representations are impacted by the added LoRA weights

( & Not the original representations obtained from the pre-trained transformer )

<br>

## (2) Idea of ReFT

***Why not directly edit the representaiton?***

$$\rightarrow$$ via **Intervention**

![figure2](/assets/img/llm/img168.png)

![figure2](/assets/img/llm/img169.png)

<br>

## (3) LoReFT

$$\Phi_{\text {LoReFT }}(\mathbf{h})=\mathbf{h}+\mathbf{R}^{\top}(\mathbf{W h}+\mathbf{b}-\mathbf{R h})$$.

- Learnable weights: $$\mathbf{W}, \mathbf{b}, \mathbf{R}$$

<br>

Examples)

![figure2](/assets/img/llm/img170.png)

- Train interventions for prefix and suffix of the tokens
  - Exact size of prefix and suffix are hyperparameters
- Intervention parameters: 
  - Either **shared or not shared** between different tokens of the **same layer**
  - **Different** between the **different layers**

<br>

# 3. Experiments

## (1) Common sense Reasoning

![figure2](/assets/img/llm/img171.png)

<br>

## (2) Arithmetic Reasoning

![figure2](/assets/img/llm/img172.png)

<br>

## (3) Instruction Following

![figure2](/assets/img/llm/img173.png)
