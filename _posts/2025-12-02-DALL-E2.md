---
title: DALL-E 2
categories: [LLM, MULT, CV]
tags: []
excerpt: arxiv 2022
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# **DALL-E 2 (arxiv 2022)**

*Hierarchical Text-Conditional Image Generation with CLIP Latents (https://arxiv.org/abs/2204.06125)*

<br>

### 간단 요약

- **CLIP latent space 활용**: 
  - Text → CLIP image embedding을 생성

- **Prior 모델 도입**: 
  - Caption으로부터 CLIP image embedding을 생성하는 Prior (AR 또는 Diffusion) 구조

- **Diffusion Decoder**: 
  - CLIP image embedding을 조건으로 받는 GLIDE 기반 diffusion U-Net으로 image 생성

- **2-stage 아키텍처**: 
  - (1) Text → CLIP latent
  - (2) CLIP latent → Image 
  - 로 분리되어 안정성과 표현력이 증가

- **고품질 & 고다양성**: 
  - Diffusion 기반 복원력
  - CLIP semantic space
  - $$\rightarrow$$ 덕분에 photorealism과 diversity 모두 향상.

- **Image manipulation** 기능 
  - variation, interpolation, text-guided editing(text-diff) 등이 자연스럽게 가능

<br>

![figure2](/assets/img/cv/img421.png)

<br>

## **Contents**

1. Abstract
2. Introduction
3. Method
   1. Decoder: CLIP Image Embedding → Image
   2. Prior: Text Caption → CLIP Image Embedding
4. Image Manipulation (CLIP latent의 장점)
5. DALL-E vs. DALL-E v2

<br>

# 1. Abstract

**기존 text-to-image 모델**의 한계점들

(e.g., GAN, autoregressive transformer 등)

- (1) **Photorealism vs diversity trade-off**
  - 이미지를 더 사진처럼(realistic) 만들수록 결과가 서로 비슷해지고 다양성이 줄어드는 경향
  - 반대로 다양성을 늘리면 퀄리티·디테일이 낮아지는 경향

- (2) **Attribute binding**
  - 텍스트의 속성이 객체에 제대로 매칭되지 않는 현상

<br>

### DALL-E 2 (unCLIP) 핵심 아이디어

핵심: **"CLIP latent space"** 활용

- [Prior] Text → **"CLIP image embedding"** 

- [Decoder] **"CLIP image embedding"** → Image 

<br>

### 장점

- **(1) CLIP latent 활용**: semantics + style을 잘 보존
- **(2) Non-deterministic**: 여러 Image 생성 가능
- **(3) Variation, interpolation, text-guided editing 가능**
- **(4) Diffusion 기반 decoder**: photorealistic high-resolution image 생성

<br>

# 2. Introduction

한 줄 요약: ***“Text → "CLIP latent" → Image”***

- **2-stage** 구조라, 더 안정적이고 표현력 높음

<br>

**unCLIP = CLIP (encoder) + Diffusion (decoder) + Prior**

- Large-scale Image-Text data + CLIP → Robust representation

- Diffusion models → High-fidelity generation


- **CLIP latent를 직접 생성하는 prior** + **CLIP inversion decoder**

<br>

# 3. Method

2 Components

- (1) **Prior:** Caption (Text) → CLIP image embedding
- (2) **Decoder:** CLIP image embedding → Image

<br>

## **(1) Prior model: Text Caption → CLIP Image Embedding**

Prior = **"Decoder에 들어갈 input"**

- (X) **"Text" Embedding**
- (O) Text Embedding으로부터 만든 **"Image" Embedding**

<br>

Prior model = **"Decoder에 들어갈 input"**을 생성하는 model

- *CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding*

<br>

### a) Goal

**Text caption $$y$$ → CLIP image embedding $$z_i$$** 를 생성하는 모델

( + CLIP text embedding $$z_t$$도 추가 conditioning 가능 )

<br>

### b) CLIP image embedding 추출 방법

Train vs. Test시의 Image embedding

- [Training] **"Ground-truth"**
  - CLIP image encoder가 직접 $$z_i$$ 를 제공 (==정답)
  
  - Prior는 caption $$y$$로부터 이 $$z_i$$를 맞추도록 학습
  
- [Test] **"Prediction"**
  - 실제 Image $$x$$가 없음 → $$z_i$$도 없음
  - (Train때 학습한) Prior모델이 $$y$$에서 $$z_i$$ 같은 걸 “생성”

<br>

공통점: Decoder가 위의 $$z_i$$를 Image로 변환

<br>

### **c) 두 종류의 Prior**

**Autoregressive (AR) Prior** = Transformer

- **(1) Prior vector**: CLIP image embedding을 **PCA로 축소**
  - 각 dimension을 1024 discrete bucket으로 quantize
- **(2) Modeling**: Transformer로 autoregressive prediction
- **(3) Input**: Text caption, CLIP text embedding을 prefix로 넣음
- 장/단점
  - 장점: simple
  - 단점: 느리고 compute 비용 큼

<br>

**Diffusion Prior** = Diffusion model

- **(1) Prior vector**: CLIP image embedding 자체를 사용

- **(2) Modeling**: (Gaussian) diffusion model

- **(3) Input**: [encoded text] + [CLIP text embedding] + [timestep embedding] + [noised CLIP image embedding]

- **(4) Loss**: unnoised embedding을 예측하는 MSE objective 사용
  - $$L = \mathbb{E}\big[ \mid \mid f_\theta(z_i^{(t)}, t, y) - z_i \mid \mid^2 \big]$$.

<br>

### **c) Prior 역할 요약**

Decoder 단독으로는 “text → Image” 생성 방법?

- (Naive 방법) CLIP text embedding
- (unCLIP) CLIP text embedding + **"CLIP image embedding"**

<br>

즉, Prior는 ***“Text 의미가 반영된 CLIP image embedding”*** 을 생성하여 Decoder의 입력으로 제공

<br>

## **(2) Decoder: CLIP Image Embedding → Image**

### **a) Goal**

- CLIP image encoder의 latent $$z_i$$ 를 입력받아,
- 원본 Image $$x$$를 복원 or 새로운 variation을 생성

<br>

### b) Decoder

**“CLIP image embedding을 입력으로 받아 Image를 그리는 diffusion 모델”**

( == GLIDE 스타일 diffusion U-Net )

- 즉, **“Image latent → Image”** decoder
- 여기서 **CLIP image embedding**이 “조건(conditioning)” 역할
- == GLIDE 스타일 diffusion U-Net

<br>

### c) Diffusion U-Net에 conditioning을 넣는 방법

한 줄 요약:

- ***“이 Noise를 t 스텝에서, 이 Text 조건에 맞게 denoise해라”***

<br>

일반적인 text-conditioned diffusion(GLIDE, Stable Diffusion 등)은 보통...

1. **U-Net 입력**: Noise가 섞인 Image $$x_t$$
2. **timestep embedding**: 현재 diffusion step $$t$$ 정보
3. **Text embedding/토큰**: “무엇을 그릴지” 정보

<br>

### **d) Architecture**

GLIDE 기반 **"diffusion model"**

- CLIP image embedding을 **timestep embedding**에 projection하여 추가

- Text encoder output sequence에 4개의 추가 토큰으로 부착

- Text caption도 optional conditioning으로 사용 (하지만 효과는 제한적)

<br>

### **e) Classifier-free Guidance**

- CLIP embedding을 10% 확률로 drop

- Text caption은 50% 확률로 drop

  → 강화된 classifier-free guidance 가능

<br>

### **f) High-Resolution Generation (up-sampling)**

- 64×64 → 256×256 diffusion upsampler
- 256×256 → 1024×1024 upsampler
- Gaussian blur + BSR degradation으로 robustness 확보
- Attention은 제거하고 convolution-only 구조로 구성

<br>

### **g) Summary**

Decoder는 CLIP image latent를 invert하는 **diffusion-based image decoder**

특징

- Photorealistic reconstruction
- Creative variation
- Text-diff manipulation

<br>

# **4. Image Manipulation (CLIP latent의 장점)**

1. **Variations** = 같은 이미지의 여러 버전 만들기
2. **Interpolation**  = 두 이미지/스타일 섞기
3. **Text-guided Editing / Text-diff** = 텍스트로 변화 방향 지정하기

<br>

## **(1) Variations**

- 동일 Image에서 여러 variation 생성 가능
- CLIP latent가 보존하는 semantics는 유지
- Residual detail은 변화

<br>

## **(2) Interpolation**

- 두 Image의 CLIP latent를 slerp로 interpolation
- 자연스러운 style/content blending 가능

<br>

### **(3) Text Diffs**

- “Image latent + (text embedding difference)” 연산
- e.g. “cat” → “anime cat”
- CLIP joint latent 덕분에 언어 기반 조작 가능 (Figure 5)

<br>

# 4. DALL-E vs. DALL-E v2

| **항목**                    | **DALL·E (v1, 2021)**                                        | **DALL·E 2 (v2, unCLIP, 2022)**                              |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **기본 철학**               | Text + Image tokens <br />→ 한 시퀀스로 autoregressive 모델링 | Text → CLIP latent → Diffusion decoder (2-stage hierarchical) |
| **Image Representation**    | dVAE로 이미지 <br />→ 32×32 discrete tokens (8192 vocab)     | CLIP image embedding <br />(continuous vector, semantic joint space) |
| **Modeling 방식**           | Transformer가 text+image token 전체를 autoregressive하게 예측 | (1) Prior: Text → CLIP image embedding (2) Decoder: CLIP latent → Image |
| **Prior 역할**              | 없음 <br />(transformer가 직접 image token 생성)             | 핵심: 텍스트로부터 CLIP image latent 생성                    |
| **Decoder**                 | Transformer 자체가 decoder 역할 수행                         | Diffusion Decoder (GLIDE 개선 모델)                          |
| **텍스트 conditioning**     | Text tokens → transformer sequence에 포함                    | Text embedding + CLIP text embedding이 Prior와 Decoder에 optional conditioning |
| **이미지 해상도**           | 256×256                                                      | Base 64×64 → 256×256 → 1024×1024 (2-stage diffusion upsampler) |
| **생성 품질(photorealism)** | 중간 수준                                                    | 매우 우수 (diffusion 기반)                                   |
| **생성 다양성(diversity)**  | moderate                                                     | 매우 높음 <br />(CLIP latent가 semantic space 유지)          |
| **텍스트–이미지 alignment** | 좋지만 세밀한 조절 어려움                                    | Prior+Decoder 구조로 alignment 강화                          |
| **Image variation**         | 없음 <br />(v1에서는 latent가 discrete token sequence라 interpolation/variation 어려움) | 매우 우수 <br />(CLIP latent를 바꾸거나 sampling noise 변경) |
| **Text-guided editing**     | 사실상 불가능                                                | 가능<br /> (CLIP joint space에서 text diff 적용)             |
| **Interpolation**           | 이미지 간 자연스러운 interpolation 어려움                    | slerp으로 자연스러운 스타일/내용 blending                    |
| **Attribute binding**       | 없음                                                         | 부분적으로 해결되나 완벽하진 않음                            |
| **CLIP 사용 여부**          | 사용 안 함                                                   | 핵심 구성요소 (text/image joint embedding space 사용)        |
| **Sampling**                | transformer autoregressive (느림)                            | diffusion sampling(다단계) + guiding 가능                    |
| **모델 크기**               | 12B parameter transformer                                    | Prior(AR or diffusion) + GLIDE decoder + CLIP encoder(≈ 수억~수십억) |
| **Zero-shot 성능**          | 강함                                                         | 더 강함 (FID 개선 + 인간 평가 우수)                          |
| **작업 범위**               | text→image                                                   | text→image + image→variation + image→interpolation + text-diff editing |

<br>

Summary

- **DALL·E v1**: “Text–Image token sequence”를 Transformer로 직접 생성
- **DALL·E v2**: “Text→CLIP latent→Diffusion image”의 **2단계 구조**