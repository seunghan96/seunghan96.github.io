SelfCondë¥¼ ì œì•ˆí•  ë•Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë¶„ì„ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤: 

## ğŸ“Š 1. ê¸°ë³¸ ì„±ëŠ¥ ë¹„êµ (Core Performance Analysis)

### 1.1 Baseline ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ
- **vs. No Self-Conditioning**: SelfCond ìœ ë¬´ì— ë”°ë¥¸ ì§ì ‘ ë¹„êµ
- **vs. Other Methods**: ë‹¤ë¥¸ probabilistic forecasting ë°©ë²•ë“¤ê³¼ ë¹„êµ (TimeGrad, CSDI, SSSD ë“±)
- **Metrics**: CRPS, MAE, MSE, PICP, QICE, QL

### 1.2 ë°ì´í„°ì…‹ë³„ ë¶„ì„
- **Dataset Characteristicsì— ë”°ë¥¸ íš¨ê³¼**:
  - Small vs. Large datasets (ë°ì´í„° í¬ê¸°)
  - Low-dim vs. High-dim (ë³€ìˆ˜ ê°œìˆ˜)
  - Stationary vs. Non-stationary (ì‹œê³„ì—´ íŠ¹ì„±)
  - Smooth vs. Volatile (ë³€ë™ì„±)

### 1.3 Prediction Horizonë³„ ì„±ëŠ¥
- Short-term (24, 48 steps) vs. Long-term (96, 192, 336, 720 steps)
- Horizonì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ SelfCondì˜ íš¨ê³¼ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€

---

## ğŸ”¬ 2. Ablation Studies

### 2.1 Self-Conditioning Probability (p) ë¶„ì„
- **Grid search**: p âˆˆ {0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0}
- **Optimal p ì°¾ê¸°**: ë°ì´í„°ì…‹ë³„ë¡œ ìµœì  í™•ë¥ ì´ ë‹¤ë¥¸ì§€
- **Trade-off ë¶„ì„**: p=1.0 (í•­ìƒ ì‚¬ìš©) vs. p=0.5 (í™•ë¥ ì  ì‚¬ìš©)

### 2.2 Self-Conditioning Source ë¹„êµ
- **v1**: Random noise
- **v2**: Model prediction (same timestep)
- **v3**: Model prediction (previous timestep t+1â†’t)
- **ê° ë²„ì „ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ì‹œë‚˜ë¦¬ì˜¤**

### 2.3 Diffusion Steps ë¶„ì„
- **T âˆˆ {10, 20, 30, 40, 50, 100}**
- SelfCondê°€ ì ì€ stepì—ì„œ ë” íš¨ê³¼ì ì¸ì§€
- Sampling efficiency vs. Quality trade-off

### 2.4 Model Capacity (d_model)
- **d âˆˆ {128, 256, 512, 1024}**
- ì‘ì€ ëª¨ë¸ì—ì„œ SelfCondì˜ regularization íš¨ê³¼
- í° ëª¨ë¸ì—ì„œì˜ ì¶”ê°€ ì´ë“

---

## ğŸ“ˆ 3. í•™ìŠµ ê³¼ì • ë¶„ì„ (Training Dynamics)

### 3.1 Convergence ì†ë„
- **Learning curves**: SelfCond vs. Baseline
- **Epochs to convergence**: ë” ë¹¨ë¦¬ ìˆ˜ë ´í•˜ëŠ”ê°€?
- **Training stability**: Lossì˜ varianceê°€ ì¤„ì–´ë“œëŠ”ê°€?

### 3.2 Overfitting ë¶„ì„
- **Train vs. Val gap**: SelfCondê°€ regularization ì—­í• ì„ í•˜ëŠ”ê°€?
- **Early stopping point**: ì–¸ì œ ë©ˆì¶”ëŠ” ê²ƒì´ ìµœì ì¸ê°€?

### 3.3 Gradient ë¶„ì„
- **Gradient norm**: ì•ˆì •ì ì¸ê°€?
- **Gradient flow**: Vanishing/exploding ë¬¸ì œê°€ ì™„í™”ë˜ëŠ”ê°€?

---

## ğŸ¯ 4. ì˜ˆì¸¡ í’ˆì§ˆ ë¶„ì„ (Prediction Quality)

### 4.1 Calibration ë¶„ì„
- **Prediction Interval Coverage Probability (PICP)**
  - Target: 95% â†’ ì‹¤ì œë¡œ 95%ì— ê°€ê¹Œìš´ê°€?
- **Quantile Calibration**
  - ê° quantile (0.1, 0.3, 0.5, 0.7, 0.9)ì˜ ì •í™•ë„
- **QICE (Quantile Interval Coverage Error)**

### 4.2 Sharpness vs. Calibration
- **Interval width ë¶„ì„**: ì˜ˆì¸¡ êµ¬ê°„ì´ ì¢ìœ¼ë©´ì„œë„ ì •í™•í•œê°€?
- **Sharpness score**: CRPSê°€ ë‚®ìœ¼ë©´ì„œ PICPë„ ì¢‹ì€ê°€?

### 4.3 Distributional Metrics
- **Energy Score**: ì „ì²´ ë¶„í¬ì˜ ì •í™•ë„
- **Variogram Score**: Multivariate dependency í¬ì°©
- **Quantile Score**: íŠ¹ì • quantileì˜ ì •í™•ë„

---

## ğŸ” 5. ì‹œê°í™” ë¶„ì„ (Visual Analysis)

### 5.1 ì˜ˆì¸¡ ë¶„í¬ ì‹œê°í™”
- **Fan chart**: Quantile bands (10%, 30%, 50%, 70%, 90%)
- **Spaghetti plot**: ì—¬ëŸ¬ ìƒ˜í”Œë“¤ì˜ trajectory
- **Baseline vs. SelfCond ë¹„êµ**: ë” ë‚ ì¹´ë¡­ê³  ì •í™•í•œê°€?

### 5.2 Case Study
- **Best cases**: SelfCondê°€ í¬ê²Œ ê°œì„ í•œ ì˜ˆì‹œ
- **Worst cases**: ì˜¤íˆë ¤ ë‚˜ë¹ ì§„ ì˜ˆì‹œ (ì™œ?)
- **Failure analysis**: ì–´ë–¤ íŒ¨í„´ì—ì„œ ì‹¤íŒ¨í•˜ëŠ”ê°€?

### 5.3 Uncertainty ë¶„ì„
- **Epistemic vs. Aleatoric uncertainty**
- **Time-varying uncertainty**: ì˜ˆì¸¡ horizonì— ë”°ë¥¸ ë¶ˆí™•ì‹¤ì„± ì¦ê°€
- **Event-based uncertainty**: íŠ¹ì • ì´ë²¤íŠ¸ì—ì„œì˜ ë¶ˆí™•ì‹¤ì„±

---

```
File "/home/seunghan9613/NsDiff-main-v2/transfer_learning/train_source_only.py", line 88, in train_source_model
    print(f"Model weights saved in: {experiment.run_save_dir}")
AttributeError: 'NsDiffForecast' object has no attribute 'run_save_dir'
```



## âš¡ 6. íš¨ìœ¨ì„± ë¶„ì„ (Efficiency)

### 6.1 Training íš¨ìœ¨ì„±
- **Training time**: SelfCondê°€ ëŠë¦°ê°€? (ì•½ 8% overhead ì˜ˆìƒ)
- **Memory usage**: ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œê°€?
- **Computational cost**: FLOPs ë¹„êµ

### 6.2 Inference íš¨ìœ¨ì„±
- **Sampling time**: ì¶”ë¡  ì†ë„ ë¹„êµ
- **Number of samples needed**: ê°™ì€ í’ˆì§ˆì— í•„ìš”í•œ ìƒ˜í”Œ ìˆ˜
- **Reduced sampling steps**: SelfCondë¡œ step ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ”ê°€?

### 6.3 Scalability
- **Large datasets**: Traffic (862 dims), Electricity (321 dims)
- **Long sequences**: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¦ê°€ ì‹œ íš¨ê³¼

---

## ğŸ§ª 7. Robustness ë¶„ì„

### 7.1 Noise Robustness
- **Input noise**: ì…ë ¥ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í–ˆì„ ë•Œ
- **Missing data**: ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œ
- **Outliers**: ì´ìƒì¹˜ì— ê°•ê±´í•œê°€?

### 7.2 Distribution Shift
- **Train-test mismatch**: ë¶„í¬ê°€ ë‹¬ë¼ì¡Œì„ ë•Œ
- **Concept drift**: ì‹œê°„ì— ë”°ë¥¸ íŒ¨í„´ ë³€í™”
- **Transfer learning**: ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ì „ì´

### 7.3 Hyperparameter Sensitivity
- **p ë³€í™”ì— ë”°ë¥¸ robustness**
- **T ë³€í™”ì— ë”°ë¥¸ robustness**
- **Initialization sensitivity**

---

## ğŸ² 8. Stochastic Behavior ë¶„ì„

### 8.1 Sample Diversity
- **Inter-sample variance**: ìƒì„±ëœ ìƒ˜í”Œë“¤ì˜ ë‹¤ì–‘ì„±
- **Mode coverage**: ë©€í‹°ëª¨ë‹¬ ë¶„í¬ë¥¼ ì˜ í¬ì°©í•˜ëŠ”ê°€?
- **Collapse analysis**: Mode collapseê°€ ë°œìƒí•˜ëŠ”ê°€?

### 8.2 Seed Stability
- **Multiple seeds**: ì—¬ëŸ¬ seedë¡œ ì‹¤í—˜ (1, 2, 3, 42, 100)
- **Variance across seeds**: ê²°ê³¼ê°€ ì•ˆì •ì ì¸ê°€?
- **Confidence intervals**: í‰ê·  Â± í‘œì¤€í¸ì°¨

---

## ğŸ”„ 9. Iterative Refinement ë¶„ì„

### 9.1 Self-Conditioningì˜ Quality Evolution
- **t+1 â†’ t ê³¼ì •ì—ì„œ prediction quality ê°œì„  ì¶”ì **
- **ëª‡ stepì—ì„œ ê°€ì¥ í° ê°œì„ ì´ ìˆëŠ”ê°€?**
- **Early vs. Late timestepsì˜ ê¸°ì—¬ë„**

### 9.2 Self-Consistency
- **Å·â‚€^prevì™€ Å·â‚€ì˜ ì¼ê´€ì„± ì¸¡ì •**
- **Consistencyê°€ ì„±ëŠ¥ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆëŠ”ê°€?**

---

## ğŸ“ 10. ì´ë¡ ì  ë¶„ì„ (Theoretical Analysis)

### 10.1 Loss Landscape
- **Loss surface visualization**: SelfCondê°€ ë” smoothí•œê°€?
- **Local minima**: ë” ì¢‹ì€ minimaì— ë„ë‹¬í•˜ëŠ”ê°€?

### 10.2 Posterior Approximation
- **KL divergence**: True posteriorì™€ì˜ ì°¨ì´
- **Evidence Lower Bound (ELBO)**: ë” tightí•œ boundì¸ê°€?

### 10.3 Information Flow
- **Mutual information**: Å·â‚€^prevì™€ yâ‚€ì˜ ìƒí˜¸ì •ë³´
- **Information bottleneck**: ì–´ë””ì„œ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•˜ëŠ”ê°€?

---

## ğŸ¯ 11. Application-Specific ë¶„ì„

### 11.1 Decision Making
- **Risk-sensitive forecasting**: ê·¹ë‹¨ê°’ ì˜ˆì¸¡
- **Cost-sensitive metrics**: ë¹„ìš© í•¨ìˆ˜ ê¸°ë°˜ í‰ê°€
- **Action recommendation**: ì˜ˆì¸¡ â†’ ì˜ì‚¬ê²°ì •

### 11.2 Anomaly Detection
- **Likelihood-based**: ë‚®ì€ likelihood = ì´ìƒì¹˜
- **Reconstruction error**: SelfCondê°€ ì •ìƒ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµí•˜ëŠ”ê°€?

---

## ğŸ“Š 12. Comparative Studies

### 12.1 Other Conditional Methods
- **Guidance**: Classifier-free guidance vs. SelfCond
- **Conditioning mechanisms**: Cross-attention vs. Concat vs. SelfCond

### 12.2 Other Diffusion Improvements
- **DDIM vs. SelfCond-DDPM**
- **Classifier-free guidance + SelfCond**: ì‹œë„ˆì§€ íš¨ê³¼

---

## ğŸ¨ ì¶”ì²œ ë¶„ì„ ìš°ì„ ìˆœìœ„

### **Tier 1 (í•„ìˆ˜)**
1. âœ… Baseline vs. SelfCond ì„±ëŠ¥ ë¹„êµ (ëª¨ë“  datasets)
2. âœ… Ablation: p (self-conditioning probability)
3. âœ… v1 vs. v2 vs. v3 ë¹„êµ
4. âœ… Calibration ë¶„ì„ (PICP, QICE)
5. âœ… ì‹œê°í™” (fan charts, case studies)

### **Tier 2 (ì¤‘ìš”)**
6. Training efficiency (time, convergence)
7. Hyperparameter grid search (d_model, T, p)
8. Robustness (noise, missing data)
9. Prediction horizon ë¶„ì„
10. Sample diversity ë¶„ì„

### **Tier 3 (ë¶€ê°€)**
11. Theoretical analysis
12. Transfer learning
13. Anomaly detection
14. Loss landscape visualization

---

ì´ ì¤‘ì—ì„œ **ì–´ë–¤ ë¶„ì„ì„ êµ¬í˜„**í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? êµ¬ì²´ì ì¸ ì½”ë“œë¥¼ ì‘ì„±í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸš€