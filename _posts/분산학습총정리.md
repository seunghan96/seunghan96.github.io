https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview

Open source

- Latest [Llama](https://huggingface.co/meta-llama) or [DeepSeek](https://huggingface.co/deepseek-ai) models
  - [Technical](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) and [Experiment](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf) reports는 쉬움
  - Most challenging part = ***the training code***

$\rightarrow$ Techniques necessary to **coordinate GPUs** to train these **massive systems**!!

<br>

Knowledge necessary to **scale the training of LLMs** from **one GPU to thousands of GPUs**

<br>

Various techniques 

- Data parallelism, tensor parallelism, pipeline parallelism or context parallelism as well as ZeRO or kernel fusion

$\rightarrow$ To make sure tfhat GPUs are **highly utilized at all times**

$\rightarrow$ Significantly reduces **training time** and makes the best use of this **expensive hardware**

<br>

Recent trends) 

Fine-tuning large models on specialized data often produces the best results & generally involving the **same distributed training techniques**

$\rightarrow$ We will progressively go over all of these techniques  ( From the **simplest** to the most **refined** ones )

<br>

**Assumption**

- Basic knowledge about LLM 
- How deep learning model are trained

( But be generally new to distributed training )

<br>

The book is built on the following **three general foundations**:

- (1) **Quick intros on theory and concepts:** 
  - Understand how each method works at a high level
  - Pros & Cons
  - Learn about which parts of a language model eat away your memory and when during training it happens.
  -  You’ll learn how we can solve memory constraints by parallelizing the models and increase the throughput by scaling up GPUs. As a result you'll understand how the following widget to compute the memory breakdown of a transformer model works:

- (2) **Clear code implementations:** 

  - Link to implementation references where possible. 

  - Two code references:

    - the [picotron](https://github.com/huggingface/picotron) repository is built for education, thus it implements concepts usually in single, self-contained short files.

    - On the other hand, to look at production ready code, we’ll refer to the [nanotron](https://github.com/huggingface/nanotron) implementations which is a production training codebase used at Hugging Face.


- (3) **Real training efficiency benchmarks:** 

  - Finally, how to *actually* scale your LLM training depends on your infrastructure

    

# High level overview

Tackle one or several of the following three key challenges

1. **Memory Usage**: it's a hard limitation - if a training step doesn't fit in memory, training cannot proceed
2. **Compute Efficiency**: 
   1. We want our hardware to spend most time computing
   2. We need to reduce time spent on data transfers or waiting for other GPUs to perform work.

3. **Communication overhead**: 
   1. We want to minimize communication overhead as it keeps GPUs idle. 
   2. Make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.


$\rightarrow$ Trade-off (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism)

$\rightarrow$ Finding the right balance is key to scaling training.



[cheatsheet](https://nanotron-ultrascale-playbook.static.hf.space/dist/assets/images/ultra-cheatsheet.svg) to help you navigate the book and get the general take-away. 



# First Steps: Training on one GPU

( Before we start to scale to many GPUs...) Very basics of model training 

Training typically consists of three steps:

- Step 1) Forward pass = Passes inputs through the model to yield its outputs,

- Step 2) Backward pass = Compute the gradients
- Step 3) Optimization step using the gradients to update the parameters

<br>

## Batch size ($b_s$)

Batch size

- One of the important hyper-parameters for model training 
  - Affects both model convergence and throughput

- Small batch size 
  - Pros) Useful early in training to quickly move along the training landscape reaching an optimal learning point. 
  - Cons) Small batch sizes will keep gradients noisy & Model may not be able to converge to the most optimal final performances. 
- Large batch size 
  - Very accurate gradient estimations
  - Tend to make less use of each training token rendering convergence slower and potentially wasting compute

( You can find a nice early discussion of this topic in OpenAI’s paper on large batch training[1]  or Section 4.2 of MiniMax-01 [technical report](https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf). )



Example)

- DeepSeek-V3/R1 training 
  - “the batch size is gradually increased from 3072 input sequences to 15360 in the training of the first 469B tokens, and then keeps at 15360 input samples in the remaining training”.

<br>

Batch size also affects the time it takes to train on a given text dataset: 

- Small batch size 

  - Require more optimizer steps to train on the same amount of samples. 

  - Optimizer steps are costly (in compute time) 

    $\rightarrow$ Total time to train will thus increase compared to using a larger batch size. 

  - Note that the batch size can often be adjusted quite largely around the optimal batch size without major impact on the performance of the model

- LLM pretraining community

  - Batch sizes are commonly reported in terms of "tokens" rather than in "number of samples"

    ( $bst$ (O), $b_s$ (X) )

  $\rightarrow$ This makes training numbers generally independent of the exact input sequence length used during the training.



In the simplest case, training on a single machine, the $b_s$(in samples) and $bst$ can be computed from the model input sequence length (seq) as follows :

$bst= b_s \times \text{seq}$

<br>

Formulas for the batch size in terms of samples 

( but you can always get its token-unit counterpart by multiplying it with the sequence length. )

A sweet spot for recent LLM training 

= Typically on the order of 4-60 million tokens per batch. 

<br>

The **batch size** & **training corpus** have been steadily increasing over the years!

- Llama 1  = Batch size of ~4M tokens for 1.4 trillion tokens while 
- DeepSeek = Batch size of ~60M tokens for 14 trillion tokens.



First challenge: OOM

- Q) What if our GPU doesn’t have enough memory to hold a full batch of our target batch size?

<br>

### Memory usage in Transformers

When training a NN, one stores several items in memory:

- Model weights
- Model gradients
- Optimizer states
- Activations needed to compute the gradients

$\rightarrow$ Stored as tensors which come in different *shapes* and *precisions*. 

- (1) *Shapes* = Determined by hyper-parameters 
  - ex) batch size, sequence length, model hidden dimensions, attention heads ...
- (2) *Precision* 
  - Formats like FP32, BF16, or FP8: respectively require 4, 2, or 1 byte 
  - (Later) Discussion of the different precisions and their trade-offs in the [Mixed Precision Training](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html?section=high_level_overview#mixed_precision_training) 
- Keep in mind that the memory requirements for these various format will be different and that will impact the memory usage of the items we need to store.

<br>

***So how can I quickly determine memory usage from these variables?***

$\rightarrow$ Simple way = Do this empirically and just measure it.

<br>

#### Profiling the memory usage

**Pytorch profiler**

- Understand how memory is allocated throughout training
- Note that memory utilization is not a static thing!
  - Varies a lot during training and during a training step

Figure

- Step1 ) First the activations increase quickly as we do the forward pass
- Step 2) Then during the backward pass the gradients build up and as the backward pass propagates, the stored activations used to compute the gradients are progressively cleared. 
- Step 3) Finally, we perform the optimization step during which we need all the gradients and then update the optimizer states before we start the next forward pass.

$\rightarrow$ Why does the first step looks different??

- The activations increase quickly and then plateau for a while. 
- In this first step the torch cache allocator does a lot of preparation preparing memory allocations to speed up the subsequent steps so that they don’t require searching for free memory blocks afterwards (see [Zach’s blog](https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html)). 
- After the first step we also see the optimizer states appearing which generally offset the memory usage for further training steps.

<br>

#### Weights/grads/optimizer states memory

Let’s see how scaling up training is often a question of maximizing compute efficiency while keeping the memory requirements of these various items (activations, parameters, gradients, optimizer states) within the memory constraints of the GPUs.

First 3 items

- (1) Model’s weights
- (2) Gradients
- (3) Optimizer states

$\rightarrow$ We can actually **pretty easily estimate** the memory needed for them!

<br>

Example) \# of params of simple transformer LLM:

Xxxxxxx



### a) Higher precision: FP32

- Both parameters and gradients require 4 bytes while the optimizer
- , if we use Adam, requires the momentum and variance to be stored, which adds another two 4 bytes per parameter

. In summary:





### b) Lower precision (feat. Mixed precision)

- For stability reasons (see [the mixed-precision training section below](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html?section=high_level_overview#mixed_precision_training)) ...

  $\rightarrow$ Don't use full low precision training

  $\rightarrow$ But a mix of higher and lower precision called "mixed precision"!

- Mixed precision training 

  - (a) is to generally use BF16 for most of the computations 

    - requiring 2 bytes per parameter and gradient

  - (b) as well as an additional copy of the model weights and gradients in FP32, 

    $\rightarrow$ (a) + (b) = thus 12 bytes per parameter in total. 

  - (c) optimizer states

    - For the Adam optimizer, this requires the momentum and the variance usually stored in FP32 for numerical stability, each using 4 bytes.

  - Mixed precision itself doesn’t save overall memory!
    - As it just distributes the memory differently across the three components
    - Rather adds another 4 bytes over full precision training if we accumulate gradients in FP32!
  - But still advantageous!
    - As computing the forward/backward passes in half precision allows us to 
      - (1) use optimized lower precision operations on the GPU which are faster and 
      - (2) reduces the activation memory requirements during the forward pass which is a large part of the memory usage as we saw on the graph above and below.

Let’s get a sense of how much general memory we need for a model (full and mixed precision giving the same overall value):



As we can see, as soon as we reach **7B** (!), weights and optimizer requirements already starts to add up significantly and exceed the size of a typical GPU memory, e.g. 80GB for a H100 GPU.

But for now, let’s start with models which still fit in a single GPU, take a look at the last big contributor to our memory budget: the activation memory.



#### Activations memory

Bit more complex to compute than the weights, gradients and optimizer states!

$\because$ Depends on the inputs of the model!

- If you’re unsure why we even need to store activations for the backward pass, [this reference](https://www.determined.ai/blog/act-mem-2) is a good quick refresh

After a careful inspection of how backward pass is computed we can estimate the total memory required for the activations in mixed precision and we arrive at the following equation:





