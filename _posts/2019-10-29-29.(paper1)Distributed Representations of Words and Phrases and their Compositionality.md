---
title: 30.(paper) 1.Distributed Representations of Words and Phrases and their Compositionality
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: Paper Review by Seunghan Lee
---

# Distributed Representations of Words and Phrases and their Compositionality (2013)

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

목차

0. Abstract
1. Introduction
2. Skip-gram model
   1. Hierarchical Softmax
   2. Negative Sampling
   3. Subsampling of Frequent words
3. Empirical Results
4. Learning Phrases
   1. Phrase Skip-Gram Results
5. Additive Compositionality
6. Comparison to Published Word Representation

<br>

#  Abstract

Skip-gram model : 

- (1) "efficient" method for learning (2) "distributed vector representation"

- Not only just the co-occurence of the word

  But also captures its meaning!



This paper suggests extensions that improves both

- 1) "quality of the vectors" 
- 2) "training speed"

How? By..

- 1) SUBSAMPLING of frequent words
- 2) NEGATIVE SAMPLING ( alternative to hierarchical softmax )

<br>

Limitation of word representation?

- 1) indifference to word order
- 2) inability to represent idiomatic phrases
  - ex) airport "Air Canada" $\neq$ Air + Canada

<br>

# 1. Introduction

Distributed respresentation of words, using Skip-gram Model



Skip-gram model

- *Efficient estimation of word representations in vector space. (Mikolov et al, 2013)*
- unlike NN, no dense matrix multiplication $\rightarrow$ efficient
- can encode its meaning!
  - ex) King - Man + Woman = Queen

<br>

This paper introduces extension of original Skip-gram model.

- 1) Subsampling of frequent words
  - speed up (2x ~ 10x)
  - improves accuracy of less frequent words

- 2) variant of NCE (Noise Contrastive Estimation)

  - faster training
  - better vector representations

  ( compared to Hierarchical Softmax )

<br>

Limitation of word representations : **"Inability to represent idiomatic phrases"**

- ex) airport "Air Canada" $\neq$ Air + Canada
- using vectors to represent the "whole phrase" ( "Air Canada" as one phrase )

- How to extend from "word-based" to "phrase-based" model? SIMPLE!
  - 1) identify a large number of phrases
  - 2) treat the phrases as individual tokens

<br>

# 2. The Skip-gram Model

![figure2](/assets/img/nlp/nlp1.png)

CBOW vs Skip-gram

- CBOW : predicting the current word based on the context

- Skip-gram : predicting words within a certain range before and after the current word, given current word.



Training Skip-gram

- How? "Maximize the average log probability"

  $\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)$.

  - $c$ : size of the training context  
  - Larger $c$ ( = more training examples ) $\rightarrow$ higher accuracy, slower speed

- softmax function

  $p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime}{ }^{\top} v_{w_{I}}\right)}$.

  - $v_w$ : input
  - $v_w'$ : output
  - $W$ : number of words (vocab)

  $\rightarrow$ Impractical ! Too many words! ( $10^5 \sim 1-^7$ terms)

  $\nabla \log p\left(w_{O} \mid w_{I}\right)$ is proportional to $W$



Inefficient of using all the $W$ words?

- softmax function : $\hat{y}_{i}=P(i \mid c)=\frac{\exp \left(u_{i}^{T} v_{c}\right)}{\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{c}\right)}$.

  where $u_i$ and $v_j$​ are the column vectors of embedded matrix

  ( let $U=\left[u_{1}, u_{2}, \ldots, u_{k}, \ldots u_{W}\right]$ be a matrix composed of $u_{k}$ column vectors  )

- loss : Cross Entropy Loss : $J=-\sum_{i=1}^{W} y_{i} \log \left(\hat{y}_{i}\right)$.

  where $y$ : one-hot encoded vector & $\hat{y}$ : softmax prediction

  <br>

  $\begin{aligned}J&=-\sum_{i=1}^{W} y_{i} \log \left(\frac{\exp \left(u_{i}^{T} v_{c}\right)}{\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{c}\right)}\right)\\
  &=-\sum_{i=1}^{W} y_{i}\left[u_{i}^{T} v_{c}-\log \left(\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{c}\right)\right)\right]\\
  &=-y_{k}\left[u_{k}^{T} v_{c}-\log \left(\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{c}\right)\right)\right]\end{aligned}$.

  <br>

  Thus...

  $\begin{aligned}\frac{\partial J}{\partial v_{c}}&=-\left[u_{k}-\frac{\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{c}\right) u_{w}}{\sum_{x=1}^{W} \exp \left(u_{x}^{T} v_{c}\right)}\right]\\
  &=\sum_{w=1}^{W}\left(\frac{\exp \left(u_{w}^{T} v_{c}\right)}{\sum_{x=1}^{W} \exp \left(u_{x}^{T} v_{c}\right)} u_{w}\right)-u_{k}\\
  &=\sum_{w=1}^{W}\left(\hat{y}_{w} u_{w}\right)-u_{k}\end{aligned}$.

  

  ## 2-1. Hierarchical Softmax

  

  

  

  