# LLaVA: Visual Instruction Tuning

## (1) Dataset

- Input: Prompt + (Caption of image, bounding box of image)
- Model: GPT4
- Output: Visual Instruction Tuning datasets (Three types)



## (2) Architecture

- [Freeze] **Vision Encoder**: CLIP ViT-L/14 
- [Train] **Projection Layer (W)**: Trainable linear projection
  - CLIP feature â†’ LLM embedding space ë³€í™˜
- **Language Model**: Vicuna (7B/13B)
  - Vicuna = LLaMA ê¸°ë°˜ instruction-following LLM 

- (ì°¸ê³ ) ë” ë³µì¡í•œ ë°©ë²• (Flamingoì˜ cross-attention, BLIP-2ì˜ Q-former)ì€ future workë¡œ ë‚¨ê²¨ë‘  .

  

## (3) Training

- System message + (Human instruction + Assistant answer) ì‹œí€€ìŠ¤ë¡œ êµ¬ì„±
-  í† í°ì„ ì‚¬ìš©í•´ ê° í„´ ì¢…ë£Œ í‘œì‹œ
- ëª¨ë¸ì€ **assistant ë‹µë³€ë§Œ ì˜ˆì¸¡**í•˜ë„ë¡ í•™ìŠµ (auto-regressive) 

<br>

#### **Stage 1: Pre-training for Feature Alignment**

**ëª©ì : Alignment (Vision feature â†” LLM)**

- Train & Freeze
  - Train: Projection layer
  - Freeze: Vision encoder + LLM
- ëª©ì : Alignment (Vision feature â†” LLM)

- **ë°ì´í„°**: CC3M â†’ noun-phrase ê¸°ë°˜ filteringìœ¼ë¡œ **595K image-text pairs** ì„ ì •
  - â€œì´ë¯¸ì§€ë¥¼ ì§§ê²Œ ì„¤ëª…í•´ë¼â€ ê°™ì€ **ë‹¨ìˆœ** instructionë§Œ ì‚¬ìš©.
  - ì´ë•Œ ì¤‘ìš”í•œ ê±´ **concept coverage** (ë‹¤ì–‘í•œ ê°ì²´/ìƒí™©ì„ í¬ê´„)

- **ë°©ì‹**: naive expansion â†’ Q: â€œì´ë¯¸ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ë¼â€ / A: caption

<br>

#### **Stage 2: Fine-tuning End-to-End**

**ëª©ì : ì‹¤ì œ instruction-following ëŠ¥ë ¥ í•™ìŠµ**

- Train & Freeze
  - Train: Projection layer + LLM
  - Freeze: Vision encoder
- **ë°ì´í„°**: LLaVA-Instruct-158K 
  - ì•ì„œ GPT-4ë¡œ ìƒì„±í•œ Conversation / Detailed / Reasoning ë°ì´í„°
- **ë‘ ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤**:
  1. **Multimodal Chatbot**: conversation + detailed + reasoning ë°ì´í„° í˜¼í•© í•™ìŠµ
  2. **ScienceQA**: multimodal QA ë°ì´í„°ì…‹ (ë¬¸ì œ + context(í…ìŠ¤íŠ¸/ì´ë¯¸ì§€) â†’ reasoning + ë‹µ) ë‹¨ì¼ í„´ í˜•ì‹ìœ¼ë¡œ í•™ìŠµ 

<br>

ğŸ“Œ ìš”ì•½

- **Stage 1**: CLIP feature â†” LLM embedding alignment (Projection Layerë§Œ í•™ìŠµ)
- **Stage 2**: Instruction-following fine-tuning (Projection + LLM í•™ìŠµ, Vision EncoderëŠ” frozen)
- ìµœì¢…ì ìœ¼ë¡œ **ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ instruction â†’ Assistant ë‹µë³€**ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” end-to-end ëª¨ë¸ ì™„ì„±.





------



ğŸ‘‰ ì—¬ê¸°ê¹Œì§€ **Visual Instruction Tuning** ì„¹ì…˜ ì •ë¦¬ì˜€ìŠµë‹ˆë‹¤.

â€œë‹¤ìŒâ€ì´ë¼ê³  í•˜ì‹œë©´, ì´ì–´ì„œ **5. Experiments**(Multimodal Chatbot, LLaVA-Bench, ScienceQA ê²°ê³¼)ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.



ë„¤ë„¤, 

- DT = 90ì 
- RF (DTxMê°œ parallel) = 95ì  > 90ì 
- XGB (DTxMê°œ sequential) = 97ì  > 90ì 

- RF (XGBxMê°œ parallel) = ?? 
- Stacking (RF+XGB+kNN+...)