# 1. 공통

(1) 일반 $A$

(2) 적응 $A$

- 적응 $A_t$ = 일반 $A$ $\cdot$ $H_t$
  - where $H_t$ 는 LSTM의 매 timestamp output
- 이때, LSTM의 input은 "graph" representation이다.



$H$ 와 $A$의 mapping function

- 그런데... $n(n-1)/2$ 개의 엣지들을..?

- 어떻게 $d$차원의 그래프벡터로, $A$ 를 모델링할수잇을까?

- 그 $d$차원의 그래프 벡터는,

  - ***(1) 노드들의 "특징" 모음이라기보다는,***
  - ***(2) 노드들의 "연결관계" 모음이어야함.***

  이를 위한 특별한 방식의 풀링이 필요할듯. -> **이 새로운 풀링방식을 제안하는 알고리즘이 핵심이 될듯?**

  - ex) structure-aware pooling

  ( 단순히 노드임베딩 벡터를 합치기만 하면은 안되고, 이전 step의 adjacency matrix가 활용이 되어야함? )

- 즉...

  - (1) $N \times d$ feature matrix와
  - (2) $N \times N$ adjacency matrix

  이 (1) + (2)를 동시에 반영한 embedding vector $1\times d^{'}$ 을 만들어야함.

  기존의 (1)만을 사용한 방식은, order-invarant해서 N의 정보가 사라짐 ( 개별 노드의 인덱스 정보가 사라짐 )

  

  아래의 3가지 정보를 취합한 $1 \times d^{'}$ 를 만들어야함.

  ***하지만, 이를 어떻게 다시 $A$ 로 변화시킬수 있을지에 대한 아이디어 필요.***

  - (1-1) $N \times d$ -> $1 \times d$
  - (1-2) $N \times d$ -> $N \times b$, where $b << d$
  - (2) $N \times N$ -> 아래 참조

![image-20220516202116808](/Users/seunghan96/Library/Application Support/typora-user-images/image-20220516202116808.png)



# 2. Down/Upsampling $A$ generation

- $t-1$ 의 $A$
- $t$의 $H$
- $t$의 $A$
- $H$라는  $1\times d$ 그래프 벡터를 계속 up-sampling해서, $N \times N$ 짜리 adjacency matrix 크기 행렬을 2개 만들기
  - (1) masking ( $A_t \rightarrow M_t A_t$ )
  - (2) noise ( $M_t A_t \rightarrow M_t A_t +b_t = A_{t+1}$ )



# 3. $\theta_{i,j}$ 접근법

일반 adjacency matrix는 0,1의 binary값

- a) 기존의 connection은 "PRIOR"
- b) 현재 스텝의 새로운 $H_t$ 는 "DATA"
- a) + b)가 작용하여, 베르누이 분포로써!

<br>

$A$ = (1) + (2) + (3)

<br>

(1) 연결 parameter

- bernoulli.
- prior = 기존
- timestep 지나면서 data 쌓여서, prior무시 가능

<br>

(2) 강도 parameter

- 시그모이드 x 2

- 아이디어 : 그냥 노드 내적 유사도가 높으면 

<br>

(3) 부등호 parameter

- 

