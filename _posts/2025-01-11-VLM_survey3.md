---
title: Vision-Language Models for Vision Tasks; A Survey (Part 3)
categories: [MULT, LLM, NLP, CV]
tags: []
excerpt: arxiv 2024
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Vision-Language Models for Vision Tasks: A Survey (Part 3)

https://arxiv.org/pdf/2304.00685

<br>

# 6. VLM Transfer Learning

(Beyond zero-shot prediction) Transfer learning has been studied!

$\rightarrow$ Adapts VLMs to fit downstream tasks via...

- (1) Prompt tuning [31], [132]
- (2) Feature adapter [33], [34]

<br>

Sections

- (1) Motivation of TL for pre-trained VLMs
- (2) Common TL setup
- (3) Three TL approaches
  - 3-1) Prompt tuning methods
  - 3-2) Feature adapter methods
  - 3-3) Etc.

<br>

## (1) Motivation of Transfer Learning

Two types of gaps while applied to various downstream tasks:

- (1) Gaps in **image and text distributions**
  - e.g., Downstream dataset may have **"task-specific image styles and text formats"** 
- (2) Gaps in **training objectives**
  - e.g., VLMs are generally ..
    - pretrained with **"task-agnostic objectives"** 
      - e.g., learn general concepts 
    - while downstream tasks often involve **"task-specific objectives"** 
      - e.g., coarse or fine-grained classification, region or pixel-level recognition, etc...

<br>

## (2) Common Setup of Transfer Learning

Three TL setups (to mitigate the domain gaps!)

- (1) Supervised transfer
- (2) Few-shot supervised transfer
- (3) Unsupervised transfer

<br>

## (3) Common Transfer Learning Methods

- (1) Prompt tuning approaches
- (2) Feature adapter approaches
- (3) Etc..

<br>

![figure2](/assets/img/llm/img407.png)

<br>

### P1) via Prompt Tuning (PT)

(Previous) Prompt engineering: **Manually** designs text prompts for each task

By finding **optimal prompts**, **without fine-tuning the entire VLM**

- a) **Text** PT
- b) **Visual** PT
- c) **Text-visual** PT

![figure2](/assets/img/llm/img408.png)

<br>

### P1-1) Text PT

Explores more effective ***learnable*** text prompts

With several ***labelled*** downstream samples for each class

Example) 

- **CoOp [31]** 

  - Expands a category word [label] into a sentence ‘[V]1, [V]2, ..., [V]m [label]’
  - [V] = **Learnable word vectors**

- **CoCoOp [32]**

  - To mitigate the overfitting of CoOP 

  - **Conditional** context optimization

    $\rightarrow$ Generates a specific prompt **for each image**

- **SubPT [132]** 

  - Designs **subspace** prompt tuning

- **LASP [133]** 

  - Regularizes learnable prompts with **hand-engineered prompts**

- **VPT [135]** 

  - Models text prompts with **instance-specific distribution**

- **KgCoOp [145]** 

  - (= Knowledge-guided Context Optimization for prompt tuning)
  - Enhances the **generalization of unseen class** by mitigating the forgetting of textual knowledge

- **SoftCPT [141]** 
  - (= PT with Soft Context Sharing)
  - Fine-tunes VLMs on multiple few-shot tasks (via multitask learning)

- **PLOT [138]** 
  - Employs optimal transport to learn multiple prompts to describe the diverse characteristics of a category. 

- **DualCoOp [139]**
  - Transfer VLMs to multi-label classification tasks
  - Adopts both positive and negative prompts for multilabel classification
- **TaI-DP [140]**
  - Transfer VLMs to multi-label classification tasks
  - Introduces double-grained prompt tuning for capturing both coarse-grained and fine-grained embeddings. 
- **DenseCLIP [142]** 
  - Explores language guided fine-tuning that employs visual features to tune text prompts for dense prediction
- **ProTeCt [146]** 
  - Improves the consistency of model predictions for hierarchical classification task.
- **UPL [143]**
  - (Unsupervised PT) Learnable prompts with **self-training** on selected **pseudo-labeled** samples. 
- **TPT [144]** 
  - (Unsupervised PT) Explores **test-time PT** to learn **adaptive prompts** from a single downstream sample

<br>

CoCoOp

![figure2](/assets/img/llm/img409.png)

<br>

LASP

![figure2](/assets/img/llm/img410.png)

![figure2](/assets/img/llm/img411.png)

<br>

VPT

![figure2](/assets/img/llm/img412.png)

<br>

KgCoOp

![figure2](/assets/img/llm/img413.png)

<br>

SoftCPT

![figure2](/assets/img/llm/img414.png)

![figure2](/assets/img/llm/img415.png)

<br>

PLOT

![figure2](/assets/img/llm/img416.png)

![figure2](/assets/img/llm/img417.png)

<br>

DualCoOp 

![figure2](/assets/img/llm/img418.png)

![figure2](/assets/img/llm/img419.png)

<br>

DenseCLIP

![figure2](/assets/img/llm/img420.png)

<br>

UPL 

![figure2](/assets/img/llm/img421.png)

![figure2](/assets/img/llm/img422.png)

<br>

TPT 

![figure2](/assets/img/llm/img423.png)

<br>

### P1-2) Visual PT

Transfers VLMs by modulating the input of image encoder

Example)

- **VP [147]** 

  - Learnable image perturbations $v$ 

  - How? Modify the input image $x^I$ by $x^I+v$

    $\rightarrow$ Aim to adjust $v$ to minimize a recognition loss

- **RePrompt [148]** 

  - Integrates retrieval mechanisms into visual prompt tuning

$\rightarrow$ Enables pixel-level adaptation to downstream tasks ( Benefit dense prediction tasks )

<br>

VP

![figure2](/assets/img/llm/img424.png)

<br>

### P1-3) Text-Visual PT

Benefiting from **joint** prompt optimization on **multiple modalities**

Example)

- **UPT [149]** 

  - **Unified** vision and language PT

- **MVLPT [150]** 

  - **Multitask** Vision-Language Prompt Tuning
  - Incorporate cross-task knowledge into text and image PT

- **MaPLe [151]** 

  - **Multi-modal** prompt learning

  - By **aligning** visual prompts with their corresponding language prompts 

    $\rightarrow$ Enabling a **mutual promotion** between text prompts & image prompts. 

- **CAVPT [152]** 

  - **Cross attention** between class-aware visual prompts & text prompts, 

<br>

UPT

![figure2](/assets/img/llm/img425.png)

![figure2](/assets/img/llm/img426.png)

<br>

MVLPT

![figure2](/assets/img/llm/img427.png)

<br>

MaPLe

![figure2](/assets/img/llm/img428.png)

![figure2](/assets/img/llm/img429.png)

<br>

### P1-4) Discussion

PT = Parameter-efficient VLM transfer

- with a few learnable text/image prompts

- requires little extra network layers or complex network modifications. 

Nonetheless, low flexibility!

<br>

### P2) via Feature Adaptation

Fine-tunes VLMs with an additional light-weight feature adapter

Example)

- Clip-Adapter [33] 

  - Several trainable linear layers after CLIP’s language and image encoders
  - Keep others frozen

- TipAdapter [34] 

  - Training-free adapter

    $\rightarrow$ Directly employs the embeddings of few-shot labelled images as the adapter weights! 

- SVL-Adapter [153] 

  - designs a selfsupervised adapter which employs an additional encoder for self-supervised learning on input images. In summary, feature adapter adapts image and text features to fit VLMs to downstream data, which provides a promising alternative to prompt tuning for VLMs transfer

<br>

Clip-Adapter

![figure2](/assets/img/llm/img430.png)

<br>

TipAdapter

![figure2](/assets/img/llm/img431.png)

<br>

SVL-Adapter

![figure2](/assets/img/llm/img432.png)

<br>

**Discussion** 

Feature adaptation adapts VLMs by modifying image and text features with an additional light-weight feature adapter. It is flexible and effective as its architecture and  the insertion manner allow tailoring flexibly for different downstream tasks. Therefore, feature adaptation has clear advantages in adapting VLMs to work on very different and complex downstream tasks [168], [169], [170], [171]. On the other hand, it requires modifying network architecture and thus can not handle VLMs that have concerns in intellectual property.

<br>

### P3) Other Methods

Several studies transfer VLMs by direct fine-tuning [162], architecture modification [163], and cross attention [157], [158]. Specifically, Wise-FT [162] combines the weights of a fine-tuned VLM and the original VLM for learning new information from downstream tasks. MaskCLIP [163] extracts dense image features by modifying the architecture of the CLIP image encoder. VT-CLIP [157] introduces visualguided attention to semantically correlate text features with downstream images, leading to a better transfer performance. CALIP [158] introduces parameter-free attention for effective interaction and communication between visual and text features, leading to text-aware image features and visual-guided text features. TaskRes [159] directly tunes text-based classifier to exploit the old knowledge in the pre-trained VLM. CuPL [160] and VCD [161] employ large language models, e.g., GPT3 [172], to augment text prompts for learning rich discriminative text information.

<br>

## (4) Summary & Discussion

In summary, prompt tuning and feature adapter are two major approaches for VLM transfer which work by modifying the input text/image and adapting image/text features, respectively. In addition, both approaches introduce very limited parameters while freezing the original VLMs, leading to efficient transfer. Further, while most studies follow few-shot supervised transfer [31], [32], [132], [134], recent studies show that unsupervised VLM transfer can achieve competitive performance on various tasks [143], [144], [160], inspiring more research on unsupervised VLM transfer.

<br>

# 7. VLM Knowledge Distillation

As VLMs capture generalizable knowledge that covers a wide range of visual and text concepts, several studies explore how to distil the general and robust VLM knowledge while tackling complex dense prediction tasks such as object detection and semantic segmentation. This section presents the motivation of distilling knowledge from VLMs as well as two groups of knowledge distillation studies on the tasks of semantic segmentation and object detection.

<br>

## (1) Motivation of Distilling Knowledge from VLMs

Different from VLM transfer that generally keeps the original VLM architecture intact in transfer [31], [132], [136], VLM knowledge distillation distils general and robust VLM knowledge to task-specific models without the restriction of VLM architecture, benefiting task-specific designs while tackling various dense prediction tasks [36], [173], [174]. For example, knowledge distillation allows transferring the general VLM knowledge to tackle detection tasks while taking the advantages of state-of-the-art detection architectures such as Faster R-CNN [55] and DETR [62].

<br>

## (2) Common Knowledge Distillation Methods

As VLMs are generally pre-trained with architectures and objectives designed for image-level representation, most VLM knowledge distillation methods focus on transferring image-level knowledge to region- or pixel-level tasks such as object detection and semantic segmentation. Table 5 shows a list of VLM knowledge distillation methods.

### P1) for Object Detection

Open-vocabulary object detection [193] aims to detect objects described by arbitrary texts, i.e., objects of any categories beyond the base classes. As VLMs like CLIP are trained with billion-scale image-text pairs that cover very broad vocabulary, many studies explore to distill VLM knowledge to enlarge the detector vocabulary. For example, ViLD [36] distills VLM knowledge to a two-stage detector whose embedding space is enforced to be consistent with that of CLIP image encoder. Following ViLD, HierKD [186] explores hierarchical global-local knowledge distillation, and RKD [187] explores region-based knowledge distillation for better aligning region-level and image-level embeddings. ZSD-YOLO [198] introduces self-labelling data augmentation for exploiting CLIP for better object detection. OADP [201] preserves proposal features while transferring contextual knowledge. BARON [200] uses neighborhood sampling to distill a bag of regions instead of individual regions. RO-ViT [199] distills regional information from VLMs for open-vocabulary detection.

<br>

Another line of research explores VLM distillation via prompt learning [165]. For example, DetPro [37] introduces a detection prompt technique for learning continuous prompt representations for open-vocabulary object detection. PromptDet [188] introduces regional prompt learning for aligning word embeddings with regional image embeddings. Additionally, several studies [180], [181], [189], [194], [197] explore VLM-predicted pseudo labels to improve object detectors. For example, PB-OVD [189] trains object detectors with VLM-predicted pseudo bounding boxes while XPM [194] introduces a robust cross-modal pseudo-labeling strategy that employs VLM-generated pseudo masks for open-vocabulary instance segmentation. P3OVD [197] exploits prompt-driven self-training that refines the VLMgenerated pseudo labels with fine-grained prompt tuning.

<br>

### P2) for Semantic Segmentation

**KD for open-vocabulary semantic segmentation**

 leverages VLMs to enlarge the vocabulary of segmentation models, aim to segment pixels described by arbitrary texts (i.e., any categories of pixels beyond base classes). For example, [35], [180], [181] achieve openvocabulary semantic segmentation by first class-agnostic segmentation by grouping pixels into multiple segments and then segment recognition with CLIP. CLIPSeg [175] introduces a lightweight transformer decoder to extend CLIP for semantic segmentation. LSeg [176] maximizes the correlation between CLIP text embeddings and pixel-wise image embedding encoded by segmentation models. ZegCLIP [174] employs CLIP to generate semantic masks and introduces a relationship descriptor to mitigate overfitting on base classes. MaskCLIP+ [163] and SSIW [177] distillknowledge with VLM-predicted pixel-level pseudo labels. FreeSeg [185] generates mask proposals firstly and then performs zero-shot classification for them. 



**KD for weakly-supervised semantic segmentation**

 aims to leverage both VLMs and weak supervision (e.g., image-level labels) for semantic segmentation. For example, CLIP-ES [184] employs CLIP to refine the class activation map by deigning a softmax function and a class-aware attention-based affinity module for mitigating the category confusion issue. CLIMS [183] employs CLIP knowledge to generate high-quality class activation maps for better weakly-supervised semantic segmentation.

<br>

## (3) Summary & Discussion

In summary, most VLM studies explore knowledge distillation over two dense visual recognition tasks, namely, object detection and semantic segmenting, where those for the former aim to better align image-level and objectlevel representations while those for the latter focus on tackling the mismatch between image-level and pixel-level representations. They can also be categorized based on their methodology, including feature-space distillation that enforces embedding consistency between VLM’s encoder and the detection (or segmentation) encoder and pseudolabelling distillation that employs VLM-generated pseudo labels to regularize detection or segmentation models. Moreover, compared with VLM transfer, VLM knowledge distillation has clearly better flexibility of allowing different downstream networks regardless of the original VLMs.

<br>

# 8. Performance Comparison

## (1) Performance of VLM Pretraining

## (2) Performance of VLM Transfer Learning

## (3) Performance of VLM Knowledge Distillation

## (4) Summary

<br>

# 9. Future Directions

VLM enables effective usage of web data, zero-shot prediction without any task-specific fine-tuning, and openvocabulary visual recognition of images of arbitrary categories. It has been achieving great success with incredible visual recognition performance. In this section, we humbly share several research challenges and potential research directions that could be pursued in the future VLM study on various visual recognition tasks

