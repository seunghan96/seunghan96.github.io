---
title: (code review 1) BERT
categories: [NLP,HBERT]
tags: [NLP, HBM]
excerpt: HBM (Hierarchical BERT Model)
---

# Code Review for BERT

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

참고 : https://github.com/frankaging/Quasi-Attention-ABSA/blob/main/code/model/BERT.py

<br>

# 1. Architecture

![figure2](/assets/img/nlp/code1.png)

BERTModel

- (1) BERTEmbedding

- (2) BERTEncoder x N
  - 2-1) BERT Attention
    - a) BERT Self-Attention
    - b) BERT Self-Output
  - 2-2) BERT Intermediate
  - 2-3) BERT Output

<br>

# 2. Class & Function introduction

## 1) Main Modules

Model : `BERTModel`

- BERT Embedding : `BERTEmbedding` 

- Encoder : `BERTEncoder` 
  - Bert layer : `BERTLayer` 
    - Attention : `BERTAttention`
      - Self Attention : `BERTSelfAttention`
      - Self Attention output : `BERTSelfOutput`
    - Intermediate layer : `BERTIntermediate`
    - Output layer : `BERTOutput`

<br>

## 2) Functions / Other Classes

- GeLU : `gelu`
- Layer Normalization : `BERTLayerNorm`
- Pooler : `BERTPooler`

<br>

# 3. Code Review (with Pytorch)

# 3-1) Main Modules

# BertModel

```python
class BertModel(nn.Module):
    def __init__(self, config: BertConfig):
        super(BertModel, self).__init__()
        self.embeddings = BERTEmbeddings(config)
        self.encoder = BERTEncoder(config)
        self.pooler = BERTPooler(config)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None):
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
            
		# (1) attention mask 생성
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.float()
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0

        # (2) 세 종류의 embedding을 더함
        embedding_output = self.embeddings(input_ids, token_type_ids)
        
        # (3) L개의 (Transformer) Encoder layer 를 거침
        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)
        
        # (4) (맨 마지막 L번째 layer output 제외하고) Pooling
        sequence_output = all_encoder_layers[-1]
        pooled_output = self.pooler(sequence_output)
        
        # (5) 전체 L개 & L-1개의 pooling 결과 반환
        return all_encoder_layers, pooled_output
```

<br>

# [A] BERT Embedding

![figure2](/assets/img/nlp/code3.png)

**세 종류의 embedding** 을 모두 더한다! ( $h$ 차원 )

- 1) `word_embeddings` : Token Embeddings ..... Embedding ( in = Vocab 개수, out = $h$)

- 2) `position_embeddings` : Position Embeddings ..... Embedding ( in = 최대 문장 길이, out = $h$ )

- 3) `token_type_embeddings` : Token Type Embeddings ..... Embedding ( in = Vocab type 개수, out =$h$ )

  ( default가 16인걸 보면, token의 종류 ex. 명사/동사/접두사/접미사 등... 일듯? )

```python
class BERTEmbeddings(nn.Module):
    def __init__(self, config):
        super(BERTEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.LayerNorm = BERTLayerNorm(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, token_type_ids=None):
        # (1) Position embedding을 위한 (문장의 token 길이만큼의) index 생성
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
		
        # (2) 세 종류의 Embedding을 더함
        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        
        # (3) Layer Normalization + Dropout
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```

<br>

# [B] BERT Encoder

![figure2](/assets/img/nlp/code2.png)

```python
class BERTEncoder(nn.Module):
    def __init__(self, config):
        super(BERTEncoder, self).__init__()
        layer = BERTLayer(config)
        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    

    def forward(self, hidden_states, attention_mask):
        all_encoder_layers = []
        for layer_module in self.layer:
            hidden_states = layer_module(hidden_states, attention_mask)
            all_encoder_layers.append(hidden_states)
        return all_encoder_layers
```

<br>

## [B-1] BERT Attention

```python
class BERTAttention(nn.Module):
    def __init__(self, config):
        super(BERTAttention, self).__init__()
        self.self = BERTSelfAttention(config)
        self.output = BERTSelfOutput(config)

    def forward(self, input_tensor, attention_mask):
        self_output = self.self(input_tensor, attention_mask)
        attention_output = self.output(self_output, input_tensor)
        return attention_output
```

<br>

### [B-1-a] Self Attention

- `num_attention_heads` : attention head의 개수

- `attention_head_size` : attention head 1개 당의 dimension

- ex) hidden dimension $h$ =100 & attention head의 개수 = $5$  

  $\rightarrow$ attention head size = $20$ 

  $\rightarrow$  all head size = $5 \times 20 = 100$

  $Q$, $K$,$V$ weight 의 dimension : $h \times \text{all head size}$  ( = $100 \times 100$ )

  - $Q$ , $K$, $V$ 모두 결국엔 all_head_size (=100) 차원

```python
class BERTSelfAttention(nn.Module):
    def __init__(self, config):
        super(BERTSelfAttention, self).__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        Q = self.transpose_for_scores(mixed_query_layer)
        K = self.transpose_for_scores(mixed_key_layer)
        V = self.transpose_for_scores(mixed_value_layer)

        attention_scores = torch.matmul(Q, K.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_scores = attention_scores + attention_mask

        A = nn.Softmax(dim=-1)(attention_scores)
        A = self.dropout(A)
        
        context_layer = torch.matmul(A, V)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        return context_layer
```

<br>

### [B-1-b] Self Attention Output

```python
class BERTSelfOutput(nn.Module):
    def __init__(self, config):
        super(BERTSelfOutput, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = BERTLayerNorm(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
```

<br>

## [B-2] Intermediate Layer

```python
class BERTIntermediate(nn.Module):
    def __init__(self, config):
        super(BERTIntermediate, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.intermediate_act_fn = gelu

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states
```

<br>

## [B-3] Output Layer

```python
class BERTOutput(nn.Module):
    def __init__(self, config):
        super(BERTOutput, self).__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = BERTLayerNorm(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
```

<br>

# 3-2) Functions / Other Classes

# GeLU

$\operatorname{GELU}(x)=0.5 x\left(1+\tanh \left(\sqrt{2 / \pi}\left(x+0.044715 x^{3}\right)\right)\right)$.

```python
def gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
```

<br>

# Layer Normalization

```python
class BERTLayerNorm(nn.Module):
    def __init__(self, config, variance_epsilon=1e-12):
        """Construct a layernorm module in the TF style (epsilon inside the square root).
        """
        super(BERTLayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(config.hidden_size))
        self.beta = nn.Parameter(torch.zeros(config.hidden_size))
        self.variance_epsilon = variance_epsilon

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        std = (x - mu).pow(2).mean(-1, keepdim=True)
        x = (x - mu) / torch.sqrt(std + self.variance_epsilon)
        return self.gamma * x + self.beta
```

<br>

# BERTPooler

```python
class BERTPooler(nn.Module):
    def __init__(self, config):
        super(BERTPooler, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        #return first_token_tensor
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```





