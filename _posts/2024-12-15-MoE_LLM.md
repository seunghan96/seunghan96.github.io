---
title: Mixture-of-Agents Enhances Large Language Model Capabilities
categories: [LLM, NLP]
tags: []
excerpt: ICLR 2025
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Mixture-of-Agents Enhances Large Language Model Capabilities

```
Wang, Junlin, et al. "Mixture-of-Agents Enhances Large Language Model Capabilities." 
```

참고: 

- https://aipapersacademy.com/mixture-of-agents/
- https://arxiv.org/pdf/2406.04692

<br>

### Contents

1. Introductions
1. The Mixture-of-Agents Method
1. Experiments

<br>

# 1. Introductions

Various LLMs

- e.g., GPT-4, Llama 3, Qwen, Mixtral ...

<br>

**Mixture-of-Agents** = LLMs can collaborate together as a team

$$\rightarrow$$ Get a response that is powered by **multiple LLMs**!

<br>

# 2. The Mixture-of-Agents Method

![figure2](/assets/img/llm/img185.png)

<br>

Mixture-of-Agents 

= Combined from multiple layers

- Each layer has multiple LLMs

<br>

MoE vs. MoA

- MoE: Experts = Parts of the same model
- MoA: Experts = Full-fledged LLMs.

<br>

**Final layer**: only a **single LLM**

- (Input) input prompt and additional responses (gathered along the way from previous layers)

![figure2](/assets/img/llm/img186.png)

<br>

# 3. Experiments

![figure2](/assets/img/llm/img187.png)

![figure2](/assets/img/llm/img188.png)



<br>
