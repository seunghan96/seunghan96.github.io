---
title: (paper 16) Revisiting SSL
categories: [CL, CV]
tags: []
excerpt: 2019
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Revisiting Self-Supervised Visual Representation Learning

<br>

## Contents

0. Abstract
0. Introduction

<br>

# 0. Abstract

Previous works :

- mostly focus on **pre-text tasks**
- but not on **CNN architectures**

$$\rightarrow$$ this paper revisit the previously proposed models!

<br>

# 1. Introduction

4 main contributions :

1. best architecture design : FULLY-supervised $$\neq$$ SELF-supervised
2. (unlike AlexNet) ResNet architecture
   - learned representations do not degrade toward the end of the model

3. increasing the model complexity of CNN

   $$\rightarrow$$ increase the quality of learned visual representation

4. (in evaluation procedure) lr is sensitive in linear model

<br>

![figure2](/assets/img/cl/img41.png)


